{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed if the notebook is run in VSCode\n",
    "from IPython.display import clear_output, DisplayHandle\n",
    "def update_patch(self, obj):\n",
    "    clear_output(wait=True)\n",
    "    self.display(obj)\n",
    "DisplayHandle.update = update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os              : Linux-6.8.0-40-generic-x86_64-with-glibc2.31\n",
      "python          : 3.10.12\n",
      "tsai            : 0.3.10\n",
      "fastai          : 2.7.15\n",
      "fastcore        : 1.5.49\n",
      "sklearn         : 1.5.1\n",
      "torch           : 2.2.2+cu121\n",
      "device          : 1 gpu (['NVIDIA GeForce RTX 3070 Ti Laptop GPU'])\n",
      "cpu cores       : 14\n",
      "threads per cpu : 1\n",
      "RAM             : 15.28 GB\n",
      "GPU memory      : [8.0] GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import sklearn\n",
    "from tsai.basics import *\n",
    "from swdf.utils import *\n",
    "my_setup(sklearn)\n",
    "from matplotlib import dates as mdates\n",
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.progress import ShowGraphCallback\n",
    "from itertools import chain\n",
    "import more_itertools as mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed for the MIT supercloud, to fix fastai's LRFinder error \n",
    "if torch.cuda.is_available() and torch.cuda.device_count() == 0:\n",
    "    from fastai.callback.schedule import LRFinder\n",
    "\n",
    "    @patch_to(LRFinder)\n",
    "    def after_fit(self):\n",
    "        self.learn.opt.zero_grad() # Needed before detaching the optimizer for future fits\n",
    "        tmp_f = self.path/self.model_dir/self.tmp_p/'_tmp.pth'\n",
    "        if tmp_f.exists():\n",
    "            self.learn.load(f'{self.tmp_p}/_tmp', with_opt=True, device='cpu')\n",
    "            self.tmp_d.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting solar drivers F10, S10, M10 and Y10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints about hyperparameters:\n",
    "- According to the authors of PatchTST: \"The ideal patch length may depend on the dataset, \n",
    "but P between {8, 16} seems to be general good numbers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_base = yaml2dict('./config/base.yaml', attrdict=True)\n",
    "config_solfsmy = yaml2dict('./config/solfsmy.yaml', attrdict=True)\n",
    "config_train = config_solfsmy.train\n",
    "config_data = config_solfsmy.data \n",
    "\n",
    "# Merge the two configs (the second one overrides the first one for any keys that are present in both)\n",
    "config = AttrDict({**config_base,\n",
    "                   **config_train, \n",
    "                   \"data\":AttrDict({**config_data})})\n",
    "\n",
    "# Add the architecture config\n",
    "if config.arch_name.lower() == 'patchtst':\n",
    "    config.arch = yaml2dict('./config/patchtst.yaml', attrdict=True)\n",
    "else:\n",
    "    config.arch = AttrDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=config.wandb.project, \n",
    "                 config=config,\n",
    "                 group=config.wandb.group,\n",
    "                 mode=config.wandb.mode, \n",
    "                 anonymous='never') if config.wandb.enabled else None\n",
    "config = dict2attrdict(run.config) if config.wandb.enabled else config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{ 'arch': {'attn_dropout': 0.4, 'd_model': 1024, 'dropout': 0.2, 'n_heads': 32},\n",
       "  'arch_config_file': 'config/patchtst.yaml',\n",
       "  'arch_name': 'PatchTST',\n",
       "  'bs': 32,\n",
       "  'data': { 'add_time_channels': False,\n",
       "            'data_columns': ['F10', 'S10', 'M10', 'Y10'],\n",
       "            'data_nb': './solfsmy_data.ipynb',\n",
       "            'data_path': '../data/SOLFSMY.TXT',\n",
       "            'data_time_columns': ['Year', 'DDD'],\n",
       "            'data_url': 'https://sol.spacenvironment.net/JB2008/indices/SOLFSMY.TXT',\n",
       "            'df_save_path': './preprocessed_data/dataframes/solfsmy.pkl',\n",
       "            'force_download': False,\n",
       "            'preproc_pipe_save_path': './preprocessed_data/pipelines/preproc_solfsmy.pkl'},\n",
       "  'deltaHL': 4.0,\n",
       "  'horizon': 6,\n",
       "  'init_weights': False,\n",
       "  'is_optuna_study': False,\n",
       "  'lookback': 24,\n",
       "  'lr_max': None,\n",
       "  'n_epoch': 300,\n",
       "  'partial_n': 0.1,\n",
       "  'seed': 42,\n",
       "  'test_end_datetime': '2018-12-31 12:00:00',\n",
       "  'test_start_datetime': '2012-10-01 12:00:00',\n",
       "  'valid_start_datetime': '2018-01-01 12:00:00',\n",
       "  'wandb': { 'enabled': False,\n",
       "             'group': None,\n",
       "             'log_learner': True,\n",
       "             'mode': 'offline',\n",
       "             'project': 'swdf'}}\n",
       "```"
      ],
      "text/plain": [
       "{'arch_name': 'PatchTST',\n",
       " 'arch_config_file': 'config/patchtst.yaml',\n",
       " 'bs': 32,\n",
       " 'horizon': 6,\n",
       " 'init_weights': False,\n",
       " 'lookback': 24,\n",
       " 'lr_max': None,\n",
       " 'n_epoch': 300,\n",
       " 'partial_n': 0.1,\n",
       " 'seed': 42,\n",
       " 'test_start_datetime': '2012-10-01 12:00:00',\n",
       " 'test_end_datetime': '2018-12-31 12:00:00',\n",
       " 'valid_start_datetime': '2018-01-01 12:00:00',\n",
       " 'deltaHL': 4.0,\n",
       " 'is_optuna_study': False,\n",
       " 'wandb': {'enabled': False,\n",
       "  'log_learner': True,\n",
       "  'mode': 'offline',\n",
       "  'group': None,\n",
       "  'project': 'swdf'},\n",
       " 'data': {'data_url': 'https://sol.spacenvironment.net/JB2008/indices/SOLFSMY.TXT',\n",
       "  'data_path': '../data/SOLFSMY.TXT',\n",
       "  'force_download': False,\n",
       "  'data_nb': './solfsmy_data.ipynb',\n",
       "  'df_save_path': './preprocessed_data/dataframes/solfsmy.pkl',\n",
       "  'preproc_pipe_save_path': './preprocessed_data/pipelines/preproc_solfsmy.pkl',\n",
       "  'data_columns': ['F10', 'S10', 'M10', 'Y10'],\n",
       "  'data_time_columns': ['Year', 'DDD'],\n",
       "  'add_time_channels': False},\n",
       " 'arch': {'attn_dropout': 0.4, 'd_model': 1024, 'dropout': 0.2, 'n_heads': 32}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>DDD</th>\n",
       "      <th>JulianDay</th>\n",
       "      <th>F10</th>\n",
       "      <th>F81c</th>\n",
       "      <th>S10</th>\n",
       "      <th>S81c</th>\n",
       "      <th>M10</th>\n",
       "      <th>M81c</th>\n",
       "      <th>Y10</th>\n",
       "      <th>Y81c</th>\n",
       "      <th>Ssrc</th>\n",
       "      <th>Ssrc_F10</th>\n",
       "      <th>Ssrc_S10</th>\n",
       "      <th>Ssrc_M10</th>\n",
       "      <th>Ssrc_Y10</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>1</td>\n",
       "      <td>2450450.0</td>\n",
       "      <td>72.400002</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>79.199997</td>\n",
       "      <td>65.400002</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>61.900002</td>\n",
       "      <td>70.699997</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>2</td>\n",
       "      <td>2450451.0</td>\n",
       "      <td>72.099998</td>\n",
       "      <td>77.900002</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>66.900002</td>\n",
       "      <td>73.699997</td>\n",
       "      <td>63.400002</td>\n",
       "      <td>70.400002</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>2450452.0</td>\n",
       "      <td>73.300003</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>70.099998</td>\n",
       "      <td>73.599998</td>\n",
       "      <td>64.900002</td>\n",
       "      <td>70.199997</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>4</td>\n",
       "      <td>2450453.0</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>77.300003</td>\n",
       "      <td>76.300003</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>71.599998</td>\n",
       "      <td>73.400002</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>69.800003</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-04 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>5</td>\n",
       "      <td>2450454.0</td>\n",
       "      <td>74.400002</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>78.900002</td>\n",
       "      <td>72.400002</td>\n",
       "      <td>73.300003</td>\n",
       "      <td>66.699997</td>\n",
       "      <td>69.599998</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-05 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  DDD  JulianDay        F10       F81c        S10       S81c  \\\n",
       "0  1997    1  2450450.0  72.400002  78.000000  74.000000  79.199997   \n",
       "1  1997    2  2450451.0  72.099998  77.900002  73.800003  79.099998   \n",
       "2  1997    3  2450452.0  73.300003  77.599998  75.000000  79.099998   \n",
       "3  1997    4  2450453.0  73.800003  77.300003  76.300003  79.000000   \n",
       "4  1997    5  2450454.0  74.400002  76.900002  77.599998  78.900002   \n",
       "\n",
       "         M10       M81c        Y10       Y81c  Ssrc Ssrc_F10 Ssrc_S10  \\\n",
       "0  65.400002  73.800003  61.900002  70.699997  1B11        1        B   \n",
       "1  66.900002  73.699997  63.400002  70.400002  1B11        1        B   \n",
       "2  70.099998  73.599998  64.900002  70.199997  1B11        1        B   \n",
       "3  71.599998  73.400002  65.500000  69.800003  1B11        1        B   \n",
       "4  72.400002  73.300003  66.699997  69.599998  1B11        1        B   \n",
       "\n",
       "  Ssrc_M10 Ssrc_Y10                Date  \n",
       "0        1        1 1997-01-01 12:00:00  \n",
       "1        1        1 1997-01-02 12:00:00  \n",
       "2        1        1 1997-01-03 12:00:00  \n",
       "3        1        1 1997-01-04 12:00:00  \n",
       "4        1        1 1997-01-05 12:00:00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, preproc_pipe = generate_preprocessed_data(config.data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper by [Licata et al. (2020)]((https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020SW002496)), authors use a period from October 2012 through the end of 2018 for the benchmarking. Therefore, we will set the test set as the same period for our analysis, using the column Date as the timestamp, from October 2012 to the end of 2018. Everything before the test set will be used for training, and everything after the test set will be used for validation. In this paper, the authors also present some the thresholds for categorizing solar activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F10': [(0, 75), (76, 150), (151, 190), (191, 99999)], 'S10': [(0, 65), (66, 150), (151, 215), (216, 99999)], 'M10': [(0, 72), (73, 144), (145, 167), (168, 99999)], 'Y10': [(0, 81), (82, 148), (149, 165), (166, 99999)]}\n"
     ]
    }
   ],
   "source": [
    "test_start_datetime = config.test_start_datetime\n",
    "test_end_datetime = config.test_end_datetime\n",
    "\n",
    "thresholds = yaml2dict(\"distributions/thresholds.yaml\")\n",
    "solact_levels = yaml2dict(\"distributions/thresholds_labels.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moderate': 0.5065749000307598,\n",
       " 'low': 0.18390495232236234,\n",
       " 'high': 0.16145032297754536,\n",
       " 'elevated': 0.14806982466933252}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_F10 = load_object(\"./preprocessed_data/dataframes/F10_historical.pkl\")\n",
    "\n",
    "df_F107_cat = get_classified_columns(df_F10, thresholds, activity_levels={'F10': solact_levels['F10']})['F10_Cat']\n",
    "historical_distribution = df_F107_cat.value_counts(normalize=True).to_dict()\n",
    "\n",
    "historical_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 0.22903290529695025, Validation size: 0.19456083279115158, Train size: 0.6209670947030498\n",
      "Total number of segments:30, Number of segments for validation: 5 (16.67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142506it [00:02, 70424.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest group of segments to F10.7 categories has an euclidean distance of 0.005517135807631744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_period = (df.Date >= test_start_datetime) & (df.Date <= test_end_datetime)\n",
    "\n",
    "df_cat = get_classified_columns(df, thresholds, solact_levels)\n",
    "df_cat = df_cat[~(test_period)]\n",
    "\n",
    "# Function parameter calculation\n",
    "test_size = df[test_period].shape[0] / df.shape[0]\n",
    "train_val_size = 1 - test_size\n",
    "val_size = 0.15 / train_val_size\n",
    "print(f\"Test size: {test_size}, Validation size: {val_size}, Train size: {1 - test_size - 0.15}\")\n",
    "\n",
    "# Best segment size found: 250, val_size: 0.15, test_size: 0.65\n",
    "best_comb, segments, distribution = find_closest_distribution(df_cat['F10_Cat'], historical_distribution, 250, val_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moderate': 0.5067145534041224,\n",
       " 'low': 0.28856964397251716,\n",
       " 'elevated': 0.1316364772017489,\n",
       " 'high': 0.0730793254216115}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_comb_idxs = [segments[i] for i in best_comb]\n",
    "validation_segments = (df.index.isin(chain.from_iterable(best_comb_idxs)))\n",
    "\n",
    "train_df = df[~validation_segments & ~test_period] \n",
    "train_distribution = get_classified_columns(train_df, thresholds, solact_levels)['F10_Cat'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "train_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Apply a sliding window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((#6230) [1136,1137,1138,1139,1140,1141,1142,1143,1144,1145...],\n",
       "  (#1136) [0,1,2,3,4,5,6,7,8,9...],\n",
       "  (#2254) [7366,7367,7368,7369,7370,7371,7372,7373,7374,7375...]),\n",
       " (9620, 4, 24),\n",
       " (9620, 4, 6))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(0,len(segments))\n",
    "train_comb = list(np.setdiff1d(a, best_comb))\n",
    "data_columns = config.data.data_columns\n",
    "\n",
    "X_val, y_val, split_val = sliding_window_generator(df, 0, data_columns=data_columns, config=config, comb=best_comb, segments=segments)\n",
    "X_train, y_train, split_train = sliding_window_generator(df, split_val[-1]+1, data_columns=data_columns, config=config, comb=train_comb, segments=segments)\n",
    "X_test, y_test, split_test = sliding_window_generator(df[test_period], split_train[-1]+1, data_columns=data_columns, config=config) \n",
    "\n",
    "X = np.concatenate([X_val, X_train, X_test])\n",
    "y = np.concatenate([y_val, y_train, y_test])\n",
    "\n",
    "splits = (split_train, split_val, split_test)\n",
    "splits, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "tmp/pipes directory already exists.\n",
      "Pipeline saved as tmp/pipes/exp_pipe.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>DDD</th>\n",
       "      <th>JulianDay</th>\n",
       "      <th>F10</th>\n",
       "      <th>F81c</th>\n",
       "      <th>S10</th>\n",
       "      <th>S81c</th>\n",
       "      <th>M10</th>\n",
       "      <th>M81c</th>\n",
       "      <th>Y10</th>\n",
       "      <th>Y81c</th>\n",
       "      <th>Ssrc</th>\n",
       "      <th>Ssrc_F10</th>\n",
       "      <th>Ssrc_S10</th>\n",
       "      <th>Ssrc_M10</th>\n",
       "      <th>Ssrc_Y10</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>1</td>\n",
       "      <td>2450450.0</td>\n",
       "      <td>-0.905757</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>-0.761630</td>\n",
       "      <td>79.199997</td>\n",
       "      <td>-1.113339</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>-1.293490</td>\n",
       "      <td>70.699997</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>2</td>\n",
       "      <td>2450451.0</td>\n",
       "      <td>-0.912979</td>\n",
       "      <td>77.900002</td>\n",
       "      <td>-0.766386</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>-1.075890</td>\n",
       "      <td>73.699997</td>\n",
       "      <td>-1.254068</td>\n",
       "      <td>70.400002</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>2450452.0</td>\n",
       "      <td>-0.884091</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>-0.737850</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>-0.996001</td>\n",
       "      <td>73.599998</td>\n",
       "      <td>-1.214647</td>\n",
       "      <td>70.199997</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>4</td>\n",
       "      <td>2450453.0</td>\n",
       "      <td>-0.872054</td>\n",
       "      <td>77.300003</td>\n",
       "      <td>-0.706937</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>-0.958552</td>\n",
       "      <td>73.400002</td>\n",
       "      <td>-1.198879</td>\n",
       "      <td>69.800003</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-04 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>5</td>\n",
       "      <td>2450454.0</td>\n",
       "      <td>-0.857611</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>-0.676023</td>\n",
       "      <td>78.900002</td>\n",
       "      <td>-0.938580</td>\n",
       "      <td>73.300003</td>\n",
       "      <td>-1.167341</td>\n",
       "      <td>69.599998</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-05 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  DDD  JulianDay       F10       F81c       S10       S81c       M10  \\\n",
       "0  1997    1  2450450.0 -0.905757  78.000000 -0.761630  79.199997 -1.113339   \n",
       "1  1997    2  2450451.0 -0.912979  77.900002 -0.766386  79.099998 -1.075890   \n",
       "2  1997    3  2450452.0 -0.884091  77.599998 -0.737850  79.099998 -0.996001   \n",
       "3  1997    4  2450453.0 -0.872054  77.300003 -0.706937  79.000000 -0.958552   \n",
       "4  1997    5  2450454.0 -0.857611  76.900002 -0.676023  78.900002 -0.938580   \n",
       "\n",
       "        M81c       Y10       Y81c  Ssrc Ssrc_F10 Ssrc_S10 Ssrc_M10 Ssrc_Y10  \\\n",
       "0  73.800003 -1.293490  70.699997  1B11        1        B        1        1   \n",
       "1  73.699997 -1.254068  70.400002  1B11        1        B        1        1   \n",
       "2  73.599998 -1.214647  70.199997  1B11        1        B        1        1   \n",
       "3  73.400002 -1.198879  69.800003  1B11        1        B        1        1   \n",
       "4  73.300003 -1.167341  69.599998  1B11        1        B        1        1   \n",
       "\n",
       "                 Date  \n",
       "0 1997-01-01 12:00:00  \n",
       "1 1997-01-02 12:00:00  \n",
       "2 1997-01-03 12:00:00  \n",
       "3 1997-01-04 12:00:00  \n",
       "4 1997-01-05 12:00:00  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have defined the splits for this particular experiment, we'll scale\n",
    "# the data\n",
    "exp_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('scaler', TSStandardScaler(columns=data_columns)),\n",
    "], verbose=True)\n",
    "\n",
    "df_scaled = exp_pipe.fit_transform(df.copy())\n",
    "\n",
    "save_object(exp_pipe, 'tmp/pipes/exp_pipe.pkl')\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'All': [0.49933374244488,\n",
       "   0.0006662575551199645,\n",
       "   0.07840012484165651,\n",
       "   0.4215998751583436]},\n",
       " {'elevated': 0.07840012484165651,\n",
       "  'moderate': 0.0006662575551199645,\n",
       "  'low': 0.49933374244488,\n",
       "  'high': 0.4215998751583436})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {k: np.abs(train_distribution.get(k, 0) - historical_distribution.get(k, 0)) for k in set(train_distribution) | set(historical_distribution)}\n",
    "factor = 1.0/sum(weights.values())\n",
    "weights_raw = {k: v*factor for k, v in weights.items()}\n",
    "\n",
    "weights = {'All': [weights_raw[k] for k in solact_levels['F10']]}\n",
    "weights, weights_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swdf.losses import LossFactory\n",
    "from swdf.metrics import ValidationMetricsHandler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "loss = LossFactory(thresholds=thresholds, weights=weights).create(config.loss_func, **config.loss_config).to(device)\n",
    "\n",
    "metrics_handler = load_object(config.metrics_handler_path)\n",
    "metrics = metrics_handler.get_metrics()\n",
    "\n",
    "wandb_callback = WandbCallback(log_preds=False)\n",
    "cbs = L(wandb_callback) if config.wandb.enabled else L()\n",
    "learn = TSForecaster(X, y, splits=splits, batch_size=config.bs,\n",
    "                     pipelines=[preproc_pipe, exp_pipe], \n",
    "                     arch=config.arch_name, \n",
    "                     metrics=metrics,\n",
    "                     loss_func=loss,\n",
    "                     arch_config=dict(config.arch), \n",
    "                     init=config.init_weights,\n",
    "                     cbs= cbs + ShowGraphCallback(), \n",
    "                     partial_n=config.partial_n)\n",
    "learn.to(device)\n",
    "\n",
    "try:\n",
    "    lr_max = learn.lr_find().valley if config.lr_max is None else config.lr_max\n",
    "except:\n",
    "    lr_max = 1e-3\n",
    "\n",
    "learn.fit_one_cycle(n_epoch=config.n_epoch, lr_max=config.lr_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.dls.loaders += [learn.dls.valid.new_dl(X[splits[2]], y[splits[2]])] # Add test datalaoder\n",
    "\n",
    "# Remove the wandb callback to avoid errors when downloading the learner\n",
    "if config.wandb.enabled:\n",
    "    learn.remove_cb(wandb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'best_valid_loss' (float)\n",
      "Learner saved:\n",
      "path          = 'tmp'\n",
      "dls_fname     = '['dls_0.pth', 'dls_1.pth', 'dls_2.pth', 'dls_3.pth']'\n",
      "model_fname   = 'model.pth'\n",
      "learner_fname = 'learner.pkl'\n"
     ]
    }
   ],
   "source": [
    "valid_metrics = learn.recorder.metrics\n",
    "%store valid_metrics\n",
    "\n",
    "best_valid_metrics = None\n",
    "%store -r best_valid_metrics\n",
    "\n",
    "save_all_or_best = (\n",
    "                    config.is_optuna_study and \n",
    "                    metrics_handler.are_best_values(config.main_metric, best_valid_metrics, valid_metrics)\n",
    "                ) or not config.is_optuna_study\n",
    "\n",
    "\n",
    "if save_all_or_best:\n",
    "    best_valid_metrics = valid_metrics\n",
    "    %store best_valid_metrics                                 \n",
    "\n",
    "    # Save locally and in wandb if online and enabled\n",
    "    learn.save_all(path='tmp', verbose=True) \n",
    "    if run is not None and config.wandb_mode and config.wandb_log_learner:\n",
    "        # Save the learner (all tmp/dls, tmp/model.pth, and tmp/learner.pkl). \n",
    "        run.log_artifact('tmp', type='learner', name='solfsmy')\n",
    "    \n",
    "    # Save the best model\n",
    "    model = {}\n",
    "    model['models'] = [learn.model]\n",
    "    model['lookback'] = config.lookback\n",
    "    model['horizon'] = config.horizon\n",
    "    model['data_columns'] = data_columns\n",
    "    model['thresholds'] = thresholds\n",
    "\n",
    "    torch.save(model, 'models/best/best_model_solfsmy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.21273422241211\n"
     ]
    }
   ],
   "source": [
    "# Log the test loss to wandb\n",
    "test_loss = learn.validate(ds_idx=2)[0]\n",
    "\n",
    "if run is not None:\n",
    "    run.log(dict(test_loss=test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run is not None:\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
