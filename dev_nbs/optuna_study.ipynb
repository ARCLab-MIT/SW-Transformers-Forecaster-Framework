{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna study\n",
    "\n",
    "> Combine it with papermill and wandb for seamless hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from tsai.optuna import *\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "import papermill as pm\n",
    "from tsai.optuna import run_optuna_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'lookback': [12, 18, 24, 36, 48, 60]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial:optuna.Trial):\n",
    "\n",
    "    # Define search space here\n",
    "    lookback = trial.suggest_int('lookback', low=12, high=60, step=6)\n",
    "\n",
    "    # Call the training notebook using papermill (don't print the output)\n",
    "    stdout_file = open('tmp/pm_stdout.txt', 'w')\n",
    "    stderr_file = open('tmp/pm_stderr.txt', 'w')\n",
    "    pm.execute_notebook(\n",
    "        './training.ipynb',\n",
    "        './tmp/pm_output.ipynb',\n",
    "        parameters = {\n",
    "            'config.lookback': lookback\n",
    "        },\n",
    "        stdout_file = stdout_file,\n",
    "        stderr_file = stderr_file\n",
    "    )\n",
    "\n",
    "    # Close the output files\n",
    "    stdout_file.close()\n",
    "    stderr_file.close()\n",
    "\n",
    "    # Get the output value of interest from the source notebook\n",
    "    %store -r valid_loss\n",
    "    return valid_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-22 20:43:48,205]\u001b[0m A new study created in memory with name: no-name-0f70d72e-e132-433f-af09-9f20950fede4\u001b[0m\n",
      "Black is not installed, parameters wont be formatted\n",
      "Executing:  85%|██████████████████████████████████████████▋       | 29/34 [00:07<00:01,  3.65cell/s]\n",
      "\u001b[33m[W 2023-03-22 20:43:56,191]\u001b[0m Trial 0 failed with parameters: {'lookback': 6} because of the following error: PapermillExecutionError(28, 21, 'cbs = L(WandbCallback(log_preds=False)) if config.use_wandb else L()\\nlearn = TSForecaster(X, y, splits=splits, batch_size=config.bs, \\n                     pipelines=[preproc_pipe, exp_pipe], arch=\"PatchTST\", \\n                     arch_config=dict(config.arch_config), \\n                    cbs= cbs + ShowGraphCallback())\\n#lr_max = learn.lr_find().valley\\nlr_max = 1e-3\\nprint(f\"#params: {sum(p.numel() for p in learn.model.parameters())}\")', 'RuntimeError', 'Trying to create tensor with negative dimension -1: [-1, 16]', ['\\x1b[0;31m---------------------------------------------------------------------------\\x1b[0m', '\\x1b[0;31mRuntimeError\\x1b[0m                              Traceback (most recent call last)', 'Cell \\x1b[0;32mIn[21], line 2\\x1b[0m\\n\\x1b[1;32m      1\\x1b[0m cbs \\x1b[38;5;241m=\\x1b[39m L(WandbCallback(log_preds\\x1b[38;5;241m=\\x1b[39m\\x1b[38;5;28;01mFalse\\x1b[39;00m)) \\x1b[38;5;28;01mif\\x1b[39;00m config\\x1b[38;5;241m.\\x1b[39muse_wandb \\x1b[38;5;28;01melse\\x1b[39;00m L()\\n\\x1b[0;32m----> 2\\x1b[0m learn \\x1b[38;5;241m=\\x1b[39m \\x1b[43mTSForecaster\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43mX\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43my\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43msplits\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43msplits\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mbatch_size\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mconfig\\x1b[49m\\x1b[38;5;241;43m.\\x1b[39;49m\\x1b[43mbs\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\n\\x1b[1;32m      3\\x1b[0m \\x1b[43m                     \\x1b[49m\\x1b[43mpipelines\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43m[\\x1b[49m\\x1b[43mpreproc_pipe\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mexp_pipe\\x1b[49m\\x1b[43m]\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43march\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[38;5;124;43m\"\\x1b[39;49m\\x1b[38;5;124;43mPatchTST\\x1b[39;49m\\x1b[38;5;124;43m\"\\x1b[39;49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\n\\x1b[1;32m      4\\x1b[0m \\x1b[43m                     \\x1b[49m\\x1b[43march_config\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[38;5;28;43mdict\\x1b[39;49m\\x1b[43m(\\x1b[49m\\x1b[43mconfig\\x1b[49m\\x1b[38;5;241;43m.\\x1b[39;49m\\x1b[43march_config\\x1b[49m\\x1b[43m)\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\n\\x1b[1;32m      5\\x1b[0m \\x1b[43m                    \\x1b[49m\\x1b[43mcbs\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43m \\x1b[49m\\x1b[43mcbs\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[38;5;241;43m+\\x1b[39;49m\\x1b[43m \\x1b[49m\\x1b[43mShowGraphCallback\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43m)\\x1b[49m\\x1b[43m)\\x1b[49m\\n\\x1b[1;32m      6\\x1b[0m \\x1b[38;5;66;03m#lr_max = learn.lr_find().valley\\x1b[39;00m\\n\\x1b[1;32m      7\\x1b[0m lr_max \\x1b[38;5;241m=\\x1b[39m \\x1b[38;5;241m1e-3\\x1b[39m\\n', 'File \\x1b[0;32m/usr/local/pip-global/tsai/tslearner.py:214\\x1b[0m, in \\x1b[0;36mTSForecaster.__init__\\x1b[0;34m(self, X, y, splits, tfms, inplace, sel_vars, sel_steps, weights, partial_n, train_metrics, valid_metrics, bs, batch_size, batch_tfms, pipelines, shuffle_train, drop_last, num_workers, do_setup, device, seed, arch, arch_config, pretrained, weights_path, exclude_head, cut, init, loss_func, opt_func, lr, metrics, cbs, wd, wd_bn_bias, train_bn, moms, path, model_dir, splitter, verbose)\\x1b[0m\\n\\x1b[1;32m    206\\x1b[0m     \\x1b[38;5;28;01melif\\x1b[39;00m \\x1b[38;5;28misinstance\\x1b[39m(arch, \\x1b[38;5;28mstr\\x1b[39m): arch \\x1b[38;5;241m=\\x1b[39m get_arch(arch)\\n\\x1b[1;32m    207\\x1b[0m     \\x1b[38;5;66;03m# if \\'xresnet\\' in arch.__name__.lower() and not \\'1d\\' in arch.__name__.lower():\\x1b[39;00m\\n\\x1b[1;32m    208\\x1b[0m     \\x1b[38;5;66;03m#     model = build_tsimage_model(arch, dls=dls, pretrained=pretrained, init=init, device=device, verbose=verbose, arch_config=arch_config)\\x1b[39;00m\\n\\x1b[1;32m    209\\x1b[0m     \\x1b[38;5;66;03m# elif \\'tabularmodel\\' in arch.__name__.lower():\\x1b[39;00m\\n\\x1b[0;32m   (...)\\x1b[0m\\n\\x1b[1;32m    212\\x1b[0m     \\x1b[38;5;66;03m#     model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\\x1b[39;00m\\n\\x1b[1;32m    213\\x1b[0m     \\x1b[38;5;66;03m#                        exclude_head=exclude_head, cut=cut, init=init, arch_config=arch_config)\\x1b[39;00m\\n\\x1b[0;32m--> 214\\x1b[0m     model \\x1b[38;5;241m=\\x1b[39m \\x1b[43mbuild_ts_model\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43march\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mdls\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mdls\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mdevice\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mdevice\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mverbose\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mverbose\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpretrained\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mpretrained\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mweights_path\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mweights_path\\x1b[49m\\x1b[43m,\\x1b[49m\\n\\x1b[1;32m    215\\x1b[0m \\x1b[43m                        \\x1b[49m\\x1b[43mexclude_head\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mexclude_head\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mcut\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mcut\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43minit\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43minit\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43march_config\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43march_config\\x1b[49m\\x1b[43m)\\x1b[49m\\n\\x1b[1;32m    216\\x1b[0m \\x1b[38;5;28;01mtry\\x1b[39;00m:\\n\\x1b[1;32m    217\\x1b[0m     \\x1b[38;5;28msetattr\\x1b[39m(model, \\x1b[38;5;124m\"\\x1b[39m\\x1b[38;5;124m__name__\\x1b[39m\\x1b[38;5;124m\"\\x1b[39m, arch\\x1b[38;5;241m.\\x1b[39m\\x1b[38;5;18m__name__\\x1b[39m)\\n', \"File \\x1b[0;32m/usr/local/pip-global/tsai/models/utils.py:158\\x1b[0m, in \\x1b[0;36mbuild_ts_model\\x1b[0;34m(arch, c_in, c_out, seq_len, d, dls, device, verbose, pretrained, weights_path, exclude_head, cut, init, arch_config, **kwargs)\\x1b[0m\\n\\x1b[1;32m    156\\x1b[0m \\x1b[38;5;28;01mif\\x1b[39;00m \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mltsf_\\x1b[39m\\x1b[38;5;124m'\\x1b[39m \\x1b[38;5;129;01min\\x1b[39;00m arch\\x1b[38;5;241m.\\x1b[39m\\x1b[38;5;18m__name__\\x1b[39m\\x1b[38;5;241m.\\x1b[39mlower() \\x1b[38;5;129;01mor\\x1b[39;00m \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mpatchtst\\x1b[39m\\x1b[38;5;124m'\\x1b[39m \\x1b[38;5;129;01min\\x1b[39;00m arch\\x1b[38;5;241m.\\x1b[39m\\x1b[38;5;18m__name__\\x1b[39m\\x1b[38;5;241m.\\x1b[39mlower():\\n\\x1b[1;32m    157\\x1b[0m     pv(\\x1b[38;5;124mf\\x1b[39m\\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124march: \\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00march\\x1b[38;5;241m.\\x1b[39m\\x1b[38;5;18m__name__\\x1b[39m\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m(c_in=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00mc_in\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m c_out=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00mc_out\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m seq_len=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00mseq_len\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m pred_dim=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00md\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m arch_config=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00march_config\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m, kwargs=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00mkwargs\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m)\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, verbose)\\n\\x1b[0;32m--> 158\\x1b[0m     model \\x1b[38;5;241m=\\x1b[39m (\\x1b[43march\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43mc_in\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mc_in\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mc_out\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mc_out\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mseq_len\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mseq_len\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpred_dim\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43md\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[38;5;241;43m*\\x1b[39;49m\\x1b[38;5;241;43m*\\x1b[39;49m\\x1b[43march_config\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[38;5;241;43m*\\x1b[39;49m\\x1b[38;5;241;43m*\\x1b[39;49m\\x1b[43mkwargs\\x1b[49m\\x1b[43m)\\x1b[49m)\\x1b[38;5;241m.\\x1b[39mto(device\\x1b[38;5;241m=\\x1b[39mdevice)\\n\\x1b[1;32m    159\\x1b[0m \\x1b[38;5;28;01melif\\x1b[39;00m \\x1b[38;5;28msum\\x1b[39m([\\x1b[38;5;241m1\\x1b[39m \\x1b[38;5;28;01mfor\\x1b[39;00m v \\x1b[38;5;129;01min\\x1b[39;00m [\\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mRNN_FCN\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mLSTM_FCN\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mRNNPlus\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mLSTMPlus\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mGRUPlus\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mInceptionTime\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mTSiT\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mSequencer\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mXceptionTimePlus\\x1b[39m\\x1b[38;5;124m'\\x1b[39m,\\n\\x1b[1;32m    160\\x1b[0m                     \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mGRU_FCN\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mOmniScaleCNN\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mmWDN\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mTST\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mXCM\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mMLP\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mMiniRocket\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, \\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124mInceptionRocket\\x1b[39m\\x1b[38;5;124m'\\x1b[39m]\\n\\x1b[1;32m    161\\x1b[0m         \\x1b[38;5;28;01mif\\x1b[39;00m v \\x1b[38;5;129;01min\\x1b[39;00m arch\\x1b[38;5;241m.\\x1b[39m\\x1b[38;5;18m__name__\\x1b[39m]):\\n\\x1b[1;32m    162\\x1b[0m     pv(\\x1b[38;5;124mf\\x1b[39m\\x1b[38;5;124m'\\x1b[39m\\x1b[38;5;124march: \\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00march\\x1b[38;5;241m.\\x1b[39m\\x1b[38;5;18m__name__\\x1b[39m\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m(c_in=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00mc_in\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m c_out=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00mc_out\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m seq_len=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00mseq_len\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m arch_config=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00march_config\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m kwargs=\\x1b[39m\\x1b[38;5;132;01m{\\x1b[39;00mkwargs\\x1b[38;5;132;01m}\\x1b[39;00m\\x1b[38;5;124m)\\x1b[39m\\x1b[38;5;124m'\\x1b[39m, verbose)\\n\", 'File \\x1b[0;32m/usr/local/pip-global/tsai/models/PatchTST.py:446\\x1b[0m, in \\x1b[0;36mPatchTST.__init__\\x1b[0;34m(self, c_in, c_out, seq_len, pred_dim, n_layers, n_heads, d_model, d_ff, dropout, attn_dropout, patch_len, stride, padding_patch, revin, affine, individual, subtract_last, decomposition, kernel_size, activation, norm, pre_norm, res_attention, store_attn)\\x1b[0m\\n\\x1b[1;32m    444\\x1b[0m     \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mpatch_num \\x1b[38;5;241m=\\x1b[39m \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mmodel_trend\\x1b[38;5;241m.\\x1b[39mpatch_num\\n\\x1b[1;32m    445\\x1b[0m \\x1b[38;5;28;01melse\\x1b[39;00m:\\n\\x1b[0;32m--> 446\\x1b[0m     \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mmodel \\x1b[38;5;241m=\\x1b[39m \\x1b[43m_PatchTST_backbone\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43mc_in\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mc_in\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mseq_len\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mseq_len\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpred_dim\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mpred_dim\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\n\\x1b[1;32m    447\\x1b[0m \\x1b[43m                                    \\x1b[49m\\x1b[43mpatch_len\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mpatch_len\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mstride\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mstride\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mn_layers\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mn_layers\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43md_model\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43md_model\\x1b[49m\\x1b[43m,\\x1b[49m\\n\\x1b[1;32m    448\\x1b[0m \\x1b[43m                                    \\x1b[49m\\x1b[43mn_heads\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mn_heads\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43md_ff\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43md_ff\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mnorm\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mnorm\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mattn_dropout\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mattn_dropout\\x1b[49m\\x1b[43m,\\x1b[49m\\n\\x1b[1;32m    449\\x1b[0m \\x1b[43m                                    \\x1b[49m\\x1b[43mdropout\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mdropout\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mact\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mactivation\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mres_attention\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mres_attention\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpre_norm\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mpre_norm\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\n\\x1b[1;32m    450\\x1b[0m \\x1b[43m                                    \\x1b[49m\\x1b[43mstore_attn\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mstore_attn\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpadding_patch\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mpadding_patch\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\n\\x1b[1;32m    451\\x1b[0m \\x1b[43m                                    \\x1b[49m\\x1b[43mindividual\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mindividual\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mrevin\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mrevin\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43maffine\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43maffine\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43msubtract_last\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43msubtract_last\\x1b[49m\\x1b[43m)\\x1b[49m\\n\\x1b[1;32m    452\\x1b[0m     \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mpatch_num \\x1b[38;5;241m=\\x1b[39m \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mmodel\\x1b[38;5;241m.\\x1b[39mpatch_num\\n', 'File \\x1b[0;32m/usr/local/pip-global/tsai/models/PatchTST.py:356\\x1b[0m, in \\x1b[0;36m_PatchTST_backbone.__init__\\x1b[0;34m(self, c_in, seq_len, pred_dim, patch_len, stride, n_layers, d_model, n_heads, d_k, d_v, d_ff, norm, attn_dropout, dropout, act, res_attention, pre_norm, store_attn, padding_patch, individual, revin, affine, subtract_last)\\x1b[0m\\n\\x1b[1;32m    353\\x1b[0m \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mpatch_len \\x1b[38;5;241m=\\x1b[39m patch_len\\n\\x1b[1;32m    355\\x1b[0m \\x1b[38;5;66;03m# Backbone \\x1b[39;00m\\n\\x1b[0;32m--> 356\\x1b[0m \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mbackbone \\x1b[38;5;241m=\\x1b[39m \\x1b[43m_TSTiEncoder\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43mc_in\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpatch_num\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mpatch_num\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpatch_len\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mpatch_len\\x1b[49m\\x1b[43m,\\x1b[49m\\n\\x1b[1;32m    357\\x1b[0m \\x1b[43m                             \\x1b[49m\\x1b[43mn_layers\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mn_layers\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43md_model\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43md_model\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mn_heads\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mn_heads\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43md_k\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43md_k\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43md_v\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43md_v\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43md_ff\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43md_ff\\x1b[49m\\x1b[43m,\\x1b[49m\\n\\x1b[1;32m    358\\x1b[0m \\x1b[43m                             \\x1b[49m\\x1b[43mattn_dropout\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mattn_dropout\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mdropout\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mdropout\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mact\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mact\\x1b[49m\\x1b[43m,\\x1b[49m\\n\\x1b[1;32m    359\\x1b[0m \\x1b[43m                             \\x1b[49m\\x1b[43mres_attention\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mres_attention\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpre_norm\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mpre_norm\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mstore_attn\\x1b[49m\\x1b[38;5;241;43m=\\x1b[39;49m\\x1b[43mstore_attn\\x1b[49m\\x1b[43m)\\x1b[49m\\n\\x1b[1;32m    361\\x1b[0m \\x1b[38;5;66;03m# Head\\x1b[39;00m\\n\\x1b[1;32m    362\\x1b[0m \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mhead_nf \\x1b[38;5;241m=\\x1b[39m d_model \\x1b[38;5;241m*\\x1b[39m patch_num\\n', 'File \\x1b[0;32m/usr/local/pip-global/tsai/models/PatchTST.py:287\\x1b[0m, in \\x1b[0;36m_TSTiEncoder.__init__\\x1b[0;34m(self, c_in, patch_num, patch_len, n_layers, d_model, n_heads, d_k, d_v, d_ff, norm, attn_dropout, dropout, act, store_attn, res_attention, pre_norm)\\x1b[0m\\n\\x1b[1;32m    284\\x1b[0m \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mseq_len \\x1b[38;5;241m=\\x1b[39m q_len\\n\\x1b[1;32m    286\\x1b[0m \\x1b[38;5;66;03m# Positional encoding\\x1b[39;00m\\n\\x1b[0;32m--> 287\\x1b[0m W_pos \\x1b[38;5;241m=\\x1b[39m \\x1b[43mtorch\\x1b[49m\\x1b[38;5;241;43m.\\x1b[39;49m\\x1b[43mempty\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43mq_len\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43md_model\\x1b[49m\\x1b[43m)\\x1b[49m\\x1b[43m)\\x1b[49m\\n\\x1b[1;32m    288\\x1b[0m nn\\x1b[38;5;241m.\\x1b[39minit\\x1b[38;5;241m.\\x1b[39muniform_(W_pos, \\x1b[38;5;241m-\\x1b[39m\\x1b[38;5;241m0.02\\x1b[39m, \\x1b[38;5;241m0.02\\x1b[39m)\\n\\x1b[1;32m    289\\x1b[0m \\x1b[38;5;28mself\\x1b[39m\\x1b[38;5;241m.\\x1b[39mW_pos \\x1b[38;5;241m=\\x1b[39m nn\\x1b[38;5;241m.\\x1b[39mParameter(W_pos)\\n', '\\x1b[0;31mRuntimeError\\x1b[0m: Trying to create tensor with negative dimension -1: [-1, 16]']).\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/pip-global/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_55034/931060581.py\", line 9, in objective\n",
      "    pm.execute_notebook(\n",
      "  File \"/usr/local/pip-global/papermill/execute.py\", line 128, in execute_notebook\n",
      "    raise_for_execution_errors(nb, output_path)\n",
      "  File \"/usr/local/pip-global/papermill/execute.py\", line 232, in raise_for_execution_errors\n",
      "    raise error\n",
      "papermill.exceptions.PapermillExecutionError: \n",
      "---------------------------------------------------------------------------\n",
      "Exception encountered at \"In [21]\":\n",
      "---------------------------------------------------------------------------\n",
      "RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[21], line 2\n",
      "      1 cbs = L(WandbCallback(log_preds=False)) if config.use_wandb else L()\n",
      "----> 2 learn = TSForecaster(X, y, splits=splits, batch_size=config.bs, \n",
      "      3                      pipelines=[preproc_pipe, exp_pipe], arch=\"PatchTST\", \n",
      "      4                      arch_config=dict(config.arch_config), \n",
      "      5                     cbs= cbs + ShowGraphCallback())\n",
      "      6 #lr_max = learn.lr_find().valley\n",
      "      7 lr_max = 1e-3\n",
      "\n",
      "File /usr/local/pip-global/tsai/tslearner.py:214, in TSForecaster.__init__(self, X, y, splits, tfms, inplace, sel_vars, sel_steps, weights, partial_n, train_metrics, valid_metrics, bs, batch_size, batch_tfms, pipelines, shuffle_train, drop_last, num_workers, do_setup, device, seed, arch, arch_config, pretrained, weights_path, exclude_head, cut, init, loss_func, opt_func, lr, metrics, cbs, wd, wd_bn_bias, train_bn, moms, path, model_dir, splitter, verbose)\n",
      "    206     elif isinstance(arch, str): arch = get_arch(arch)\n",
      "    207     # if 'xresnet' in arch.__name__.lower() and not '1d' in arch.__name__.lower():\n",
      "    208     #     model = build_tsimage_model(arch, dls=dls, pretrained=pretrained, init=init, device=device, verbose=verbose, arch_config=arch_config)\n",
      "    209     # elif 'tabularmodel' in arch.__name__.lower():\n",
      "   (...)\n",
      "    212     #     model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n",
      "    213     #                        exclude_head=exclude_head, cut=cut, init=init, arch_config=arch_config)\n",
      "--> 214     model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n",
      "    215                         exclude_head=exclude_head, cut=cut, init=init, arch_config=arch_config)\n",
      "    216 try:\n",
      "    217     setattr(model, \"__name__\", arch.__name__)\n",
      "\n",
      "File /usr/local/pip-global/tsai/models/utils.py:158, in build_ts_model(arch, c_in, c_out, seq_len, d, dls, device, verbose, pretrained, weights_path, exclude_head, cut, init, arch_config, **kwargs)\n",
      "    156 if 'ltsf_' in arch.__name__.lower() or 'patchtst' in arch.__name__.lower():\n",
      "    157     pv(f'arch: {arch.__name__}(c_in={c_in} c_out={c_out} seq_len={seq_len} pred_dim={d} arch_config={arch_config}, kwargs={kwargs})', verbose)\n",
      "--> 158     model = (arch(c_in=c_in, c_out=c_out, seq_len=seq_len, pred_dim=d, **arch_config, **kwargs)).to(device=device)\n",
      "    159 elif sum([1 for v in ['RNN_FCN', 'LSTM_FCN', 'RNNPlus', 'LSTMPlus', 'GRUPlus', 'InceptionTime', 'TSiT', 'Sequencer', 'XceptionTimePlus',\n",
      "    160                     'GRU_FCN', 'OmniScaleCNN', 'mWDN', 'TST', 'XCM', 'MLP', 'MiniRocket', 'InceptionRocket']\n",
      "    161         if v in arch.__name__]):\n",
      "    162     pv(f'arch: {arch.__name__}(c_in={c_in} c_out={c_out} seq_len={seq_len} arch_config={arch_config} kwargs={kwargs})', verbose)\n",
      "\n",
      "File /usr/local/pip-global/tsai/models/PatchTST.py:446, in PatchTST.__init__(self, c_in, c_out, seq_len, pred_dim, n_layers, n_heads, d_model, d_ff, dropout, attn_dropout, patch_len, stride, padding_patch, revin, affine, individual, subtract_last, decomposition, kernel_size, activation, norm, pre_norm, res_attention, store_attn)\n",
      "    444     self.patch_num = self.model_trend.patch_num\n",
      "    445 else:\n",
      "--> 446     self.model = _PatchTST_backbone(c_in=c_in, seq_len=seq_len, pred_dim=pred_dim, \n",
      "    447                                     patch_len=patch_len, stride=stride, n_layers=n_layers, d_model=d_model,\n",
      "    448                                     n_heads=n_heads, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n",
      "    449                                     dropout=dropout, act=activation, res_attention=res_attention, pre_norm=pre_norm, \n",
      "    450                                     store_attn=store_attn, padding_patch=padding_patch, \n",
      "    451                                     individual=individual, revin=revin, affine=affine, subtract_last=subtract_last)\n",
      "    452     self.patch_num = self.model.patch_num\n",
      "\n",
      "File /usr/local/pip-global/tsai/models/PatchTST.py:356, in _PatchTST_backbone.__init__(self, c_in, seq_len, pred_dim, patch_len, stride, n_layers, d_model, n_heads, d_k, d_v, d_ff, norm, attn_dropout, dropout, act, res_attention, pre_norm, store_attn, padding_patch, individual, revin, affine, subtract_last)\n",
      "    353 self.patch_len = patch_len\n",
      "    355 # Backbone \n",
      "--> 356 self.backbone = _TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len,\n",
      "    357                              n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
      "    358                              attn_dropout=attn_dropout, dropout=dropout, act=act,\n",
      "    359                              res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn)\n",
      "    361 # Head\n",
      "    362 self.head_nf = d_model * patch_num\n",
      "\n",
      "File /usr/local/pip-global/tsai/models/PatchTST.py:287, in _TSTiEncoder.__init__(self, c_in, patch_num, patch_len, n_layers, d_model, n_heads, d_k, d_v, d_ff, norm, attn_dropout, dropout, act, store_attn, res_attention, pre_norm)\n",
      "    284 self.seq_len = q_len\n",
      "    286 # Positional encoding\n",
      "--> 287 W_pos = torch.empty((q_len, d_model))\n",
      "    288 nn.init.uniform_(W_pos, -0.02, 0.02)\n",
      "    289 self.W_pos = nn.Parameter(W_pos)\n",
      "\n",
      "RuntimeError: Trying to create tensor with negative dimension -1: [-1, 16]\n",
      "\n",
      "\u001b[33m[W 2023-03-22 20:43:56,192]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "PapermillExecutionError",
     "evalue": "\n---------------------------------------------------------------------------\nException encountered at \"In [21]\":\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[21], line 2\n      1 cbs = L(WandbCallback(log_preds=False)) if config.use_wandb else L()\n----> 2 learn = TSForecaster(X, y, splits=splits, batch_size=config.bs, \n      3                      pipelines=[preproc_pipe, exp_pipe], arch=\"PatchTST\", \n      4                      arch_config=dict(config.arch_config), \n      5                     cbs= cbs + ShowGraphCallback())\n      6 #lr_max = learn.lr_find().valley\n      7 lr_max = 1e-3\n\nFile /usr/local/pip-global/tsai/tslearner.py:214, in TSForecaster.__init__(self, X, y, splits, tfms, inplace, sel_vars, sel_steps, weights, partial_n, train_metrics, valid_metrics, bs, batch_size, batch_tfms, pipelines, shuffle_train, drop_last, num_workers, do_setup, device, seed, arch, arch_config, pretrained, weights_path, exclude_head, cut, init, loss_func, opt_func, lr, metrics, cbs, wd, wd_bn_bias, train_bn, moms, path, model_dir, splitter, verbose)\n    206     elif isinstance(arch, str): arch = get_arch(arch)\n    207     # if 'xresnet' in arch.__name__.lower() and not '1d' in arch.__name__.lower():\n    208     #     model = build_tsimage_model(arch, dls=dls, pretrained=pretrained, init=init, device=device, verbose=verbose, arch_config=arch_config)\n    209     # elif 'tabularmodel' in arch.__name__.lower():\n   (...)\n    212     #     model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n    213     #                        exclude_head=exclude_head, cut=cut, init=init, arch_config=arch_config)\n--> 214     model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n    215                         exclude_head=exclude_head, cut=cut, init=init, arch_config=arch_config)\n    216 try:\n    217     setattr(model, \"__name__\", arch.__name__)\n\nFile /usr/local/pip-global/tsai/models/utils.py:158, in build_ts_model(arch, c_in, c_out, seq_len, d, dls, device, verbose, pretrained, weights_path, exclude_head, cut, init, arch_config, **kwargs)\n    156 if 'ltsf_' in arch.__name__.lower() or 'patchtst' in arch.__name__.lower():\n    157     pv(f'arch: {arch.__name__}(c_in={c_in} c_out={c_out} seq_len={seq_len} pred_dim={d} arch_config={arch_config}, kwargs={kwargs})', verbose)\n--> 158     model = (arch(c_in=c_in, c_out=c_out, seq_len=seq_len, pred_dim=d, **arch_config, **kwargs)).to(device=device)\n    159 elif sum([1 for v in ['RNN_FCN', 'LSTM_FCN', 'RNNPlus', 'LSTMPlus', 'GRUPlus', 'InceptionTime', 'TSiT', 'Sequencer', 'XceptionTimePlus',\n    160                     'GRU_FCN', 'OmniScaleCNN', 'mWDN', 'TST', 'XCM', 'MLP', 'MiniRocket', 'InceptionRocket']\n    161         if v in arch.__name__]):\n    162     pv(f'arch: {arch.__name__}(c_in={c_in} c_out={c_out} seq_len={seq_len} arch_config={arch_config} kwargs={kwargs})', verbose)\n\nFile /usr/local/pip-global/tsai/models/PatchTST.py:446, in PatchTST.__init__(self, c_in, c_out, seq_len, pred_dim, n_layers, n_heads, d_model, d_ff, dropout, attn_dropout, patch_len, stride, padding_patch, revin, affine, individual, subtract_last, decomposition, kernel_size, activation, norm, pre_norm, res_attention, store_attn)\n    444     self.patch_num = self.model_trend.patch_num\n    445 else:\n--> 446     self.model = _PatchTST_backbone(c_in=c_in, seq_len=seq_len, pred_dim=pred_dim, \n    447                                     patch_len=patch_len, stride=stride, n_layers=n_layers, d_model=d_model,\n    448                                     n_heads=n_heads, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n    449                                     dropout=dropout, act=activation, res_attention=res_attention, pre_norm=pre_norm, \n    450                                     store_attn=store_attn, padding_patch=padding_patch, \n    451                                     individual=individual, revin=revin, affine=affine, subtract_last=subtract_last)\n    452     self.patch_num = self.model.patch_num\n\nFile /usr/local/pip-global/tsai/models/PatchTST.py:356, in _PatchTST_backbone.__init__(self, c_in, seq_len, pred_dim, patch_len, stride, n_layers, d_model, n_heads, d_k, d_v, d_ff, norm, attn_dropout, dropout, act, res_attention, pre_norm, store_attn, padding_patch, individual, revin, affine, subtract_last)\n    353 self.patch_len = patch_len\n    355 # Backbone \n--> 356 self.backbone = _TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len,\n    357                              n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n    358                              attn_dropout=attn_dropout, dropout=dropout, act=act,\n    359                              res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn)\n    361 # Head\n    362 self.head_nf = d_model * patch_num\n\nFile /usr/local/pip-global/tsai/models/PatchTST.py:287, in _TSTiEncoder.__init__(self, c_in, patch_num, patch_len, n_layers, d_model, n_heads, d_k, d_v, d_ff, norm, attn_dropout, dropout, act, store_attn, res_attention, pre_norm)\n    284 self.seq_len = q_len\n    286 # Positional encoding\n--> 287 W_pos = torch.empty((q_len, d_model))\n    288 nn.init.uniform_(W_pos, -0.02, 0.02)\n    289 self.W_pos = nn.Parameter(W_pos)\n\nRuntimeError: Trying to create tensor with negative dimension -1: [-1, 16]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPapermillExecutionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_optuna_study(objective, study_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgridsearch\u001b[39;49m\u001b[39m'\u001b[39;49m, search_space\u001b[39m=\u001b[39;49msearch_space,\n\u001b[1;32m      2\u001b[0m                   direction\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mminimize\u001b[39;49m\u001b[39m'\u001b[39;49m, path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtmp\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/pip-global/tsai/optuna.py:80\u001b[0m, in \u001b[0;36mrun_optuna_study\u001b[0;34m(objective, resume, study_type, multivariate, search_space, evaluate, seed, sampler, pruner, study_name, direction, load_if_exists, n_trials, timeout, gc_after_trial, show_progress_bar, save_study, path, show_plots)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m evaluate: study\u001b[39m.\u001b[39menqueue_trial(evaluate)\n\u001b[1;32m     79\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mn_trials, timeout\u001b[39m=\u001b[39;49mtimeout, gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial, show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar)\n\u001b[1;32m     81\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/pip-global/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/pip-global/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/pip-global/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/pip-global/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/pip-global/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[79], line 9\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      7\u001b[0m stdout_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtmp/pm_stdout.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m stderr_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtmp/pm_stderr.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m pm\u001b[39m.\u001b[39;49mexecute_notebook(\n\u001b[1;32m     10\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m./training.ipynb\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m./tmp/pm_output.ipynb\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     parameters \u001b[39m=\u001b[39;49m {\n\u001b[1;32m     13\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mconfig.lookback\u001b[39;49m\u001b[39m'\u001b[39;49m: lookback\n\u001b[1;32m     14\u001b[0m     },\n\u001b[1;32m     15\u001b[0m     stdout_file \u001b[39m=\u001b[39;49m stdout_file,\n\u001b[1;32m     16\u001b[0m     stderr_file \u001b[39m=\u001b[39;49m stderr_file\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39m# Close the output files\u001b[39;00m\n\u001b[1;32m     20\u001b[0m stdout_file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/pip-global/papermill/execute.py:128\u001b[0m, in \u001b[0;36mexecute_notebook\u001b[0;34m(input_path, output_path, parameters, engine_name, request_save_on_cell_execute, prepare_only, kernel_name, language, progress_bar, log_output, stdout_file, stderr_file, start_timeout, report_mode, cwd, **engine_kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         nb \u001b[39m=\u001b[39m papermill_engines\u001b[39m.\u001b[39mexecute_notebook_with_engine(\n\u001b[1;32m    114\u001b[0m             engine_name,\n\u001b[1;32m    115\u001b[0m             nb,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mengine_kwargs\n\u001b[1;32m    125\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[39m# Check for errors first (it saves on error before raising)\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     raise_for_execution_errors(nb, output_path)\n\u001b[1;32m    130\u001b[0m \u001b[39m# Write final output in case the engine didn't write it on cell completion.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m write_ipynb(nb, output_path)\n",
      "File \u001b[0;32m/usr/local/pip-global/papermill/execute.py:232\u001b[0m, in \u001b[0;36mraise_for_execution_errors\u001b[0;34m(nb, output_path)\u001b[0m\n\u001b[1;32m    229\u001b[0m nb\u001b[39m.\u001b[39mcells\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, error_msg_cell)\n\u001b[1;32m    231\u001b[0m write_ipynb(nb, output_path)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[0;31mPapermillExecutionError\u001b[0m: \n---------------------------------------------------------------------------\nException encountered at \"In [21]\":\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[21], line 2\n      1 cbs = L(WandbCallback(log_preds=False)) if config.use_wandb else L()\n----> 2 learn = TSForecaster(X, y, splits=splits, batch_size=config.bs, \n      3                      pipelines=[preproc_pipe, exp_pipe], arch=\"PatchTST\", \n      4                      arch_config=dict(config.arch_config), \n      5                     cbs= cbs + ShowGraphCallback())\n      6 #lr_max = learn.lr_find().valley\n      7 lr_max = 1e-3\n\nFile /usr/local/pip-global/tsai/tslearner.py:214, in TSForecaster.__init__(self, X, y, splits, tfms, inplace, sel_vars, sel_steps, weights, partial_n, train_metrics, valid_metrics, bs, batch_size, batch_tfms, pipelines, shuffle_train, drop_last, num_workers, do_setup, device, seed, arch, arch_config, pretrained, weights_path, exclude_head, cut, init, loss_func, opt_func, lr, metrics, cbs, wd, wd_bn_bias, train_bn, moms, path, model_dir, splitter, verbose)\n    206     elif isinstance(arch, str): arch = get_arch(arch)\n    207     # if 'xresnet' in arch.__name__.lower() and not '1d' in arch.__name__.lower():\n    208     #     model = build_tsimage_model(arch, dls=dls, pretrained=pretrained, init=init, device=device, verbose=verbose, arch_config=arch_config)\n    209     # elif 'tabularmodel' in arch.__name__.lower():\n   (...)\n    212     #     model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n    213     #                        exclude_head=exclude_head, cut=cut, init=init, arch_config=arch_config)\n--> 214     model = build_ts_model(arch, dls=dls, device=device, verbose=verbose, pretrained=pretrained, weights_path=weights_path,\n    215                         exclude_head=exclude_head, cut=cut, init=init, arch_config=arch_config)\n    216 try:\n    217     setattr(model, \"__name__\", arch.__name__)\n\nFile /usr/local/pip-global/tsai/models/utils.py:158, in build_ts_model(arch, c_in, c_out, seq_len, d, dls, device, verbose, pretrained, weights_path, exclude_head, cut, init, arch_config, **kwargs)\n    156 if 'ltsf_' in arch.__name__.lower() or 'patchtst' in arch.__name__.lower():\n    157     pv(f'arch: {arch.__name__}(c_in={c_in} c_out={c_out} seq_len={seq_len} pred_dim={d} arch_config={arch_config}, kwargs={kwargs})', verbose)\n--> 158     model = (arch(c_in=c_in, c_out=c_out, seq_len=seq_len, pred_dim=d, **arch_config, **kwargs)).to(device=device)\n    159 elif sum([1 for v in ['RNN_FCN', 'LSTM_FCN', 'RNNPlus', 'LSTMPlus', 'GRUPlus', 'InceptionTime', 'TSiT', 'Sequencer', 'XceptionTimePlus',\n    160                     'GRU_FCN', 'OmniScaleCNN', 'mWDN', 'TST', 'XCM', 'MLP', 'MiniRocket', 'InceptionRocket']\n    161         if v in arch.__name__]):\n    162     pv(f'arch: {arch.__name__}(c_in={c_in} c_out={c_out} seq_len={seq_len} arch_config={arch_config} kwargs={kwargs})', verbose)\n\nFile /usr/local/pip-global/tsai/models/PatchTST.py:446, in PatchTST.__init__(self, c_in, c_out, seq_len, pred_dim, n_layers, n_heads, d_model, d_ff, dropout, attn_dropout, patch_len, stride, padding_patch, revin, affine, individual, subtract_last, decomposition, kernel_size, activation, norm, pre_norm, res_attention, store_attn)\n    444     self.patch_num = self.model_trend.patch_num\n    445 else:\n--> 446     self.model = _PatchTST_backbone(c_in=c_in, seq_len=seq_len, pred_dim=pred_dim, \n    447                                     patch_len=patch_len, stride=stride, n_layers=n_layers, d_model=d_model,\n    448                                     n_heads=n_heads, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout,\n    449                                     dropout=dropout, act=activation, res_attention=res_attention, pre_norm=pre_norm, \n    450                                     store_attn=store_attn, padding_patch=padding_patch, \n    451                                     individual=individual, revin=revin, affine=affine, subtract_last=subtract_last)\n    452     self.patch_num = self.model.patch_num\n\nFile /usr/local/pip-global/tsai/models/PatchTST.py:356, in _PatchTST_backbone.__init__(self, c_in, seq_len, pred_dim, patch_len, stride, n_layers, d_model, n_heads, d_k, d_v, d_ff, norm, attn_dropout, dropout, act, res_attention, pre_norm, store_attn, padding_patch, individual, revin, affine, subtract_last)\n    353 self.patch_len = patch_len\n    355 # Backbone \n--> 356 self.backbone = _TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len,\n    357                              n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n    358                              attn_dropout=attn_dropout, dropout=dropout, act=act,\n    359                              res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn)\n    361 # Head\n    362 self.head_nf = d_model * patch_num\n\nFile /usr/local/pip-global/tsai/models/PatchTST.py:287, in _TSTiEncoder.__init__(self, c_in, patch_num, patch_len, n_layers, d_model, n_heads, d_k, d_v, d_ff, norm, attn_dropout, dropout, act, store_attn, res_attention, pre_norm)\n    284 self.seq_len = q_len\n    286 # Positional encoding\n--> 287 W_pos = torch.empty((q_len, d_model))\n    288 nn.init.uniform_(W_pos, -0.02, 0.02)\n    289 self.W_pos = nn.Parameter(W_pos)\n\nRuntimeError: Trying to create tensor with negative dimension -1: [-1, 16]\n"
     ]
    }
   ],
   "source": [
    "run_optuna_study(objective, study_type='gridsearch', search_space=search_space,\n",
    "                  direction='minimize', path='tmp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
