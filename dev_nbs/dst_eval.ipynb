{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Dst forecasts\n",
    "\n",
    "> The baseline is the benchmark by Licata et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from tsai.basics import *\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from swdf.utils import *\n",
    "import wandb\n",
    "wandb_api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ARTIFACT_DOWNLOAD_PATH = Path(os.environ[\"WANDB_DIR\"])/\"wandb/artifacts/solfsmy_eval_tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{ 'dst_data_path': '../data/DST_IAGA2002.txt',\n",
       "  'learner_artifact': None,\n",
       "  'solfsmy_data_path': '../data/SOLFSMY.TXT'}\n",
       "```"
      ],
      "text/plain": [
       "{'dst_data_path': '../data/DST_IAGA2002.txt',\n",
       " 'learner_artifact': None,\n",
       " 'solfsmy_data_path': '../data/SOLFSMY.TXT'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config\n",
    "config = yaml2dict('config/dst.yaml')\n",
    "config = config.eval\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner loaded:\n",
      "path          = 'tmp'\n",
      "dls_fname     = '['dls_0.pth', 'dls_1.pth', 'dls_2.pth', 'dls_3.pth']'\n",
      "model_fname   = 'model.pth'\n",
      "learner_fname = 'learner.pkl'\n"
     ]
    }
   ],
   "source": [
    "if config.learner_artifact is None:\n",
    "    learner_path = 'tmp'\n",
    "else:\n",
    "    learner_path = wandb_api.artifact(config.learner_artifact).download(root=ARTIFACT_DOWNLOAD_PATH)\n",
    "learn = load_learner_all('tmp', verbose=True, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_preds.shape: (45888, 1, 144)\n"
     ]
    }
   ],
   "source": [
    "y_test_preds, y_test = learn.get_preds(ds_idx = 2, with_targs=True)\n",
    "y_test_preds = to_np(y_test_preds)\n",
    "y_test = to_np(y_test)\n",
    "print(f\"y_test_preds.shape: {y_test_preds.shape}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = y_test.shape[-1]\n",
    "data_columns_fcst = ['DST']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold classify the forecasts of Dst are based on the values of the Dst itself\n",
    "and the solar activity level, as defined by the F10.7 (according to Licata et al.).\n",
    "Therefore, below is the table of all possible combined conditions to evaluate Dst.\n",
    "\n",
    "| Dst Level | Solar Level | Dst Value         | Solar Value       |\n",
    "|-----------|-------------|-------------------|-------------------|\n",
    "| G0        | Low         | Dst ≥ -30         | F 10.7 ≤ 75       |\n",
    "| G0        | Moderate    | Dst ≥ -30         | 75 < F 10.7 ≤ 150 |\n",
    "| G0        | Elevated    | Dst ≥ -30         | 150 < F 10.7 ≤ 190|\n",
    "| G0        | High        | Dst ≥ -30         | F 10.7 > 190      |\n",
    "| G1        | Low         | -30 > Dst ≥ -50   | F 10.7 ≤ 75       |\n",
    "| G1        | Moderate    | -30 > Dst ≥ -50   | 75 < F 10.7 ≤ 150 |\n",
    "| G1        | Elevated    | -30 > Dst ≥ -50   | 150 < F 10.7 ≤ 190|\n",
    "| G1        | High        | -30 > Dst ≥ -50   | F 10.7 > 190      |\n",
    "| G2        | Low         | -50 > Dst ≥ -90   | F 10.7 ≤ 75       |\n",
    "| G2        | Moderate    | -50 > Dst ≥ -90   | 75 < F 10.7 ≤ 150 |\n",
    "| G2        | Elevated    | -50 > Dst ≥ -90   | 150 < F 10.7 ≤ 190|\n",
    "| G2        | High        | -50 > Dst ≥ -90   | F 10.7 > 190      |\n",
    "| G3        | Low         | -90 > Dst ≥ -130  | F 10.7 ≤ 75       |\n",
    "| G3        | Moderate    | -90 > Dst ≥ -130  | 75 < F 10.7 ≤ 150 |\n",
    "| G3        | Elevated    | -90 > Dst ≥ -130  | 150 < F 10.7 ≤ 190|\n",
    "| G3        | High        | -90 > Dst ≥ -130  | F 10.7 > 190      |\n",
    "| G4        | Low         | -130 > Dst ≥ -350 | F 10.7 ≤ 75       |\n",
    "| G4        | Moderate    | -130 > Dst ≥ -350 | 75 < F 10.7 ≤ 150 |\n",
    "| G4        | Elevated    | -130 > Dst ≥ -350 | 150 < F 10.7 ≤ 190|\n",
    "| G4        | High        | -130 > Dst ≥ -350 | F 10.7 > 190      |\n",
    "| G5        | Low         | Dst ≤ -350        | F 10.7 ≤ 75       |\n",
    "| G5        | Moderate    | Dst ≤ -350        | 75 < F 10.7 ≤ 150 |\n",
    "| G5        | Elevated    | Dst ≤ -350        | 150 < F 10.7 ≤ 190|\n",
    "| G5        | High        | Dst ≤ -350        | F 10.7 > 190      |\n",
    "\n",
    "\n",
    "In order to create all those combined conditions, let's merge the data from Dst\n",
    "and F10.7. Since this data is provided with different frequencies (1 day for F10.7 \n",
    "and 1 hour for Dst, we will fill the missing timestamps of F10.7 with forward-filling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65388/1854806815.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_dst_raw = pd.read_csv(config.dst_data_path, sep='\\s{2,}|\\s',\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>DST</th>\n",
       "      <th>F10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-01 00:00:00</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>72.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-01-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-01-01 02:00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>72.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-01-01 03:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>72.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-01-01 04:00:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>72.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  DST   F10\n",
       "0 1997-01-01 00:00:00 -5.0  72.4\n",
       "1 1997-01-01 01:00:00  0.0  72.4\n",
       "2 1997-01-01 02:00:00  3.0  72.4\n",
       "3 1997-01-01 03:00:00  6.0  72.4\n",
       "4 1997-01-01 04:00:00  9.0  72.4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll load again the data from Dst with datetimes, and \n",
    "# merge the DATE and TIME columns into a single datetime column\n",
    "df_dst_raw = pd.read_csv(config.dst_data_path, sep='\\s{2,}|\\s', \n",
    "                         names=['DATE', 'TIME', 'DOY', 'DST'])\n",
    "df_dst_raw['datetime'] = pd.to_datetime(df_dst_raw.DATE + ' ' + df_dst_raw.TIME)\n",
    "\n",
    "# We need to load the F10.7 data to classify the DST predictions\n",
    "df_solfsmy = pd.read_csv(config.solfsmy_data_path, delim_whitespace=True, comment='#', header=None, \n",
    "                 names=['Year', 'DDD', 'JulianDay', 'F10', 'F81c', 'S10', 'S81c', \n",
    "                        'M10', 'M81c', 'Y10', 'Y81c', 'Ssrc'])\n",
    "\n",
    "# # Convert the JulianDay column to a datetime column\n",
    "df_solfsmy['datetime'] = pd.to_datetime(df_solfsmy['JulianDay'], unit='D', origin='julian')\n",
    "# Shift the datetime from 12:00 to 00:00\n",
    "df_solfsmy['datetime'] = df_solfsmy['datetime'] - pd.Timedelta(hours=12)\n",
    "\n",
    "# Merge the DST and F10.7 dataframes on the datetime column, and keep only the columns we need\n",
    "df_combined = pd.merge(df_dst_raw, df_solfsmy, on='datetime', how='left')\n",
    "df_combined = df_combined[['datetime', 'DST', 'F10']]\n",
    "\n",
    "# The F10 is given only at 12:00 (00:00 now that we shifted), so we'll forward \n",
    "# fill the rest of the values\n",
    "df_combined['F10'] = df_combined['F10'].fillna(method='ffill')\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((230497, 2, 144), (230497, 2, 144))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined, y_combined = prepare_forecasting_data(df_combined, fcst_history=learn.dls.len, fcst_horizon=learn.dls.d[-1], \n",
    "                                x_vars=['DST', 'F10'], y_vars=['DST', 'F10'])\n",
    "X_combined.shape, y_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45888, 2, 144)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_combined_test = y_combined[learn.dls[2].splits]\n",
    "y_combined_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# The threshold classify the forecasts of Dst are based on the values of the Dst itself\n",
    "# and the solar activity level, as defined by the F10.7 (according to Licata et al.).\n",
    "# Therefore, below is the table of all possible combined conditions to evaluate Dst.\n",
    "\n",
    "# | Dst Level | Solar Level | Dst Value         | Solar Value       |\n",
    "# |-----------|-------------|-------------------|-------------------|\n",
    "# | G0        | Low         | Dst ≥ -30         | F 10.7 ≤ 75       |\n",
    "# | G0        | Moderate    | Dst ≥ -30         | 75 < F 10.7 ≤ 150 |\n",
    "# | G0        | Elevated    | Dst ≥ -30         | 150 < F 10.7 ≤ 190|\n",
    "# | G0        | High        | Dst ≥ -30         | F 10.7 > 190      |\n",
    "# | G1        | Low         | -30 > Dst ≥ -50   | F 10.7 ≤ 75       |\n",
    "# | G1        | Moderate    | -30 > Dst ≥ -50   | 75 < F 10.7 ≤ 150 |\n",
    "# | G1        | Elevated    | -30 > Dst ≥ -50   | 150 < F 10.7 ≤ 190|\n",
    "# | G1        | High        | -30 > Dst ≥ -50   | F 10.7 > 190      |\n",
    "# | G2        | Low         | -50 > Dst ≥ -90   | F 10.7 ≤ 75       |\n",
    "# | G2        | Moderate    | -50 > Dst ≥ -90   | 75 < F 10.7 ≤ 150 |\n",
    "# | G2        | Elevated    | -50 > Dst ≥ -90   | 150 < F 10.7 ≤ 190|\n",
    "# | G2        | High        | -50 > Dst ≥ -90   | F 10.7 > 190      |\n",
    "# | G3        | Low         | -90 > Dst ≥ -130  | F 10.7 ≤ 75       |\n",
    "# | G3        | Moderate    | -90 > Dst ≥ -130  | 75 < F 10.7 ≤ 150 |\n",
    "# | G3        | Elevated    | -90 > Dst ≥ -130  | 150 < F 10.7 ≤ 190|\n",
    "# | G3        | High        | -90 > Dst ≥ -130  | F 10.7 > 190      |\n",
    "# | G4        | Low         | -130 > Dst ≥ -350 | F 10.7 ≤ 75       |\n",
    "# | G4        | Moderate    | -130 > Dst ≥ -350 | 75 < F 10.7 ≤ 150 |\n",
    "# | G4        | Elevated    | -130 > Dst ≥ -350 | 150 < F 10.7 ≤ 190|\n",
    "# | G4        | High        | -130 > Dst ≥ -350 | F 10.7 > 190      |\n",
    "# | G5        | Low         | Dst ≤ -350        | F 10.7 ≤ 75       |\n",
    "# | G5        | Moderate    | Dst ≤ -350        | 75 < F 10.7 ≤ 150 |\n",
    "# | G5        | Elevated    | Dst ≤ -350        | 150 < F 10.7 ≤ 190|\n",
    "# | G5        | High        | Dst ≤ -350        | F 10.7 > 190      |\n",
    "\n",
    "\n",
    "# In order to create all those combined conditions, let's merge the data from Dst\n",
    "# and F10.7. Since this data is provided with different frequencies (1 day for F10.7 \n",
    "# and 1 hour for Dst, we will fill the missing timestamps of F10.7 with forward-filling)\n",
    "\n",
    "\n",
    "\n",
    "def split_data_by_dst_f107(data):\n",
    "    # function that splits the Dst data into all the possible Dst x F10.7 combinations\n",
    "    # according to the thresholds defined above. # The decision is made based on \n",
    "    # the timestemp of each sample\n",
    "    # The function returns a dictionary with the Dst x F10.7 combinations as keys\n",
    "    # and the corresponding Dst data as values.\n",
    "    # Input:\n",
    "    # data: Dst data and F10 data (numpy array of shape (n_samples, 2, n_timesteps))\n",
    "    # Output:\n",
    "    # data_split: dictionary with the Dst x F10.7 combinations as keys, and the\n",
    "    # corresponding Dst data as values.\n",
    "    data_split = {}\n",
    "    # Dst ≥ -30\n",
    "    data_split['G0_Low'] = data[(data[:, 0, 0] >= -30) & (data[:, 1, 0] <= 75)]\n",
    "    data_split['G0_Moderate'] = data[(data[:, 0, 0] >= -30) & (data[:, 1, 0] > 75) & (data[:, 1, 0] <= 150)]\n",
    "    data_split['G0_Elevated'] = data[(data[:, 0, 0] >= -30) & (data[:, 1, 0] > 150) & (data[:, 1, 0] <= 190)]\n",
    "    data_split['G0_High'] = data[(data[:, 0, 0] >= -30) & (data[:, 1, 0] > 190)]\n",
    "    # -30 > Dst ≥ -50\n",
    "    data_split['G1_Low'] = data[(data[:, 0, 0] < -30) & (data[:, 0, 0] >= -50) & (data[:, 1, 0] <= 75)]\n",
    "    data_split['G1_Moderate'] = data[(data[:, 0, 0] < -30) & (data[:, 0, 0] >= -50) & (data[:, 1, 0] > 75) & (data[:, 1, 0] <= 150)]\n",
    "    data_split['G1_Elevated'] = data[(data[:, 0, 0] < -30) & (data[:, 0, 0] >= -50) & (data[:, 1, 0] > 150) & (data[:, 1, 0] <= 190)]\n",
    "    data_split['G1_High'] = data[(data[:, 0, 0] < -30) & (data[:, 0, 0] >= -50) & (data[:, 1, 0] > 190)]\n",
    "    # -50 > Dst ≥ -90\n",
    "    data_split['G2_Low'] = data[(data[:, 0, 0] < -50) & (data[:, 0, 0] >= -90) & (data[:, 1, 0] <= 75)]\n",
    "    data_split['G2_Moderate'] = data[(data[:, 0, 0] < -50) & (data[:, 0, 0] >= -90) & (data[:, 1, 0] > 75) & (data[:, 1, 0] <= 150)]\n",
    "    data_split['G2_Elevated'] = data[(data[:, 0, 0] < -50) & (data[:, 0, 0] >= -90) & (data[:, 1, 0] > 150) & (data[:, 1, 0] <= 190)]\n",
    "    data_split['G2_High'] = data[(data[:, 0, 0] < -50) & (data[:, 0, 0] >= -90) & (data[:, 1, 0] > 190)]\n",
    "    # -90 > Dst ≥ -130\n",
    "    data_split['G3_Low'] = data[(data[:, 0, 0] < -90) & (data[:, 0, 0] >= -130) & (data[:, 1, 0] <= 75)]\n",
    "    data_split['G3_Moderate'] = data[(data[:, 0, 0] < -90) & (data[:, 0, 0] >= -130) & (data[:, 1, 0] > 75) & (data[:, 1, 0] <= 150)]\n",
    "    data_split['G3_Elevated'] = data[(data[:, 0, 0] < -90) & (data[:, 0, 0] >= -130) & (data[:, 1, 0] > 150) & (data[:, 1, 0] <= 190)]\n",
    "    data_split['G3_High'] = data[(data[:, 0, 0] < -90) & (data[:, 0, 0] >= -130) & (data[:, 1, 0] > 190)]\n",
    "    # -130 > Dst ≥ -350\n",
    "    data_split['G4_Low'] = data[(data[:, 0, 0] < -130) & (data[:, 0, 0] >= -350) & (data[:, 1, 0] <= 75)]\n",
    "    data_split['G4_Moderate'] = data[(data[:, 0, 0] < -130) & (data[:, 0, 0] >= -350) & (data[:, 1, 0] > 75) & (data[:, 1, 0] <= 150)]\n",
    "    data_split['G4_Elevated'] = data[(data[:, 0, 0] < -130) & (data[:, 0, 0] >= -350) & (data[:, 1, 0] > 150) & (data[:, 1, 0] <= 190)]\n",
    "    data_split['G4_High'] = data[(data[:, 0, 0] < -130) & (data[:, 0, 0] >= -350) & (data[:, 1, 0] > 190)]\n",
    "    # Dst < -350\n",
    "    data_split['G5_Low'] = data[(data[:, 0, 0] < -350) & (data[:, 1, 0] <= 75)]\n",
    "    data_split['G5_Moderate'] = data[(data[:, 0, 0] < -350) & (data[:, 1, 0] > 75) & (data[:, 1, 0] <= 150)]\n",
    "    data_split['G5_Elevated'] = data[(data[:, 0, 0] < -350) & (data[:, 1, 0] > 150) & (data[:, 1, 0] <= 190)]\n",
    "    data_split['G5_High'] = data[(data[:, 0, 0] < -350) & (data[:, 1, 0] > 190)]\n",
    "    return data_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0_Low (5667, 2, 144)\n",
      "G0_Moderate (29957, 2, 144)\n",
      "G0_Elevated (4248, 2, 144)\n",
      "G0_High (478, 2, 144)\n",
      "G1_Low (190, 2, 144)\n",
      "G1_Moderate (3240, 2, 144)\n",
      "G1_Elevated (544, 2, 144)\n",
      "G1_High (74, 2, 144)\n",
      "G2_Low (24, 2, 144)\n",
      "G2_Moderate (1064, 2, 144)\n",
      "G2_Elevated (164, 2, 144)\n",
      "G2_High (0, 2, 144)\n",
      "G3_Low (0, 2, 144)\n",
      "G3_Moderate (190, 2, 144)\n",
      "G3_Elevated (12, 2, 144)\n",
      "G3_High (0, 2, 144)\n",
      "G4_Low (0, 2, 144)\n",
      "G4_Moderate (36, 2, 144)\n",
      "G4_Elevated (0, 2, 144)\n",
      "G4_High (0, 2, 144)\n",
      "G5_Low (0, 2, 144)\n",
      "G5_Moderate (0, 2, 144)\n",
      "G5_Elevated (0, 2, 144)\n",
      "G5_High (0, 2, 144)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "y_combined_test_split = split_data_by_dst_f107(y_combined_test)\n",
    "\n",
    "# Check the shape of each key\n",
    "for key in y_combined_test_split.keys():\n",
    "    print(key, y_combined_test_split[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the first variable of each key (the Dst)\n",
    "y_test_split = {}\n",
    "for key in y_combined_test_split.keys():\n",
    "    y_test_split[key] = y_combined_test_split[key][:, 0, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
