{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draftsheet\n",
    "\n",
    "> First experiment and ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from tsai.basics import *\n",
    "my_setup(sklearn)\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = download_data('https://sol.spacenvironment.net/jb2008/indices/SOLFSMY.TXT')\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file into a pandas DataFrame, ignoring the lines starting with '#'\n",
    "# Column names: YYYY DDD   JulianDay  F10   F81c  S10   S81c  M10   M81c  Y10   Y81c  Ssrc\n",
    "df_raw = pd.read_csv(data_path, delim_whitespace=True, comment='#', header=None, \n",
    "                 names=['Year', 'DDD', 'JulianDay', 'F10', 'F81c', 'S10', 'S81c', \n",
    "                        'M10', 'M81c', 'Y10', 'Y81c', 'Ssrc'])\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F10, S10, M10, and Y10 (81c) have different observation and report times; to standardize reporting, all values are reported in sfu units at 12UT (Universal Time); observations are 3-times daily for F10 (20 UT used), every 5 minutes for S10 (daily average used), twice daily for M10 (7 and 16 UT), and every 1 minute for Y10 (Xrays are each minute while Lya is daily); \n",
    "\n",
    "For model inputs the values should be used as a daily value between 0-24 UT for a given calendar date; F10 and S10 are 1-day lagged, M10 is 2-day, and Y10 is 5-day lagged in JB2008; the 81-day centered values are used with the same respective lag times. Ssrc has 4 fields (1 for each index): \n",
    "\n",
    "*  0 = (F10, S10, M10, Y10) spline-filled if value or missing if no value; \n",
    "* 1 = (F10, M10, Y10) derived or measured index, (S10) SOHO/SEM; \n",
    "* 2 = (S10) TIMED/SEE v11; \n",
    "* 3 = (S10) SOHO gap (daily); \n",
    "* 4 = (S10) SOHO gap (average); \n",
    "* 5 = (F10) F10 mean (2 surrounding values), (S10) SDO/EVE; \n",
    "* 6 = (S10) GOES/EUVS fill-in, (M10) M10 mean (2 surrounding values); \n",
    "* 7 = (S10) S10 scaled to match M10 change from previous day; \n",
    "* 8 = (S10) SDO/EVE corrections and all S10 tweaked from sat 12388 delta B%, (Y10) UARS/SOLSTICE V18; \n",
    "* 9 = (S10) replace original v4.0h data for versions 4.0 and higher, (Y10) UARS/SOLSTICE v19; \n",
    "* A = (S10) TIMED/SEE solar minimum correction; \n",
    "* B = (S10) replace with original v4.0h S10 data for versions 4.0 and higher, (M10) SORCE/SOLSTICE/SIM v9; \n",
    "* C = (S10) SDO/EVE correction, (Y10) GOES XRS; \n",
    "* D = (S10) validated TIMED/SEE, (Y10) GOES XRS and SET composite LYA; \n",
    "* E = (S10) S10 composite, (Y10) SET composite LYA; \n",
    "* F = (F10, S10, M10, Y10) mean of bordering values\n",
    "\n",
    "Acronyms:\n",
    "* SOHO/SEM: Solar and Heliospheric Observatory/ Spacecraft's Solar Extreme-ultraviolet Monitor (SEM)\n",
    "* SDO/EVE: Solar Dynamics Observatory/Extreme Ultraviolet Variability Experiment.\n",
    "* UARS/SOLSTICE: Upper Atmosphere Research Satellite/Solar Stellar Irradiance Comparison Experiment\n",
    "* SORCE/SOLSTICE/SIM: Solar Radiation and Climate Experiment/SOLSTICE/Spectral Irradiance Monitor\n",
    "* GOES/XRS: Geostationary Operational Environmental Satellite/X-Ray Sensor\n",
    "* \"SET composite LYA\" refers to the solar irradiance in the Lyman-alpha (LyÎ±) wavelength range, as measured by the Solar EUV Experiment Telescope (SET) onboard the Solar Radiation and Climate Experiment (SORCE) spacecraft.\n",
    "\n",
    "This webpage contains forecasts (paid forecast) that we can use to compare to\n",
    "https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020SW002496. It's interesting\n",
    "to see what they forecast from the previous data in order to try the same thing \n",
    "with the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing values\n",
    "df_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the JulianDay column to a datetime column, and set it as index\n",
    "df_raw['Date'] = pd.to_datetime(df_raw['JulianDay'], unit='D', origin='julian')\n",
    "df_raw['Date'].head()\n",
    "df_raw.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct value of the column Ssrc\n",
    "df_raw.Ssrc.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the Ssrc columns into four colums, one for each character of the string,\n",
    "# The names of the new columns will be SsrcF10, SsrcS10, SsrcM10, and SsrcY10,\n",
    "# Cast the new columns into categories. Use a loop\n",
    "for i, c in enumerate('F10 S10 M10 Y10'.split()):\n",
    "    df_raw[f'Ssrc_{c}'] = df_raw['Ssrc'].str[i].astype('category')\n",
    "df_raw[['Ssrc_F10', 'Ssrc_S10', 'Ssrc_M10', 'Ssrc_Y10']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the categories of the column Ssrc_S10\n",
    "df_raw.Ssrc_S10.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variable S10. The color of the line will be determined by the value of Ssrc_S10\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax.scatter(df_raw.index, df_raw.S10, c=df_raw.Ssrc_S10.cat.codes, cmap='tab10', s=10)\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('S10')\n",
    "ax.set_title('S10 and Ssrc_S10')\n",
    "# TODO: Add a legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: How can we detected those anomalies between 1988 and 1999? Maybe the ones equal to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of values equlas to zero in S10\n",
    "print((df_raw.S10 == 0).sum())\n",
    "# convert them to NA\n",
    "df_raw.loc[df_raw.S10 == 0, 'S10'] = np.nan\n",
    "# plot the variable S10 again\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax.scatter(df_raw.index, df_raw.S10, c=df_raw.Ssrc_S10.cat.codes, cmap='tab10', s=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_col = 'Date'\n",
    "freq = '1D'\n",
    "data_columns = 'F10 S10 M10 Y10'.split()\n",
    "imputation_method = 'ffill'\n",
    "\n",
    "# sklearn's preprocessing pipeline\n",
    "preproc_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('shrinker', TSShrinkDataFrame()), # shrik dataframe memory usage and set the right dtypes\n",
    "    ('drop_duplicates', TSDropDuplicates(use_index=True)), # drop duplicates\n",
    "    ('fill_missing', TSFillMissing(columns=data_columns, method=imputation_method, value=None)), # fill missing data (1st ffill. 2nd value=0)\n",
    "], verbose=True)\n",
    "\n",
    "df = preproc_pipe.fit_transform(df_raw)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the paper by Licata et al. (2020) (https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020SW002496),\n",
    "# authors use a period from October 2012 through the end of 2018 for the benchmarking.\n",
    "# Therefore, we will set the test set as the same period for our analysis, \n",
    "# using the column Date as the timestamp, from October 2012 to the end of 2018. \n",
    "# Everything before the test set will be used for training, and everything after the test set\n",
    "# will be used for validation\n",
    "test_start_datetime = '2012-10-01'\n",
    "test_end_datetime = '2018-12-31'\n",
    "valid_start_datetime = '2018-01-01'\n",
    "\n",
    "\n",
    "# Plot the variables F10, S10, M10 and Y10, covering the different periods (training, test and validation)\n",
    "# with different colors. Do it for the 4 variables mentioned above \n",
    "fig, ax = plt.subplots(4, 1, figsize=(20, 10))\n",
    " \n",
    "for i, var in enumerate(['F10', 'S10', 'M10', 'Y10']):\n",
    "    ax[i].plot(df[var], label='train')\n",
    "    ax[i].plot(df[var][(df.index >= test_start_datetime) & (df.index <= test_end_datetime)],\n",
    "               label='test')\n",
    "    ax[i].plot(df[var][(df.index >= valid_start_datetime)], label='valid')\n",
    "    ax[i].set_title(var)\n",
    "    ax[i].legend()\n",
    "    ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) # format x-axis ticks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits: Since the validation period is after the test period in this use case, we cannot\n",
    "# use the default `get_forecasting_splits` from tsai. Instead, we will do manually\n",
    "# the validation splits, and use the funcion only for the test splits\n",
    "\n",
    "horizon = 6 # same as paper by Licata et al. (2020)\n",
    "lookback = 6*horizon # same as paper by Stevenson et al. (2021) \n",
    "val_idxs = L(df.reset_index()[df.index >= valid_start_datetime].index.tolist())\n",
    "splits_ = get_forecasting_splits(df[df.index < valid_start_datetime], \n",
    "                             fcst_history=lookback, \n",
    "                             fcst_horizon=horizon, \n",
    "                             use_index=True, \n",
    "                             test_cutoff_datetime=test_start_datetime, \n",
    "                             show_plot=False)\n",
    "splits = (splits_[0], val_idxs, splits_[1])\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where the index 9477 is in the splits tuple\n",
    "for i, s in enumerate(splits):\n",
    "    if 9477 in s:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined the splits for this particular experiment, we'll scaled\n",
    "# the data\n",
    "train_split = splits[0]\n",
    "exp_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('scaler', TSStandardScaler(columns=data_columns)),\n",
    "], verbose=True)\n",
    "save_object(exp_pipe, 'tmp/exp_pipe.pkl')\n",
    "exp_pipe = load_object('tmp/exp_pipe.pkl')\n",
    "\n",
    "df_scaled = exp_pipe.fit_transform(df.reset_index(), scaler__idxs = train_split)\n",
    "df_scaled.set_index(datetime_col, inplace=True)\n",
    "df_scaled.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Apply a sliding window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll approach the time series forecasting task as a supervised learning problem. \n",
    "# Remember that tsai requires that both inputs and outputs have the following shape:\n",
    "# (samples, features, steps)\n",
    "\n",
    "# To get those inputs and outputs we're going to use a function called \n",
    "# `prepare_forecasting_data`` that applies a sliding window along the dataframe:\n",
    "x_vars = data_columns\n",
    "y_vars = data_columns\n",
    "X, y = prepare_forecasting_data(df, fcst_history=lookback, fcst_horizon=horizon, \n",
    "                                x_vars=x_vars, y_vars=y_vars)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(splits[1]), max(splits[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_config = dict(\n",
    "    n_layers=3,  # number of encoder layers\n",
    "    n_heads=4,  # number of heads\n",
    "    d_model=16,  # dimension of model\n",
    "    d_ff=128,  # dimension of fully connected network\n",
    "    attn_dropout=0.0, # dropout applied to the attention weights\n",
    "    dropout=0.3,  # dropout applied to all linear layers in the encoder except q,k&v projections\n",
    "    patch_len=24,  # length of the patch applied to the time series to create patches\n",
    "    stride=2,  # stride used when creating patches\n",
    "    padding_patch=True,  # padding_patch\n",
    ")\n",
    "learn = TSForecaster(X, y, splits=splits, batch_size=16, path=\"models\", pipelines=[preproc_pipe, exp_pipe],\n",
    "                        arch=\"PatchTST\", arch_config=arch_config, metrics=[mse, mae], cbs=[ShowGraph()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
