{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os              : Linux-6.8.0-39-generic-x86_64-with-glibc2.31\n",
      "python          : 3.10.12\n",
      "tsai            : 0.3.10\n",
      "fastai          : 2.7.15\n",
      "fastcore        : 1.5.49\n",
      "sklearn         : 1.5.1\n",
      "torch           : 2.2.2+cu121\n",
      "device          : 1 gpu (['NVIDIA GeForce RTX 3070 Ti Laptop GPU'])\n",
      "cpu cores       : 14\n",
      "threads per cpu : 1\n",
      "RAM             : 15.28 GB\n",
      "GPU memory      : [8.0] GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from swdf.utils import *\n",
    "convert_uuids_to_indices()\n",
    "import sklearn\n",
    "from tsai.basics import *\n",
    "my_setup(sklearn)\n",
    "from matplotlib import dates as mdates\n",
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.progress import ShowGraphCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed for the MIT supercloud, to fix fastai's LRFinder error \n",
    "if torch.cuda.is_available() and torch.cuda.device_count() == 0:\n",
    "    from fastai.callback.schedule import LRFinder\n",
    "\n",
    "    @patch_to(LRFinder)\n",
    "    def after_fit(self):\n",
    "        self.learn.opt.zero_grad() # Needed before detaching the optimizer for future fits\n",
    "        tmp_f = self.path/self.model_dir/self.tmp_p/'_tmp.pth'\n",
    "        if tmp_f.exists():\n",
    "            self.learn.load(f'{self.tmp_p}/_tmp', with_opt=True, device='cpu')\n",
    "            self.tmp_d.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_base = yaml2dict('./config/base.yaml', attrdict=True)\n",
    "config_dstap = yaml2dict('./config/geodstap.yaml', attrdict=True)\n",
    "config_train = config_dstap.train\n",
    "config_data = config_dstap.data\n",
    "\n",
    "# Merge the two configs (the second one overrides the first one for any keys that are present in both)\n",
    "config = AttrDict({**config_base,\n",
    "                   **config_train, \n",
    "                   \"data\":AttrDict({**config_data})})\n",
    "\n",
    "# Add the architecture config\n",
    "if config.arch_name.lower() == 'patchtst':\n",
    "    config.arch = yaml2dict('./config/patchtst.yaml', attrdict=True)\n",
    "else:\n",
    "    config.arch = AttrDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=config.wandb.project, \n",
    "                 config=config,\n",
    "                 group=config.wandb.group,\n",
    "                 mode=config.wandb.mode, \n",
    "                 anonymous='never') if config.wandb.enabled else None\n",
    "config = dict2attrdict(run.config) if config.wandb.enabled else config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{ 'add_time_channels': False,\n",
       "  'arch': {'attn_dropout': 0.4, 'd_model': 1024, 'dropout': 0.2, 'n_heads': 32},\n",
       "  'arch_config_file': 'config/patchtst.yaml',\n",
       "  'arch_name': 'PatchTST',\n",
       "  'bs': 64,\n",
       "  'data': { 'ap_config': './config/geoap.yaml',\n",
       "            'data_nb': './geodstap_data.ipynb',\n",
       "            'df_save_path': './preprocessed_data/dataframes/geodstap.pkl',\n",
       "            'dst_config': './config/geodst.yaml',\n",
       "            'preproc_pipe_save_path': './preprocessed_data/pipelines/preproc_geodstap.pkl'},\n",
       "  'deltaHL': 4.0,\n",
       "  'horizon': 48,\n",
       "  'init_weights': False,\n",
       "  'is_optuna_study': False,\n",
       "  'lookback': 144,\n",
       "  'lr_max': None,\n",
       "  'n_epoch': 50,\n",
       "  'partial_n': 0.001,\n",
       "  'seed': 42,\n",
       "  'test_end_datetime': '2018-12-31 12:00:00',\n",
       "  'test_start_datetime': '2012-10-01 12:00:00',\n",
       "  'valid_start_datetime': '2018-01-01 12:00:00',\n",
       "  'wandb': { 'enabled': False,\n",
       "             'group': None,\n",
       "             'log_learner': True,\n",
       "             'mode': 'offline',\n",
       "             'project': 'swdf'}}\n",
       "```"
      ],
      "text/plain": [
       "{'arch_name': 'PatchTST',\n",
       " 'arch_config_file': 'config/patchtst.yaml',\n",
       " 'bs': 64,\n",
       " 'horizon': 48,\n",
       " 'init_weights': False,\n",
       " 'lookback': 144,\n",
       " 'lr_max': None,\n",
       " 'n_epoch': 50,\n",
       " 'partial_n': 0.001,\n",
       " 'seed': 42,\n",
       " 'test_start_datetime': '2012-10-01 12:00:00',\n",
       " 'test_end_datetime': '2018-12-31 12:00:00',\n",
       " 'valid_start_datetime': '2018-01-01 12:00:00',\n",
       " 'deltaHL': 4.0,\n",
       " 'is_optuna_study': False,\n",
       " 'wandb': {'enabled': False,\n",
       "  'log_learner': True,\n",
       "  'mode': 'offline',\n",
       "  'group': None,\n",
       "  'project': 'swdf'},\n",
       " 'add_time_channels': False,\n",
       " 'data': {'dst_config': './config/geodst.yaml',\n",
       "  'ap_config': './config/geoap.yaml',\n",
       "  'data_nb': './geodstap_data.ipynb',\n",
       "  'df_save_path': './preprocessed_data/dataframes/geodstap.pkl',\n",
       "  'preproc_pipe_save_path': './preprocessed_data/pipelines/preproc_geodstap.pkl'},\n",
       " 'arch': {'attn_dropout': 0.4, 'd_model': 1024, 'dropout': 0.2, 'n_heads': 32}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>DOY</th>\n",
       "      <th>DST</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1957-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1957-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>9.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1957-01-01 06:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1957-01-01 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1957-01-01 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Datetime  DOY   DST  AP\n",
       "0 1957-01-01 00:00:00    1  12.0   2\n",
       "1 1957-01-01 03:00:00    1   9.3   3\n",
       "2 1957-01-01 06:00:00    1   5.0   3\n",
       "3 1957-01-01 09:00:00    1  -5.0  22\n",
       "4 1957-01-01 12:00:00    1   0.0  15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, preproc_pipe = generate_preprocessed_data(config.data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the paper by Licata et al. (2020) (https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020SW002496),\n",
    "# authors use a period from October 2012 through the end of 2018 for the benchmarking.\n",
    "# Therefore, we will set the test set as the same period for our analysis, \n",
    "# using the column Date as the timestamp, from October 2012 to the end of 2018. \n",
    "# Everything before the test set will be used for training, and everything after the test set\n",
    "# will be used for validation\n",
    "test_start_datetime = config.test_start_datetime\n",
    "test_end_datetime = config.test_end_datetime\n",
    "valid_start_datetime = config.valid_start_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AP', 'DST']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_columns = ['AP', 'DST']\n",
    "data_columns_time = ['DOY']\n",
    "data_columns_fcst = data_columns + data_columns_time if config.add_time_channels else data_columns\n",
    "data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(20, 5))\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "for i, value in enumerate(data_columns): \n",
    "    ax[i].plot(df.Datetime, df[value])\n",
    "    ax[i].axvspan(df.Datetime.min(), test_start_datetime, alpha=0.5, color='green')\n",
    "    ax[i].axvspan(test_start_datetime, test_end_datetime, alpha=0.5, color='red')\n",
    "    ax[i].axvspan(test_end_datetime, df.Datetime.max(), alpha=0.5, color='blue')\n",
    "    ax[i].set_title(f'{value} - Train (green), Test (red), Validation (blue)')\n",
    "    ax[i].set_xlabel('Time')\n",
    "    ax[i].set_ylabel('AP (2nT)')\n",
    "    ax[i].xaxis.set_major_locator(mdates.YearLocator(2))\n",
    "    ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "    ax[i].xaxis.set_major_locator(mdates.YearLocator(2))\n",
    "    ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    plt.setp(ax[i].get_xticklabels(), rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#162710) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#19088) [178244,178245,178246,178247,178248,178249,178250,178251,178252,178253...],\n",
       " (#15296) [162757,162758,162759,162760,162761,162762,162763,162764,162765,162766...])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_ = get_forecasting_splits(df[df.Datetime < valid_start_datetime], \n",
    "                             fcst_history=config.lookback, \n",
    "                             fcst_horizon=config.horizon, \n",
    "                             use_index=False, \n",
    "                             test_cutoff_datetime=test_start_datetime, \n",
    "                             show_plot=False, \n",
    "                             datetime_col='Datetime')\n",
    "foo = df[df.Datetime >= valid_start_datetime]\n",
    "bar = get_forecasting_splits(foo, config.lookback, config.horizon, valid_size=0.0, \n",
    "                             test_size=0.0, show_plot=False)\n",
    "val_idxs = L(foo.index[bar[0]].tolist())\n",
    "\n",
    "splits = (splits_[0], val_idxs, splits_[1])\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp directory already exists.\n",
      "Pipeline saved as tmp/exp_pipe.pkl\n",
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>DOY</th>\n",
       "      <th>DST</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1957-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.156901</td>\n",
       "      <td>-0.584428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1957-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.043766</td>\n",
       "      <td>-0.534888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1957-01-01 06:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.863589</td>\n",
       "      <td>-0.534888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1957-01-01 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444572</td>\n",
       "      <td>0.406378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1957-01-01 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.654080</td>\n",
       "      <td>0.059596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Datetime  DOY       DST        AP\n",
       "0 1957-01-01 00:00:00    1  1.156901 -0.584428\n",
       "1 1957-01-01 03:00:00    1  1.043766 -0.534888\n",
       "2 1957-01-01 06:00:00    1  0.863589 -0.534888\n",
       "3 1957-01-01 09:00:00    1  0.444572  0.406378\n",
       "4 1957-01-01 12:00:00    1  0.654080  0.059596"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split = splits[0]\n",
    "exp_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('scaler', TSStandardScaler(columns=data_columns)),\n",
    "], verbose=True)\n",
    "save_object(exp_pipe, 'tmp/exp_pipe.pkl')\n",
    "exp_pipe = load_object('tmp/exp_pipe.pkl')\n",
    "\n",
    "df_scaled = exp_pipe.fit_transform(df.copy(), scaler__idxs = train_split)\n",
    "\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((197332, 2, 144), (197332, 2, 48))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll approach the time series forecasting task as a supervised learning problem. \n",
    "# Remember that tsai requires that both inputs and outputs have the following shape:\n",
    "# (samples, features, steps)\n",
    "\n",
    "# To get those inputs and outputs we're going to use a function called \n",
    "# `prepare_forecasting_data`` that applies a sliding window along the dataframe\n",
    "X, y = prepare_forecasting_data(df, fcst_history=config.lookback, \n",
    "                                fcst_horizon=config.horizon,\n",
    "                                x_vars=data_columns, \n",
    "                                y_vars=data_columns_fcst)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    \"AP\": [\n",
    "        (-np.inf, 10),  # Low\n",
    "        (10, 50),    # Moderate\n",
    "        (50, np.inf)   # Active\n",
    "    ],\n",
    "    \"DST\": [\n",
    "        (-30, np.inf),    # G0\n",
    "        (-50, -30),     # G1\n",
    "        (-90, -50),     # G2\n",
    "        (-130, -90),    # G3\n",
    "        (-350, -130),   # G4\n",
    "        (-np.inf, -350)    # G5\n",
    "    ]\n",
    "}\n",
    "\n",
    "activity_levels = {\n",
    "    'AP': [\"Low\", \"Moderate\", \"Active\"],\n",
    "    'DST': [\"G0\", \"G1\", \"G2\", \"G3\", \"G4\", \"G5\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AP': [0.05027553210414406, 0.08743208708643083, 0.862292380809425],\n",
       " 'DST': [8.803351903177007e-05,\n",
       "  0.0006020729557296344,\n",
       "  0.0014018781299297467,\n",
       "  0.007761028027872513,\n",
       "  0.01549121753377997,\n",
       "  0.9746557698336564]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat = get_classified_columns(df, thresholds, activity_levels)\n",
    "\n",
    "def get_weights(column:pd.Series):\n",
    "    distribution = column.value_counts(normalize=True).to_dict()\n",
    "    inverted_weights = {k: 1/v for k, v in distribution.items()}\n",
    "    total_weight = sum(inverted_weights.values())\n",
    "    weights = [v/total_weight for v in inverted_weights.values()]\n",
    "    return weights\n",
    "\n",
    "weights = {column: get_weights(df_cat[df.Datetime < test_start_datetime][f'{column}_Cat']) for column in data_columns}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp directory already exists.\n",
      "dict saved as tmp/weights_geodstap.pkl\n",
      "tmp directory already exists.\n",
      "dict saved as tmp/thresholds_geodstap.pkl\n"
     ]
    }
   ],
   "source": [
    "save_object(weights, 'tmp/weights_geodstap.pkl')\n",
    "save_object(thresholds, 'tmp/thresholds_geodstap.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swdf.losses import LossFactory\n",
    "from swdf.metrics import ValidationMetricsHandler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "loss = LossFactory(thresholds=thresholds, weights=weights).create(config.loss_func, **config.loss_config).to(device)\n",
    "\n",
    "metrics_handler = ValidationMetricsHandler(config.metrics)\n",
    "metrics = metrics_handler.get_metrics()\n",
    "\n",
    "wandb_callback = WandbCallback(log_preds=False)\n",
    "cbs = L(wandb_callback) if config.wandb.enabled else L()\n",
    "learn = TSForecaster(X, y, splits=splits, batch_size=config.bs,\n",
    "                     pipelines=[preproc_pipe, exp_pipe], \n",
    "                     arch=config.arch_name, \n",
    "                     metrics=metrics,\n",
    "                     loss_func=loss,\n",
    "                     arch_config=dict(config.arch), \n",
    "                     init=config.init_weights,\n",
    "                     cbs= cbs + ShowGraphCallback(), \n",
    "                     partial_n=config.partial_n)\n",
    "learn.to(device)\n",
    "\n",
    "try:\n",
    "    lr_max = learn.lr_find().valley if config.lr_max is None else config.lr_max\n",
    "except:\n",
    "    lr_max = 1e-3\n",
    "\n",
    "learn.fit_one_cycle(n_epoch=config.n_epoch, lr_max=config.lr_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8741647005081177\n",
      "Stored 'valid_loss' (float)\n"
     ]
    }
   ],
   "source": [
    "# Print the validation loss and save it in case other notebooks (optuna) wants to\n",
    "# use it for hyperparameter optimization\n",
    "valid_metrics = learn.recorder.metrics\n",
    "\n",
    "print(valid_metrics)\n",
    "%store valid_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "learn.dls.loaders += [learn.dls.valid.new_dl(X[splits[2]], y[splits[2]])] # Add test datalaoder\n",
    "\n",
    "# Remove the wandb callback to avoid errors when downloading the learner\n",
    "if config.wandb.enabled:\n",
    "    learn.remove_cb(wandb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'best_valid_loss' (float)\n",
      "Learner saved:\n",
      "path          = 'tmp'\n",
      "dls_fname     = '['dls_0.pth', 'dls_1.pth', 'dls_2.pth', 'dls_3.pth']'\n",
      "model_fname   = 'model_dstap.pth'\n",
      "learner_fname = 'learner_dstap.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_valid_metrics = None\n",
    "%store -r best_valid_metrics\n",
    "\n",
    "# save_all_or_best = (config.is_optuna_study and valid_loss < best_valid_loss) or not config.is_optuna_study\n",
    "save_all_or_best = (config.is_optuna_study and \n",
    "                    metrics_handler.are_best_values(config.main_metric, best_valid_metrics, valid_metrics)) or not config.is_optuna_study\n",
    "\n",
    "\n",
    "if save_all_or_best:\n",
    "    best_valid_metrics = valid_metrics\n",
    "    %store best_valid_metrics                                           \n",
    "\n",
    "    # Save locally and in wandb if online and enabled\n",
    "    learn.save_all(path='tmp', verbose=True) \n",
    "    if run is not None and config.wandb_mode and config.wandb_log_learner:\n",
    "        # Save the learner (all tmp/dls, tmp/model.pth, and tmp/learner.pkl). \n",
    "        run.log_artifact('tmp', type='learner', name='solfsmy')\n",
    "    \n",
    "    # Save the best model\n",
    "    model = {}\n",
    "    model['models'] = [learn.model]\n",
    "    model['lookback'] = config.lookback\n",
    "    model['horizon'] = config.horizon\n",
    "    model['data_columns'] = data_columns\n",
    "    model['thresholds'] = thresholds\n",
    "\n",
    "    torch.save(model, 'models/best/best_model_solfsmy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.188929319381714\n"
     ]
    }
   ],
   "source": [
    "# Log the test loss to wandb\n",
    "test_loss = learn.validate(ds_idx=2)[0]\n",
    "print(test_loss)\n",
    "if run is not None:\n",
    "    run.log(dict(test_loss=test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run is not None:\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
