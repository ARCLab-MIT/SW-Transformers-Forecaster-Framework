####################
## Data
####################
data:
    data_url: 'https://www-app3.gfz-potsdam.de/kp_index/Kp_ap_since_1932.txt'
    data_path: '../data/SOLAP.TXT'
    force_download: False
    data_nb: "./geoap_data.ipynb"
    df_save_path: "./preprocessed_data/dataframes/geoap.pkl"
    preproc_pipe_save_path: "./preprocessed_data/pipelines/preproc_geoap.pkl"
    exp_pipe_save_path: "./preprocessed_data/pipelines/exp_geoap.pkl"
    data_columns: ['AP']

####################
## Train
####################
train:
  n_epoch: 30  # Number of epochs to train for
  partial_n: .1 # null uses all training set, float in [0,1] uses a percentage, list filters valid too
  arch_name: "PatchTST" # See `all_arch_names` in tsasi
  arch_config_file: 'config/patchtst.yaml' # Absolute, or relative to the execution environment. Null for default arch config
  bs: 128 # Batch size
  horizon: 24 # same as paper by Licata et al. (2020)
  init_weights: False  # Kaiming init weights
  lookback: 96
  lr_max: null # Maximum learning rate. If none, it will be computed with fastai's LRFinder
  metrics_handler_path: null
  main_metric: default
  loss_func: wMSE

   # If you are using losses that require from aditional parameters, put them here. Those are:
   #  - Hubber and wHubber: delta
   #  - Quantile and wQuantile: quantile
   #  - Classification: primary_loss (plus all the parameters the primary loss may need), alpha
   #  - Trended: primary_loss 
  loss_config: {}

####################
## Eval
####################
eval:
  solap_data_path: '../data/SOLAP.TXT' # SAME USED TO TRAIN THE LEARNER!
  # Path to learner wandb artifact to load (project/entity/run/artifact:version)
  # If null, the learner in the local tmp folder will be used
  learner_artifact: null
  round_preds: True # Round predictions to the nearest integer
  # Path to the F10.7 data
  solfsmy_data_path: '../data/SOLFSMY.TXT'
  timestep: 3 # Data points every 3 hours
