arch_name: "PatchTST" # See `all_arch_names` in tsasi
arch_config_file: 'config/patchtst.yaml' # Absolute, or relative to the execution environment. Null for default arch config
bs: 32 # Batch size
horizon: 6 # same as paper by Licata et al. (2020)
init_weights: False  # Kaiming init weights
lookback: 36 
lr_max: 1.0e-6 # Maximum learning rate. If none, it will be computed with fastai's LRFinder
n_epoch: 200 # Number of epochs to train for
partial_n: .1 # null uses all training set, float in [0,1] uses a percentage, list filters valid too
seed: 42 # Random seed (null for random)
test_start_datetime: '2012-10-01 12:00:00' # Benchmark test set by Licata et al. (2020)
test_end_datetime: '2018-12-31 12:00:00' # Benchmark test set by Licata et al. (2020) 
valid_start_datetime: '2018-01-01 12:00:00' # Benchmark validation set by Licata et al. (2020)
deltaHL: 2. # Delta value used by Hubber loss. See `delta` in nbs.wHubberLoss.
is_optuna_study: False
wandb:
  enabled: False # To use it, the environment variable WANDB_API_KEY must be set
  log_learner: True # Log learner to wandb
  mode: 'offline' # 'online' or 'offline' for wandb
  group: null # Useful to group runs that belong to the same optuna study
  project: 'swdf' # Name of wandb project