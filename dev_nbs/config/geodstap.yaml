####################
## Data
####################
data:
  dst_config: "./config/geodst.yaml"
  ap_config: "./config/geoap.yaml"
  data_nb: "./geodstap_data.ipynb"
  df_save_path: "./preprocessed_data/dataframes/geodstap.pkl"
  preproc_pipe_save_path: "./preprocessed_data/pipelines/preproc_geodstap.pkl"
  exp_pipe_save_path: "./preprocessed_data/pipelines/exp_geodstap.pkl"
  data_columns: ['DST', 'AP']
  add_time_channels: false
 
####################
## Train
####################
train:
  n_epoch: 30  # Number of epochs to train for
  partial_n: .1 # null uses all training set, float in [0,1] uses a percentage, list filters valid too
  arch_name: "PatchTST" # See `all_arch_names` in tsasi
  arch_config_file: 'config/patchtst.yaml' # Absolute, or relative to the execution environment. Null for default arch config
  bs: 128 # Batch size
  horizon: 24 # same as paper by Licata et al. (2020)
  init_weights: False  # Kaiming init weights
  lookback: 96
  lr_max: null # Maximum learning rate. If none, it will be computed with fastai's LRFinder
  metrics_handler_path: null
  main_metric: default
  loss_func: wMSE

   # If you are using losses that require from aditional parameters, put them here. Those are:
   #  - Hubber and wHubber: delta
   #  - Quantile and wQuantile: quantile
   #  - Classification: primary_loss (plus all the parameters the primary loss may need), alpha
   #  - Trended: primary_loss 
  loss_config: {}




####################
## Eval
####################
eval:
  dst_data_path: '../data/DST_IAGA2002.txt' # SAME USED TO TRAIN THE LEARNER!
  # Path to learner wandb artifact to load (project/entity/run/artifact:version)
  # If null, the learner in the local tmp folder will be used
  learner_artifact: null
  round_preds: True # Round predictions to the nearest integer
  # Path to the F10.7 data
  solfsmy_data_path: '../data/SOLFSMY.TXT'