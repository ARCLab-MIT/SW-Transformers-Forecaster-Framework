{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed if the notebook is run in VSCode\n",
    "from IPython.display import clear_output, DisplayHandle\n",
    "def update_patch(self, obj):\n",
    "    clear_output(wait=True)\n",
    "    self.display(obj)\n",
    "DisplayHandle.update = update_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os              : Linux-6.8.0-39-generic-x86_64-with-glibc2.31\n",
      "python          : 3.10.12\n",
      "tsai            : 0.3.10\n",
      "fastai          : 2.7.15\n",
      "fastcore        : 1.5.49\n",
      "sklearn         : 1.5.1\n",
      "torch           : 2.2.2+cu121\n",
      "device          : 1 gpu (['NVIDIA GeForce RTX 3070 Ti Laptop GPU'])\n",
      "cpu cores       : 14\n",
      "threads per cpu : 1\n",
      "RAM             : 15.28 GB\n",
      "GPU memory      : [8.0] GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import sklearn\n",
    "from tsai.basics import *\n",
    "from swdf.utils import *\n",
    "my_setup(sklearn)\n",
    "from matplotlib import dates as mdates\n",
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.progress import ShowGraphCallback\n",
    "from itertools import chain\n",
    "import more_itertools as mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed for the MIT supercloud, to fix fastai's LRFinder error \n",
    "if torch.cuda.is_available() and torch.cuda.device_count() == 0:\n",
    "    from fastai.callback.schedule import LRFinder\n",
    "\n",
    "    @patch_to(LRFinder)\n",
    "    def after_fit(self):\n",
    "        self.learn.opt.zero_grad() # Needed before detaching the optimizer for future fits\n",
    "        tmp_f = self.path/self.model_dir/self.tmp_p/'_tmp.pth'\n",
    "        if tmp_f.exists():\n",
    "            self.learn.load(f'{self.tmp_p}/_tmp', with_opt=True, device='cpu')\n",
    "            self.tmp_d.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting solar drivers F10, S10, M10 and Y10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hints about hyperparameters:\n",
    "- According to the authors of PatchTST: \"The ideal patch length may depend on the dataset, \n",
    "but P between {8, 16} seems to be general good numbers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_base = yaml2dict('./config/base.yaml', attrdict=True)\n",
    "config_solfsmy = yaml2dict('./config/solfsmy.yaml', attrdict=True)\n",
    "config_train = config_solfsmy.train\n",
    "config_data = config_solfsmy.data \n",
    "\n",
    "# Merge the two configs (the second one overrides the first one for any keys that are present in both)\n",
    "config = AttrDict({**config_base,\n",
    "                   **config_train, \n",
    "                   \"data\":AttrDict({**config_data})})\n",
    "\n",
    "# Add the architecture config\n",
    "if config.arch_name.lower() == 'patchtst':\n",
    "    config.arch = yaml2dict('./config/patchtst.yaml', attrdict=True)\n",
    "else:\n",
    "    config.arch = AttrDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=config.wandb.project, \n",
    "                 config=config,\n",
    "                 group=config.wandb.group,\n",
    "                 mode=config.wandb.mode, \n",
    "                 anonymous='never') if config.wandb.enabled else None\n",
    "config = dict2attrdict(run.config) if config.wandb.enabled else config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{ 'arch': {'attn_dropout': 0.4, 'd_model': 1024, 'dropout': 0.2, 'n_heads': 32},\n",
       "  'arch_config_file': 'config/patchtst.yaml',\n",
       "  'arch_name': 'PatchTST',\n",
       "  'bs': 32,\n",
       "  'data': { 'add_time_channels': False,\n",
       "            'data_columns': ['F10', 'S10', 'M10', 'Y10'],\n",
       "            'data_nb': './solfsmy_data.ipynb',\n",
       "            'data_path': '../data/SOLFSMY.TXT',\n",
       "            'data_time_columns': ['Year', 'DDD'],\n",
       "            'data_url': 'https://sol.spacenvironment.net/JB2008/indices/SOLFSMY.TXT',\n",
       "            'df_save_path': './preprocessed_data/dataframes/solfsmy.pkl',\n",
       "            'force_download': False,\n",
       "            'preproc_pipe_save_path': './preprocessed_data/pipelines/preproc_solfsmy.pkl'},\n",
       "  'deltaHL': 4.0,\n",
       "  'horizon': 6,\n",
       "  'init_weights': False,\n",
       "  'is_optuna_study': False,\n",
       "  'lookback': 36,\n",
       "  'lr_max': None,\n",
       "  'n_epoch': 50,\n",
       "  'partial_n': 0.1,\n",
       "  'seed': 42,\n",
       "  'test_end_datetime': '2018-12-31 12:00:00',\n",
       "  'test_start_datetime': '2012-10-01 12:00:00',\n",
       "  'valid_start_datetime': '2018-01-01 12:00:00',\n",
       "  'wandb': { 'enabled': False,\n",
       "             'group': None,\n",
       "             'log_learner': True,\n",
       "             'mode': 'offline',\n",
       "             'project': 'swdf'}}\n",
       "```"
      ],
      "text/plain": [
       "{'arch_name': 'PatchTST',\n",
       " 'arch_config_file': 'config/patchtst.yaml',\n",
       " 'bs': 32,\n",
       " 'horizon': 6,\n",
       " 'init_weights': False,\n",
       " 'lookback': 36,\n",
       " 'lr_max': None,\n",
       " 'n_epoch': 50,\n",
       " 'partial_n': 0.1,\n",
       " 'seed': 42,\n",
       " 'test_start_datetime': '2012-10-01 12:00:00',\n",
       " 'test_end_datetime': '2018-12-31 12:00:00',\n",
       " 'valid_start_datetime': '2018-01-01 12:00:00',\n",
       " 'deltaHL': 4.0,\n",
       " 'is_optuna_study': False,\n",
       " 'wandb': {'enabled': False,\n",
       "  'log_learner': True,\n",
       "  'mode': 'offline',\n",
       "  'group': None,\n",
       "  'project': 'swdf'},\n",
       " 'data': {'data_url': 'https://sol.spacenvironment.net/JB2008/indices/SOLFSMY.TXT',\n",
       "  'data_path': '../data/SOLFSMY.TXT',\n",
       "  'force_download': False,\n",
       "  'data_nb': './solfsmy_data.ipynb',\n",
       "  'df_save_path': './preprocessed_data/dataframes/solfsmy.pkl',\n",
       "  'preproc_pipe_save_path': './preprocessed_data/pipelines/preproc_solfsmy.pkl',\n",
       "  'data_columns': ['F10', 'S10', 'M10', 'Y10'],\n",
       "  'data_time_columns': ['Year', 'DDD'],\n",
       "  'add_time_channels': False},\n",
       " 'arch': {'attn_dropout': 0.4, 'd_model': 1024, 'dropout': 0.2, 'n_heads': 32}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>DDD</th>\n",
       "      <th>JulianDay</th>\n",
       "      <th>F10</th>\n",
       "      <th>F81c</th>\n",
       "      <th>S10</th>\n",
       "      <th>S81c</th>\n",
       "      <th>M10</th>\n",
       "      <th>M81c</th>\n",
       "      <th>Y10</th>\n",
       "      <th>Y81c</th>\n",
       "      <th>Ssrc</th>\n",
       "      <th>Ssrc_F10</th>\n",
       "      <th>Ssrc_S10</th>\n",
       "      <th>Ssrc_M10</th>\n",
       "      <th>Ssrc_Y10</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>1</td>\n",
       "      <td>2450450.0</td>\n",
       "      <td>72.400002</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>79.199997</td>\n",
       "      <td>65.400002</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>61.900002</td>\n",
       "      <td>70.699997</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>2</td>\n",
       "      <td>2450451.0</td>\n",
       "      <td>72.099998</td>\n",
       "      <td>77.900002</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>66.900002</td>\n",
       "      <td>73.699997</td>\n",
       "      <td>63.400002</td>\n",
       "      <td>70.400002</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>2450452.0</td>\n",
       "      <td>73.300003</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>70.099998</td>\n",
       "      <td>73.599998</td>\n",
       "      <td>64.900002</td>\n",
       "      <td>70.199997</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>4</td>\n",
       "      <td>2450453.0</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>77.300003</td>\n",
       "      <td>76.300003</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>71.599998</td>\n",
       "      <td>73.400002</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>69.800003</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-04 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>5</td>\n",
       "      <td>2450454.0</td>\n",
       "      <td>74.400002</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>78.900002</td>\n",
       "      <td>72.400002</td>\n",
       "      <td>73.300003</td>\n",
       "      <td>66.699997</td>\n",
       "      <td>69.599998</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-05 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  DDD  JulianDay        F10       F81c        S10       S81c  \\\n",
       "0  1997    1  2450450.0  72.400002  78.000000  74.000000  79.199997   \n",
       "1  1997    2  2450451.0  72.099998  77.900002  73.800003  79.099998   \n",
       "2  1997    3  2450452.0  73.300003  77.599998  75.000000  79.099998   \n",
       "3  1997    4  2450453.0  73.800003  77.300003  76.300003  79.000000   \n",
       "4  1997    5  2450454.0  74.400002  76.900002  77.599998  78.900002   \n",
       "\n",
       "         M10       M81c        Y10       Y81c  Ssrc Ssrc_F10 Ssrc_S10  \\\n",
       "0  65.400002  73.800003  61.900002  70.699997  1B11        1        B   \n",
       "1  66.900002  73.699997  63.400002  70.400002  1B11        1        B   \n",
       "2  70.099998  73.599998  64.900002  70.199997  1B11        1        B   \n",
       "3  71.599998  73.400002  65.500000  69.800003  1B11        1        B   \n",
       "4  72.400002  73.300003  66.699997  69.599998  1B11        1        B   \n",
       "\n",
       "  Ssrc_M10 Ssrc_Y10                Date  \n",
       "0        1        1 1997-01-01 12:00:00  \n",
       "1        1        1 1997-01-02 12:00:00  \n",
       "2        1        1 1997-01-03 12:00:00  \n",
       "3        1        1 1997-01-04 12:00:00  \n",
       "4        1        1 1997-01-05 12:00:00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, preproc_pipe = generate_preprocessed_data(config.data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper by [Licata et al. (2020)]((https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020SW002496)), authors use a period from October 2012 through the end of 2018 for the benchmarking. Therefore, we will set the test set as the same period for our analysis, using the column Date as the timestamp, from October 2012 to the end of 2018. Everything before the test set will be used for training, and everything after the test set will be used for validation. In this paper, the authors also present some the thresholds for categorizing solar activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_datetime = config.test_start_datetime\n",
    "test_end_datetime = config.test_end_datetime\n",
    "\n",
    "thresholds = {\n",
    "    'F10': [(0,75), (76,150), (151,190), (191, np.inf)],\n",
    "    'S10': [(0,65), (66,150), (151,215), (216, np.inf)],\n",
    "    'M10': [(0,72), (73,144), (145,167), (168, np.inf)],\n",
    "    'Y10': [(0,81), (82,148), (149,165), (166, np.inf)]\n",
    "}\n",
    "\n",
    "solact_levels = {\n",
    "    'F10': ['low', 'moderate', 'elevated', 'high'] ,\n",
    "    'S10': ['low', 'moderate', 'elevated', 'high'] ,\n",
    "    'M10': ['low', 'moderate', 'elevated', 'high'] ,\n",
    "    'Y10': ['low', 'moderate', 'elevated', 'high'] \n",
    "}\n",
    "\n",
    "dict2yaml(thresholds, \"distributions/thresholds.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moderate': 0.5065749000307598,\n",
       " 'low': 0.18390495232236234,\n",
       " 'high': 0.16145032297754536,\n",
       " 'elevated': 0.14806982466933252}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_F10 = load_object(\"./preprocessed_data/dataframes/F10_historical.pkl\")\n",
    "\n",
    "df_F107_cat = get_classified_columns(df_F10, thresholds, activity_levels={'F10': solact_levels['F10']})['F10_Cat']\n",
    "historical_distribution = df_F107_cat.value_counts(normalize=True).to_dict()\n",
    "\n",
    "historical_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 0.22903290529695025, Validation size: 0.19456083279115158, Train size: 0.6209670947030498\n",
      "Total number of segments:30, Number of segments for validation: 5 (16.67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "142506it [00:01, 71752.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest group of segments to F10.7 categories has an euclidean distance of 0.005517135807631744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_period = (df.Date >= test_start_datetime) & (df.Date <= test_end_datetime)\n",
    "\n",
    "df_cat = get_classified_columns(df, thresholds, solact_levels)\n",
    "df_cat = df_cat[~(test_period)]\n",
    "\n",
    "# Function parameter calculation\n",
    "test_size = df[test_period].shape[0] / df.shape[0]\n",
    "train_val_size = 1 - test_size\n",
    "val_size = 0.15 / train_val_size\n",
    "print(f\"Test size: {test_size}, Validation size: {val_size}, Train size: {1 - test_size - 0.15}\")\n",
    "\n",
    "# Best segment size found: 250, val_size: 0.15, test_size: 0.65\n",
    "best_comb, segments, distribution = find_closest_distribution(df_cat['F10_Cat'], historical_distribution, 250, val_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/data directory already exists.\n",
      "tuple saved as tmp/data/best_comb.pkl\n",
      "tmp/data directory already exists.\n",
      "list saved as tmp/data/segments.pkl\n"
     ]
    }
   ],
   "source": [
    "save_object(best_comb, \"tmp/data/best_comb.pkl\")\n",
    "save_object(segments, \"tmp/data/segments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'moderate': 0.5067145534041224,\n",
       " 'low': 0.28856964397251716,\n",
       " 'elevated': 0.1316364772017489,\n",
       " 'high': 0.0730793254216115}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_comb_idxs = [segments[i] for i in best_comb]\n",
    "validation_segments = (df.index.isin(chain.from_iterable(best_comb_idxs)))\n",
    "\n",
    "train_df = df[~validation_segments & ~test_period] \n",
    "train_distribution = get_classified_columns(train_df, thresholds, solact_levels)['F10_Cat'].value_counts(normalize=True).to_dict()\n",
    "\n",
    "train_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# Plot of the distribution found\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(distribution.keys(), distribution.values(), alpha=0.8, label='Validation Distribution')\n",
    "plt.bar(historical_distribution.keys(), historical_distribution.values(), alpha=0.5, label='Historical Distribution')\n",
    "plt.bar(train_distribution.keys(), train_distribution.values(), alpha=0.5, label='Train Distribution')\n",
    "\n",
    "plt.xlabel('Solar activity categories')\n",
    "plt.ylabel('Percentage of data')\n",
    "plt.title('Solar Activity Categories Distribution Comparison of F10.7')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = df.loc[chain.from_iterable(best_comb_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches as mpatches\n",
    "fig, ax = plt.subplots(4, 1, figsize=(20, 10))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "var_name = ['F10.7', 'S10.7', 'M10.7', 'Y10.7']\n",
    "\n",
    "for i, var in enumerate(['F10', 'S10', 'M10', 'Y10']):\n",
    "    ax[i].scatter(train_df.Date, train_df[var], label='Train', s=3)  # s parameter sets the size of the points\n",
    "    ax[i].scatter(df.Date[(df.Date >= test_start_datetime) & (df.Date <= test_end_datetime)],\n",
    "                  df[var][(df.Date >= test_start_datetime) & (df.Date <= test_end_datetime)],\n",
    "                  label='Test', s=3)\n",
    "    ax[i].scatter(validation.Date,\n",
    "                  validation[var],\n",
    "                  label='Validation', s=3)\n",
    "    ax[i].set_title(var_name[i])\n",
    "    ax[i].set_ylabel('SFU')  # Add y-axis label with SFU\n",
    "    ax[i].legend()\n",
    "\n",
    "    # Format x-axis to show only the years\n",
    "    ax[i].xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax[i].xaxis.set_minor_locator(mdates.MonthLocator())  # optional, for minor ticks\n",
    "\n",
    "    # Ensure all years are shown on the x-axis\n",
    "    ax[i].set_xlim([df.Date.min(), df.Date.max()])\n",
    "\n",
    "    set_ranges = np.zeros((int(df[var].max()), df.shape[0], 4))  # (Height, Width)\n",
    "\n",
    "    for j, (start, end) in enumerate(thresholds[var]):\n",
    "        if end == np.inf:\n",
    "            end = 9999\n",
    "        set_ranges[int(start):int(end), :, :] = [0, 0, 0, ((j + 1) / 4)]\n",
    "   \n",
    "    ax[i].imshow(set_ranges, extent=[df.Date.min(), df.Date.max(), 0, df[var].max()], aspect='auto', alpha=0.5, origin='lower')\n",
    "\n",
    "# Optionally, you can set a common x-axis label for all subplots if desired\n",
    "plt.xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((#6158) [1076,1077,1078,1079,1080,1081,1082,1083,1084,1085...],\n",
       "  (#1076) [0,1,2,3,4,5,6,7,8,9...],\n",
       "  (#2242) [7234,7235,7236,7237,7238,7239,7240,7241,7242,7243...]),\n",
       " (9476, 4, 36),\n",
       " (9476, 4, 6))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(0,len(segments))\n",
    "train_comb = list(np.setdiff1d(a, best_comb))\n",
    "data_columns = config.data.data_columns\n",
    "\n",
    "X_val, y_val, split_val = sliding_window_generator(df, 0, data_columns=data_columns, config=config, comb=best_comb, segments=segments)\n",
    "X_train, y_train, split_train = sliding_window_generator(df, split_val[-1]+1, data_columns=data_columns, config=config, comb=train_comb, segments=segments)\n",
    "X_test, y_test, split_test = sliding_window_generator(df[test_period], split_train[-1]+1, data_columns=data_columns, config=config) \n",
    "\n",
    "X = np.concatenate([X_val, X_train, X_test])\n",
    "y = np.concatenate([y_val, y_train, y_test])\n",
    "\n",
    "splits = (split_train, split_val, split_test)\n",
    "splits, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............ (step 1 of 1) Processing scaler, total=   0.0s\n",
      "tmp/pipes directory already exists.\n",
      "Pipeline saved as tmp/pipes/exp_pipe.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>DDD</th>\n",
       "      <th>JulianDay</th>\n",
       "      <th>F10</th>\n",
       "      <th>F81c</th>\n",
       "      <th>S10</th>\n",
       "      <th>S81c</th>\n",
       "      <th>M10</th>\n",
       "      <th>M81c</th>\n",
       "      <th>Y10</th>\n",
       "      <th>Y81c</th>\n",
       "      <th>Ssrc</th>\n",
       "      <th>Ssrc_F10</th>\n",
       "      <th>Ssrc_S10</th>\n",
       "      <th>Ssrc_M10</th>\n",
       "      <th>Ssrc_Y10</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>1</td>\n",
       "      <td>2450450.0</td>\n",
       "      <td>72.400002</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>79.199997</td>\n",
       "      <td>65.400002</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>61.900002</td>\n",
       "      <td>70.699997</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>2</td>\n",
       "      <td>2450451.0</td>\n",
       "      <td>72.099998</td>\n",
       "      <td>77.900002</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>66.900002</td>\n",
       "      <td>73.699997</td>\n",
       "      <td>63.400002</td>\n",
       "      <td>70.400002</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>2450452.0</td>\n",
       "      <td>73.300003</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>70.099998</td>\n",
       "      <td>73.599998</td>\n",
       "      <td>64.900002</td>\n",
       "      <td>70.199997</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>4</td>\n",
       "      <td>2450453.0</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>77.300003</td>\n",
       "      <td>76.300003</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>71.599998</td>\n",
       "      <td>73.400002</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>69.800003</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-04 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>5</td>\n",
       "      <td>2450454.0</td>\n",
       "      <td>74.400002</td>\n",
       "      <td>76.900002</td>\n",
       "      <td>77.599998</td>\n",
       "      <td>78.900002</td>\n",
       "      <td>72.400002</td>\n",
       "      <td>73.300003</td>\n",
       "      <td>66.699997</td>\n",
       "      <td>69.599998</td>\n",
       "      <td>1B11</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-01-05 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  DDD  JulianDay        F10       F81c        S10       S81c  \\\n",
       "0  1997    1  2450450.0  72.400002  78.000000  74.000000  79.199997   \n",
       "1  1997    2  2450451.0  72.099998  77.900002  73.800003  79.099998   \n",
       "2  1997    3  2450452.0  73.300003  77.599998  75.000000  79.099998   \n",
       "3  1997    4  2450453.0  73.800003  77.300003  76.300003  79.000000   \n",
       "4  1997    5  2450454.0  74.400002  76.900002  77.599998  78.900002   \n",
       "\n",
       "         M10       M81c        Y10       Y81c  Ssrc Ssrc_F10 Ssrc_S10  \\\n",
       "0  65.400002  73.800003  61.900002  70.699997  1B11        1        B   \n",
       "1  66.900002  73.699997  63.400002  70.400002  1B11        1        B   \n",
       "2  70.099998  73.599998  64.900002  70.199997  1B11        1        B   \n",
       "3  71.599998  73.400002  65.500000  69.800003  1B11        1        B   \n",
       "4  72.400002  73.300003  66.699997  69.599998  1B11        1        B   \n",
       "\n",
       "  Ssrc_M10 Ssrc_Y10                Date  \n",
       "0        1        1 1997-01-01 12:00:00  \n",
       "1        1        1 1997-01-02 12:00:00  \n",
       "2        1        1 1997-01-03 12:00:00  \n",
       "3        1        1 1997-01-04 12:00:00  \n",
       "4        1        1 1997-01-05 12:00:00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have defined the splits for this particular experiment, we'll scale\n",
    "# the data\n",
    "exp_pipe = sklearn.pipeline.Pipeline([\n",
    "    ('scaler', TSStandardScaler(columns=data_columns)),\n",
    "], verbose=True)\n",
    "\n",
    "df_scaled = exp_pipe.fit_transform(df.copy())\n",
    "\n",
    "save_object(exp_pipe, 'tmp/pipes/exp_pipe.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Apply a sliding window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'All': [0.49933374244488,\n",
       "   0.0006662575551199645,\n",
       "   0.07840012484165651,\n",
       "   0.4215998751583436]},\n",
       " {'moderate': 0.0006662575551199645,\n",
       "  'low': 0.49933374244488,\n",
       "  'elevated': 0.07840012484165651,\n",
       "  'high': 0.4215998751583436})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {k: np.abs(train_distribution.get(k, 0) - historical_distribution.get(k, 0)) for k in set(train_distribution) | set(historical_distribution)}\n",
    "factor = 1.0/sum(weights.values())\n",
    "weights_raw = {k: v*factor for k, v in weights.items()}\n",
    "\n",
    "weights = {'All': [weights_raw[k] for k in solact_levels['F10']]}\n",
    "weights, weights_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/loss_param directory already exists.\n",
      "dict saved as tmp/loss_param/solfsmy_weights.pkl\n"
     ]
    }
   ],
   "source": [
    "save_object(weights, \"tmp/loss_param/solfsmy_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swdf.losses import *\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "loss = wMSELoss(thresholds, weights).to(device)\n",
    "metrics = LossMetrics(loss, solact_levels['F10']).get_metrics()\n",
    "\n",
    "wandb_callback = WandbCallback(log_preds=False)\n",
    "cbs = L(wandb_callback) if config.wandb.enabled else L()\n",
    "learn = TSForecaster(X, y, splits=splits, batch_size=config.bs,\n",
    "                     pipelines=[preproc_pipe, exp_pipe], \n",
    "                     arch=config.arch_name, \n",
    "                     metrics=metrics,\n",
    "                     loss_func=loss,\n",
    "                     arch_config=dict(config.arch), \n",
    "                     init=config.init_weights,\n",
    "                     cbs= cbs + ShowGraphCallback(), \n",
    "                     partial_n=config.partial_n)\n",
    "lr_max = learn.lr_find().valley if config.lr_max is None else config.lr_max\n",
    "\n",
    "learn.fit_one_cycle(n_epoch=config.n_epoch, lr_max=config.lr_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.369739532470703\n",
      "Stored 'valid_loss' (float)\n"
     ]
    }
   ],
   "source": [
    "# Print the validation loss and save it in case other notebooks (optuna) wants to\n",
    "# use it for hyperparameter optimization\n",
    "valid_loss = learn.validate()[0]\n",
    "\n",
    "print(valid_loss)\n",
    "%store valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "learn.dls.loaders += [learn.dls.valid.new_dl(X[splits[2]], y[splits[2]])] # Add test datalaoder\n",
    "# Remove the wandb callback to avoid errors when downloading the learner\n",
    "if config.wandb.enabled:\n",
    "    learn.remove_cb(wandb_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'best_valid_loss' (float)\n",
      "Learner saved:\n",
      "path          = 'tmp'\n",
      "dls_fname     = '['dls_0.pth', 'dls_1.pth', 'dls_2.pth', 'dls_3.pth']'\n",
      "model_fname   = 'model.pth'\n",
      "learner_fname = 'learner.pkl'\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = np.inf\n",
    "%store -r best_valid_loss\n",
    "\n",
    "save_all_or_best = (config.is_optuna_study and valid_loss < best_valid_loss) or not config.is_optuna_study\n",
    "\n",
    "if save_all_or_best:\n",
    "    best_valid_loss = valid_loss\n",
    "    %store best_valid_loss                                             \n",
    "\n",
    "    # Save locally and in wandb if online and enabled\n",
    "    learn.save_all(path='tmp', verbose=True) \n",
    "    if run is not None and config.wandb_mode and config.wandb_log_learner:\n",
    "        # Save the learner (all tmp/dls, tmp/model.pth, and tmp/learner.pkl). \n",
    "        run.log_artifact('tmp', type='learner', name='solfsmy')\n",
    "    \n",
    "    # Save the best model\n",
    "    model = {}\n",
    "    model['models'] = [learn.model]\n",
    "    model['lookback'] = config.lookback\n",
    "    model['horizon'] = config.horizon\n",
    "    model['data_columns'] = data_columns\n",
    "    model['thresholds'] = thresholds\n",
    "\n",
    "    torch.save(model, 'models/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.897622108459473\n"
     ]
    }
   ],
   "source": [
    "# Log the test loss to wandb\n",
    "test_loss = learn.validate(ds_idx=2)[0]\n",
    "print(test_loss)\n",
    "if run is not None:\n",
    "    run.log(dict(test_loss=test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run is not None:\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
