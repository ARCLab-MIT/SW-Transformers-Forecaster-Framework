{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna study\n",
    "\n",
    "> Combine it with papermill and wandb for seamless hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/pip-global/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import optuna\n",
    "from tsai.optuna import *\n",
    "from tsai.basics import load_object\n",
    "import papermill as pm\n",
    "from fastcore.basics import *\n",
    "from optuna.distributions import *\n",
    "from optuna.samplers import TPESampler\n",
    "import wandb\n",
    "from swdf.metrics import ValidationMetricsHandler\n",
    "from swdf.losses import LossFactory\n",
    "from swdf.utils import run_optuna_study, filter_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list(LossFactory.losses.keys())\n",
    "primary_losses = losses[:-2] # Primary losses are the ones that do not need another loss to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table>\n",
       "            <thead>\n",
       "                <tr>\n",
       "                    <th style='text-align: left;'>Metric Name</th>\n",
       "                    <th style='text-align: left;'>Description</th>\n",
       "                </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "                <tr><td style='text-align: left;'>Skewness_Difference</td><td style='text-align: left;'><p>Calculate the absolute difference in skewness between the actual and predicted values, which measures the asymmetry of the data distribution.</p>          <h3>Parameters:</h3>         <ul>             <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>             <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>         </ul>          <h3>Returns:</h3>         <p>torch.Tensor: Absolute difference in skewness between y_true and y_pred<p></td></tr><tr><td style='text-align: left;'>Kurtosis_Difference</td><td style='text-align: left;'><p>Calculate the absolute difference in kurtosis between the actual and predicted values, which measures the tailedness of the data distribution.</p>          <h3>Parameters:</h3>         <ul>             <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>             <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>         </ul>          <h3>Returns:</h3>         <p>torch.Tensor: Absolute difference in kurtosis between y_true and y_pred<p></td></tr><tr><td style='text-align: left;'>R_Correlation</td><td style='text-align: left;'><p>Calculate the Pearson Correlation Coefficient (R Correlation) between true and predicted values.</p>                  <h3>Parameters:</h3>         <ul>             <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>             <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>         </ul>                  <h3>Returns:</h3>         <p>torch.Tensor: R Correlation coefficient<p></td></tr><tr><td style='text-align: left;'>MSA</td><td style='text-align: left;'>Calculate the Median Symmetric Accuracy (MSA).                  Parameters:         y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)         y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true                  Returns:         torch.Tensor: MSA value</td></tr><tr><td style='text-align: left;'>SSPB</td><td style='text-align: left;'><p>Calculate the Symmetric Signed Percentage Bias (SSPB), which measures the percentage bias with consideration for the direction of the bias.</p>                  <h3>Parameters:</h3>         <ul>             <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>             <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>         </ul>                  <h3>Returns:</h3>         <p>torch.Tensor: SSPB value<p></td></tr>\n",
       "            </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_metrics = ValidationMetricsHandler([\n",
    "    'R_Correlation',\n",
    "    'Kurtosis_Difference',\n",
    "    'Skewness_Difference',\n",
    "    'MSA',\n",
    "    'SSPB'\n",
    "]) \n",
    "\n",
    "# The order of the metrics will be the one that appears when listing them.\n",
    "val_metrics.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp directory already exists.\n",
      "ValidationMetricsHandler saved as tmp/metrics.pkl\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{ 'extra_params': { 'bs': 64,\n",
       "                    'is_optuna_study': True,\n",
       "                    'main_metric': 'R_Correlation',\n",
       "                    'metrics_handler_path': 'tmp/metrics.pkl',\n",
       "                    'n_epoch': 1},\n",
       "  'loss_config': { 'alpha': FloatDistribution(high=0.9, log=False, low=0.1, step=0.05),\n",
       "                   'delta': FloatDistribution(high=10.0, log=False, low=0.2, step=0.2),\n",
       "                   'primary_loss': CategoricalDistribution(choices=('MSE', 'MAE', 'MSLE', 'RMSLE', 'Hubber', 'Quantile', 'wMSE', 'wMAE', 'wMSLE', 'wRMSLE', 'wHubber', 'wQuantile')),\n",
       "                   'quantile': FloatDistribution(high=0.9, log=False, low=0.1, step=0.05)},\n",
       "  'n_trials': 100,\n",
       "  'search_space': { 'arch.attn_dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1),\n",
       "                    'arch.d_ff': IntUniformDistribution(high=512, low=32, step=32),\n",
       "                    'arch.d_model': IntUniformDistribution(high=512, low=32, step=32),\n",
       "                    'arch.decomposition': CategoricalDistribution(choices=(True, False)),\n",
       "                    'arch.dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1),\n",
       "                    'arch.individual': CategoricalDistribution(choices=(True, False)),\n",
       "                    'arch.n_heads': CategoricalDistribution(choices=(2, 4, 8, 16, 32)),\n",
       "                    'arch.n_layers': IntUniformDistribution(high=6, low=1, step=1),\n",
       "                    'init_weights': CategoricalDistribution(choices=(True, False)),\n",
       "                    'lookback': CategoricalDistribution(choices=(18, 24, 36, 128, 192)),\n",
       "                    'loss_func': CategoricalDistribution(choices=('MSE', 'MAE', 'MSLE', 'RMSLE', 'Hubber', 'Quantile', 'wMSE', 'wMAE', 'wMSLE', 'wRMSLE', 'wHubber', 'wQuantile', 'Classification', 'Trended'))},\n",
       "  'study_name': 'general_study_extended',\n",
       "  'study_type': 'bayesian',\n",
       "  'train_nb': '/workspaces/sw-driver-forecaster/dev_nbs/solfsmy_train_losses.ipynb',\n",
       "  'use_wandb': True,\n",
       "  'wandb_mode': 'offline'}\n",
       "```"
      ],
      "text/plain": [
       "{'study_name': 'general_study_extended',\n",
       " 'study_type': 'bayesian',\n",
       " 'n_trials': 100,\n",
       " 'train_nb': '/workspaces/sw-driver-forecaster/dev_nbs/solfsmy_train_losses.ipynb',\n",
       " 'search_space': {'arch.attn_dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1),\n",
       "  'arch.d_model': IntUniformDistribution(high=512, low=32, step=32),\n",
       "  'arch.d_ff': IntUniformDistribution(high=512, low=32, step=32),\n",
       "  'arch.decomposition': CategoricalDistribution(choices=(True, False)),\n",
       "  'arch.dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1),\n",
       "  'arch.individual': CategoricalDistribution(choices=(True, False)),\n",
       "  'arch.n_layers': IntUniformDistribution(high=6, low=1, step=1),\n",
       "  'arch.n_heads': CategoricalDistribution(choices=(2, 4, 8, 16, 32)),\n",
       "  'init_weights': CategoricalDistribution(choices=(True, False)),\n",
       "  'lookback': CategoricalDistribution(choices=(18, 24, 36, 128, 192)),\n",
       "  'loss_func': CategoricalDistribution(choices=('MSE', 'MAE', 'MSLE', 'RMSLE', 'Hubber', 'Quantile', 'wMSE', 'wMAE', 'wMSLE', 'wRMSLE', 'wHubber', 'wQuantile', 'Classification', 'Trended'))},\n",
       " 'loss_config': {'delta': FloatDistribution(high=10.0, log=False, low=0.2, step=0.2),\n",
       "  'quantile': FloatDistribution(high=0.9, log=False, low=0.1, step=0.05),\n",
       "  'alpha': FloatDistribution(high=0.9, log=False, low=0.1, step=0.05),\n",
       "  'primary_loss': CategoricalDistribution(choices=('MSE', 'MAE', 'MSLE', 'RMSLE', 'Hubber', 'Quantile', 'wMSE', 'wMAE', 'wMSLE', 'wRMSLE', 'wHubber', 'wQuantile'))},\n",
       " 'extra_params': {'n_epoch': 1,\n",
       "  'bs': 64,\n",
       "  'is_optuna_study': True,\n",
       "  'metrics_handler_path': 'tmp/metrics.pkl',\n",
       "  'main_metric': 'R_Correlation'},\n",
       " 'use_wandb': True,\n",
       " 'wandb_mode': 'offline'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AttrDict(\n",
    "    study_name = 'general_study_extended', # name of the Optuna study\n",
    "    study_type = 'bayesian', # 'bayesian' or 'gridsearch' or 'random'\n",
    "    n_trials = 100, # number of trials\n",
    "    train_nb = f'{os.getcwd()}/solfsmy_train_losses.ipynb', # path to the notebook to be executed\n",
    "    search_space = {\n",
    "        \"arch.attn_dropout\": DiscreteUniformDistribution(0.0, 0.5, 0.1),\n",
    "        \"arch.d_model\": IntUniformDistribution(32, 1024, 32),\n",
    "        \"arch.d_ff\": IntUniformDistribution(32, 4096, 32),\n",
    "        \"arch.decomposition\": CategoricalDistribution([True, False]),\n",
    "        \"arch.dropout\": DiscreteUniformDistribution(0.0, 0.5, 0.1), \n",
    "        \"arch.individual\": CategoricalDistribution([True, False]),\n",
    "        \"arch.n_layers\": IntUniformDistribution(1, 6, 1),\n",
    "        \"arch.n_heads\": CategoricalDistribution([2, 4, 8, 16, 32]),\n",
    "        \"init_weights\": CategoricalDistribution([True, False]), # true = kaiming\n",
    "        \"lookback\": CategoricalDistribution([18, 24, 36, 128, 192]),\n",
    "        \"loss_func\": CategoricalDistribution(losses),\n",
    "    },\n",
    "    loss_config = {\n",
    "        \"delta\": FloatDistribution(0.2, 10., step=0.2),\n",
    "        \"quantile\": FloatDistribution(0.1, 0.9, step=0.05),\n",
    "        \"alpha\": FloatDistribution(0.1, 0.9, step=0.05),\n",
    "        \"primary_loss\": CategoricalDistribution(primary_losses),\n",
    "    },\n",
    "    # Add extra parameters that are fixed, but not part of the search space\n",
    "    extra_params = {\n",
    "        \"n_epoch\": 100,\n",
    "        \"bs\": 64,\n",
    "        \"is_optuna_study\": True,\n",
    "        \"metrics_handler_path\": val_metrics.save(),\n",
    "        \"main_metric\": 'R_Correlation',\n",
    "    },\n",
    "    use_wandb = True,\n",
    "    wandb_mode = 'offline'\n",
    ")\n",
    "\n",
    "%store -d best_valid_metrics                                            \n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective(train_nb, search_space, extra_params=None, use_wandb=False):\n",
    "    \"\"\"\n",
    "        Create objective function to be minimized by Optuna.\n",
    "        Inputs:\n",
    "            trial: Optuna trial object\n",
    "            train_nb: path to the training notebook\n",
    "            search_vars: keys of the search space to be used\n",
    "            wandb_group: name of the wandb group to be used\n",
    "        Output:\n",
    "            valid_loss: validation loss\n",
    "    \"\"\"\n",
    "    def objective(trial:optuna.Trial):\n",
    "        # Define the parameters to be passed to the training notebook through papermill\n",
    "        pm_parameters = {}\n",
    "        for k,v in search_space.items():\n",
    "            pm_parameters['config.' + k] = trial._suggest(k, v)\n",
    "            if k == 'loss_func':\n",
    "                pm_parameters['config.loss_config'] = {}\n",
    "\n",
    "                if pm_parameters['config.loss_func'] in ['Hubber', 'wHubber']:\n",
    "                    pm_parameters['config.loss_config'] = {\n",
    "                        'delta': trial._suggest('delta', config.loss_config['delta'])\n",
    "                    }\n",
    "\n",
    "                if pm_parameters['config.loss_func'] in ['Quantile', 'wQuantile']:\n",
    "                    pm_parameters['config.loss_config'] = {\n",
    "                        'quantile': trial._suggest('quantile', config.loss_config['quantile'])\n",
    "                    }\n",
    "\n",
    "                if pm_parameters['config.loss_func'] in ['Classification', 'Trended']:\n",
    "                    pm_parameters['config.loss_config'] = {}\n",
    "\n",
    "                    primary_loss = trial._suggest('primary_loss', config.loss_config['primary_loss'])\n",
    "                    pm_parameters['config.loss_config']['primary_loss'] = primary_loss\n",
    "\n",
    "                    if primary_loss in ['Hubber', 'wHubber']:\n",
    "                        pm_parameters['config.loss_config']['delta'] = trial._suggest('delta', config.loss_config['delta'])\n",
    "\n",
    "                    if primary_loss in ['Quantile', 'wQuantile']:\n",
    "                        pm_parameters['config.loss_config']['quantile'] = trial._suggest('quantile', config.loss_config['quantile'])\n",
    "                    \n",
    "                    if pm_parameters['config.loss_func'] == 'Classification':\n",
    "                        pm_parameters['config.loss_config']['alpha'] = trial._suggest('alpha', config.loss_config['alpha'])\n",
    "                \n",
    "                print(f\"Loss function values: {pm_parameters['config.loss_config']}\")\n",
    "\n",
    "        # Add the extra parameters to the dictionary. The key of every parameter \n",
    "        # must be 'config.<param_name>'\n",
    "        if extra_params is not None:\n",
    "            for k,v in extra_params.items():\n",
    "                pm_parameters['config.' + k] = v\n",
    "                \n",
    "        # If using wandb, enable that in the training runs, all of them gathered\n",
    "        # into a group (NOTE: The train nb must have and use these config arguments)\n",
    "        if use_wandb:\n",
    "            pm_parameters['config.use_wandb'] = True\n",
    "            pm_parameters['config.wandb_group'] = config.study_name + '_runs'\n",
    "\n",
    "        # Call the training notebook using papermill (don't print the output)\n",
    "        stdout_file = open('tmp/pm_stdout.txt', 'w')\n",
    "        stderr_file = open('tmp/pm_stderr.txt', 'w')\n",
    "\n",
    "        pm.execute_notebook(\n",
    "            train_nb,\n",
    "            './tmp/pm_output.ipynb',\n",
    "            parameters = pm_parameters,\n",
    "            stdout_file = stdout_file,\n",
    "            stderr_file = stderr_file\n",
    "        )\n",
    "\n",
    "        # Close the output files\n",
    "        stdout_file.close()\n",
    "        stderr_file.close()\n",
    "\n",
    "        # Get the output value of interest from the source notebook\n",
    "\n",
    "        %store -r valid_metrics\n",
    "        objective_values = val_metrics.get_objective_values(valid_metrics, show_metrics=True)\n",
    "\n",
    "        \n",
    "        return *objective_values,\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the cells that are unnecesary as diagram plots or data update\n",
    "train_nb = filter_nb(config.train_nb, ['skip'])\n",
    "\n",
    "obj = create_objective(train_nb, config.search_space, \n",
    "                       extra_params=config.extra_params, use_wandb=True)\n",
    "study = run_optuna_study(obj, study_type='bayesian', direction=val_metrics.get_study_directions(), path='./tmp',\n",
    "                 study_name=config.study_name, n_trials=config.n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(config=config, mode=config.wandb_mode, \n",
    "                 job_type='optuna-study') if config.use_wandb else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run is not None:\n",
    "    # Log best trials and their corresponding values\n",
    "    best_trials_info = {}\n",
    "    for i, trial in enumerate(study.best_trials):\n",
    "        best_trials_info[f'best_params_{i}'] = trial.params\n",
    "        best_trials_info[f'best_value_{i}'] = trial.values\n",
    "        best_trials_info[f'best_trial_number_{i}'] = trial.number\n",
    "    \n",
    "    run.log(best_trials_info)\n",
    "    run.log_artifact(f'./tmp/{config.study_name}.pkl', type='optuna_study')\n",
    "    \n",
    "    # Log visualizations\n",
    "    for i in range(len(study.directions)):  # Assuming each objective has a direction\n",
    "            run.log({\n",
    "                f'contour_obj_{i}': optuna.visualization.plot_contour(study, target=lambda t: t.values[i], target_name=f'Objective {i}'),\n",
    "                f'edf_obj_{i}': optuna.visualization.plot_edf(study, target=lambda t: t.values[i], target_name=f'Objective {i}'),\n",
    "                f'optimization_history_obj_{i}': optuna.visualization.plot_optimization_history(study, target=lambda t: t.values[i], target_name=f'Objective {i}'),\n",
    "                f'parallel_coordinate_obj_{i}': optuna.visualization.plot_parallel_coordinate(study, target=lambda t: t.values[i], target_name=f'Objective {i}'),\n",
    "                f'param_importances_obj_{i}': optuna.visualization.plot_param_importances(study, target=lambda t: t.values[i], target_name=f'Objective {i}'),\n",
    "                f'slice_obj_{i}': optuna.visualization.plot_slice(study, target=lambda t: t.values[i], target_name=f'Objective {i}')\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_trial_number_0</td><td>▁</td></tr><tr><td>best_trial_number_1</td><td>▁</td></tr><tr><td>best_trial_number_10</td><td>▁</td></tr><tr><td>best_trial_number_11</td><td>▁</td></tr><tr><td>best_trial_number_12</td><td>▁</td></tr><tr><td>best_trial_number_13</td><td>▁</td></tr><tr><td>best_trial_number_14</td><td>▁</td></tr><tr><td>best_trial_number_15</td><td>▁</td></tr><tr><td>best_trial_number_16</td><td>▁</td></tr><tr><td>best_trial_number_17</td><td>▁</td></tr><tr><td>best_trial_number_18</td><td>▁</td></tr><tr><td>best_trial_number_19</td><td>▁</td></tr><tr><td>best_trial_number_2</td><td>▁</td></tr><tr><td>best_trial_number_20</td><td>▁</td></tr><tr><td>best_trial_number_21</td><td>▁</td></tr><tr><td>best_trial_number_22</td><td>▁</td></tr><tr><td>best_trial_number_23</td><td>▁</td></tr><tr><td>best_trial_number_24</td><td>▁</td></tr><tr><td>best_trial_number_25</td><td>▁</td></tr><tr><td>best_trial_number_26</td><td>▁</td></tr><tr><td>best_trial_number_27</td><td>▁</td></tr><tr><td>best_trial_number_28</td><td>▁</td></tr><tr><td>best_trial_number_29</td><td>▁</td></tr><tr><td>best_trial_number_3</td><td>▁</td></tr><tr><td>best_trial_number_30</td><td>▁</td></tr><tr><td>best_trial_number_31</td><td>▁</td></tr><tr><td>best_trial_number_32</td><td>▁</td></tr><tr><td>best_trial_number_33</td><td>▁</td></tr><tr><td>best_trial_number_34</td><td>▁</td></tr><tr><td>best_trial_number_35</td><td>▁</td></tr><tr><td>best_trial_number_36</td><td>▁</td></tr><tr><td>best_trial_number_37</td><td>▁</td></tr><tr><td>best_trial_number_38</td><td>▁</td></tr><tr><td>best_trial_number_39</td><td>▁</td></tr><tr><td>best_trial_number_4</td><td>▁</td></tr><tr><td>best_trial_number_40</td><td>▁</td></tr><tr><td>best_trial_number_41</td><td>▁</td></tr><tr><td>best_trial_number_42</td><td>▁</td></tr><tr><td>best_trial_number_43</td><td>▁</td></tr><tr><td>best_trial_number_44</td><td>▁</td></tr><tr><td>best_trial_number_45</td><td>▁</td></tr><tr><td>best_trial_number_46</td><td>▁</td></tr><tr><td>best_trial_number_47</td><td>▁</td></tr><tr><td>best_trial_number_48</td><td>▁</td></tr><tr><td>best_trial_number_49</td><td>▁</td></tr><tr><td>best_trial_number_5</td><td>▁</td></tr><tr><td>best_trial_number_50</td><td>▁</td></tr><tr><td>best_trial_number_51</td><td>▁</td></tr><tr><td>best_trial_number_6</td><td>▁</td></tr><tr><td>best_trial_number_7</td><td>▁</td></tr><tr><td>best_trial_number_8</td><td>▁</td></tr><tr><td>best_trial_number_9</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_trial_number_0</td><td>2</td></tr><tr><td>best_trial_number_1</td><td>3</td></tr><tr><td>best_trial_number_10</td><td>22</td></tr><tr><td>best_trial_number_11</td><td>24</td></tr><tr><td>best_trial_number_12</td><td>25</td></tr><tr><td>best_trial_number_13</td><td>26</td></tr><tr><td>best_trial_number_14</td><td>32</td></tr><tr><td>best_trial_number_15</td><td>34</td></tr><tr><td>best_trial_number_16</td><td>35</td></tr><tr><td>best_trial_number_17</td><td>36</td></tr><tr><td>best_trial_number_18</td><td>38</td></tr><tr><td>best_trial_number_19</td><td>39</td></tr><tr><td>best_trial_number_2</td><td>4</td></tr><tr><td>best_trial_number_20</td><td>40</td></tr><tr><td>best_trial_number_21</td><td>41</td></tr><tr><td>best_trial_number_22</td><td>42</td></tr><tr><td>best_trial_number_23</td><td>46</td></tr><tr><td>best_trial_number_24</td><td>47</td></tr><tr><td>best_trial_number_25</td><td>48</td></tr><tr><td>best_trial_number_26</td><td>49</td></tr><tr><td>best_trial_number_27</td><td>51</td></tr><tr><td>best_trial_number_28</td><td>53</td></tr><tr><td>best_trial_number_29</td><td>54</td></tr><tr><td>best_trial_number_3</td><td>7</td></tr><tr><td>best_trial_number_30</td><td>55</td></tr><tr><td>best_trial_number_31</td><td>60</td></tr><tr><td>best_trial_number_32</td><td>62</td></tr><tr><td>best_trial_number_33</td><td>64</td></tr><tr><td>best_trial_number_34</td><td>65</td></tr><tr><td>best_trial_number_35</td><td>66</td></tr><tr><td>best_trial_number_36</td><td>68</td></tr><tr><td>best_trial_number_37</td><td>70</td></tr><tr><td>best_trial_number_38</td><td>72</td></tr><tr><td>best_trial_number_39</td><td>73</td></tr><tr><td>best_trial_number_4</td><td>11</td></tr><tr><td>best_trial_number_40</td><td>76</td></tr><tr><td>best_trial_number_41</td><td>77</td></tr><tr><td>best_trial_number_42</td><td>80</td></tr><tr><td>best_trial_number_43</td><td>83</td></tr><tr><td>best_trial_number_44</td><td>85</td></tr><tr><td>best_trial_number_45</td><td>87</td></tr><tr><td>best_trial_number_46</td><td>88</td></tr><tr><td>best_trial_number_47</td><td>93</td></tr><tr><td>best_trial_number_48</td><td>94</td></tr><tr><td>best_trial_number_49</td><td>95</td></tr><tr><td>best_trial_number_5</td><td>14</td></tr><tr><td>best_trial_number_50</td><td>96</td></tr><tr><td>best_trial_number_51</td><td>99</td></tr><tr><td>best_trial_number_6</td><td>16</td></tr><tr><td>best_trial_number_7</td><td>17</td></tr><tr><td>best_trial_number_8</td><td>19</td></tr><tr><td>best_trial_number_9</td><td>21</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /workspaces/sw-driver-forecaster/dev_nbs/wandb/offline-run-20240822_162641-0ciwra38<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20240822_162641-0ciwra38/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if run is not None:\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
