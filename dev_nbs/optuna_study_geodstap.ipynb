{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna study\n",
    "\n",
    "> Combine it with papermill and wandb for seamless hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/pip-global/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import optuna\n",
    "from tsai.optuna import *\n",
    "from tsai.basics import load_object\n",
    "import papermill as pm\n",
    "from tsai.optuna import run_optuna_study\n",
    "from fastcore.basics import *\n",
    "from optuna.distributions import *\n",
    "from optuna.samplers import TPESampler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{ 'extra_params': { 'arch.d_model': 128,\n",
       "                    'arch.n_heads': 8,\n",
       "                    'arch_name': 'TSiTPlus',\n",
       "                    'bs': 32,\n",
       "                    'is_optuna_study': True,\n",
       "                    'n_epoch': 30},\n",
       "  'n_trials': 30,\n",
       "  'search_space': { 'arch.attn_dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1),\n",
       "                    'arch.dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1),\n",
       "                    'deltaHL': FloatDistribution(high=15.0, log=False, low=1.0, step=1.0),\n",
       "                    'lookback': CategoricalDistribution(choices=(144, 192, 240, 288, 336))},\n",
       "  'study_name': 'general_study_extended',\n",
       "  'study_type': 'bayesian',\n",
       "  'train_nb': '/workspaces/sw-driver-forecaster/dev_nbs/geodstap_train.ipynb',\n",
       "  'use_wandb': True,\n",
       "  'wandb_mode': 'offline'}\n",
       "```"
      ],
      "text/plain": [
       "{'study_name': 'general_study_extended',\n",
       " 'study_type': 'bayesian',\n",
       " 'n_trials': 30,\n",
       " 'train_nb': '/workspaces/sw-driver-forecaster/dev_nbs/geodstap_train.ipynb',\n",
       " 'search_space': {'arch.attn_dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1),\n",
       "  'arch.dropout': DiscreteUniformDistribution(high=0.5, low=0.0, q=0.1),\n",
       "  'lookback': CategoricalDistribution(choices=(144, 192, 240, 288, 336)),\n",
       "  'deltaHL': FloatDistribution(high=15.0, log=False, low=1.0, step=1.0)},\n",
       " 'extra_params': {'n_epoch': 30,\n",
       "  'bs': 32,\n",
       "  'is_optuna_study': True,\n",
       "  'arch_name': 'TSiTPlus',\n",
       "  'arch.d_model': 128,\n",
       "  'arch.n_heads': 8},\n",
       " 'use_wandb': True,\n",
       " 'wandb_mode': 'offline'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AttrDict(\n",
    "    study_name='general_study_extended',  # name of the Optuna study\n",
    "    study_type='bayesian',  # 'bayesian' or 'gridsearch' or 'random'\n",
    "    n_trials=30,  # number of trials\n",
    "    train_nb=f'{os.getcwd()}/geodstap_train.ipynb',  # path to the notebook to be executed\n",
    "    search_space={\n",
    "        \"arch.attn_dropout\": DiscreteUniformDistribution(0.0, 0.5, 0.1),\n",
    "        # \"arch.d_model\": IntUniformDistribution(32, 2048, 64),\n",
    "        # \"arch.d_ff\": IntUniformDistribution(32, 4096, 128), # Not used in TSiT\n",
    "        # \"arch.decomposition\": CategoricalDistribution([True, False]), # Not used in TSiT\n",
    "        \"arch.dropout\": DiscreteUniformDistribution(0.0, 0.5, 0.1),\n",
    "        # \"arch.individual\": CategoricalDistribution([True, False]), # Not used in TSiT\n",
    "        # \"arch.n_layers\": IntUniformDistribution(1, 6, 1),\n",
    "        # \"arch.n_heads\": CategoricalDistribution([2, 4, 8, 16, 32]),\n",
    "        # \"arch.patch_len\": CategoricalDistribution([4, 8, 16, 32, 64, 128]), # Not used in TSiT\n",
    "        # \"init_weights\": CategoricalDistribution([True, False]), # Not used in TSiT\n",
    "        \"lookback\": CategoricalDistribution([144, 192, 240, 288, 336]), # Not used in TSiT\n",
    "        \"deltaHL\": FloatDistribution(1., 15., step=1.), # Not used in TSiT\n",
    "    },\n",
    "    # Add extra parameters that are fixed, but not part of the search space\n",
    "    extra_params={\n",
    "        \"n_epoch\": 30,\n",
    "        \"bs\": 32,\n",
    "        \"is_optuna_study\": True,\n",
    "        \"arch_name\": 'TSiTPlus',   \n",
    "        \"arch.d_model\": 128,\n",
    "        \"arch.n_heads\": 8\n",
    "    },\n",
    "    use_wandb=True,\n",
    "    wandb_mode='offline'\n",
    ")\n",
    "\n",
    "%store -d best_valid_loss                                             \n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective(train_nb, search_space, extra_params=None, use_wandb=False):\n",
    "    \"\"\"\n",
    "        Create objective function to be minimized by Optuna.\n",
    "        Inputs:\n",
    "            trial: Optuna trial object\n",
    "            train_nb: path to the training notebook\n",
    "            search_vars: keys of the search space to be used\n",
    "            wandb_group: name of the wandb group to be used\n",
    "        Output:\n",
    "            valid_loss: validation loss\n",
    "    \"\"\"\n",
    "    def objective(trial:optuna.Trial):\n",
    "        # Define the parameters to be passed to the training notebook through papermill\n",
    "        pm_parameters = {}\n",
    "        for k,v in search_space.items():\n",
    "            pm_parameters['config.' + k] = trial._suggest(k, v)\n",
    "\n",
    "        # Add the extra parameters to the dictionary. The key of every parameter \n",
    "        # must be 'config.<param_name>'\n",
    "        if extra_params is not None:\n",
    "            for k,v in extra_params.items():\n",
    "                pm_parameters['config.' + k] = v\n",
    "                \n",
    "        # If using wandb, enable that in the training runs, all of them gathered\n",
    "        # into a group (NOTE: The train nb must have and use these config arguments)\n",
    "        if use_wandb:\n",
    "            pm_parameters['config.use_wandb'] = True\n",
    "            pm_parameters['config.wandb_group'] = config.study_name + '_runs'\n",
    "\n",
    "        # Call the training notebook using papermill (don't print the output)\n",
    "        stdout_file = open('tmp/pm_stdout.txt', 'w')\n",
    "        stderr_file = open('tmp/pm_stderr.txt', 'w')\n",
    "\n",
    "        pm.execute_notebook(\n",
    "            train_nb,\n",
    "            './tmp/pm_output.ipynb',\n",
    "            parameters = pm_parameters,\n",
    "            stdout_file = stdout_file,\n",
    "            stderr_file = stderr_file,\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Close the output files\n",
    "        stdout_file.close()\n",
    "        stderr_file.close()\n",
    "\n",
    "        # Get the output value of interest from the source notebook\n",
    "        loss = None\n",
    "        %store -r valid_loss\n",
    "        return valid_loss\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "def filter_nb (path:str, skip_tags:list):\n",
    "    # Load the notebook\n",
    "    nb = nbformat.read(path, as_version=4)\n",
    "\n",
    "    # Filter out cells with specific tags\n",
    "    filtered_cells = [cell for cell in nb.cells if not set(skip_tags) & set(cell.metadata.get('tags', []))]\n",
    "    nb.cells = filtered_cells\n",
    "    \n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the cells that are unnecesary as diagram plots or data update\n",
    "train_nb = filter_nb(config.train_nb, ['skip'])\n",
    "\n",
    "obj = create_objective(train_nb, config.search_space, \n",
    "                       extra_params=config.extra_params, use_wandb=True)\n",
    "study = run_optuna_study(obj, study_type='bayesian', direction='minimize', path='./tmp',\n",
    "                 study_name=config.study_name, n_trials=config.n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = '/workspaces/sw-driver-forecaster/dev_nbs/optuna_study_geodstap.ipynb'\n",
    "\n",
    "run = wandb.init(config=config, mode=config['wandb_mode'], job_type='optuna-study') if config['use_wandb'] else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-08-13 13:59:37,863] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,865] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,866] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,867] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,869] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,870] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,872] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,874] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,875] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,876] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,877] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,879] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,881] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,882] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,884] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,885] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,887] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,888] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,891] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,893] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,894] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,896] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,897] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,898] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,900] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,902] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,903] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,905] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,907] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,909] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,911] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,913] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,915] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,917] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,920] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,921] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,923] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,925] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,927] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,929] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,930] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,932] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,933] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,935] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,937] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,937] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,938] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,939] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,941] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,942] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,943] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,945] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,946] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,947] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,948] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,949] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,950] Param arch.individual unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,951] Param lookback unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,952] Param arch.n_layers unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:37,953] Param init_weights unique value length is less than 2.\n",
      "[W 2024-08-13 13:59:39,976] You need to set up the pruning feature to utilize `plot_intermediate_values()`\n"
     ]
    }
   ],
   "source": [
    "if run is not None:\n",
    "    run.log(dict(study.best_params, **{'best_value': study.best_value, \n",
    "                                       'best_trial_number': study.best_trial.number}))\n",
    "    run.log_artifact(f'./tmp/{config.study_name}.pkl', type='optuna_study')\n",
    "    run.log({\n",
    "        'contour': optuna.visualization.plot_contour(study),\n",
    "        'edf': optuna.visualization.plot_edf(study),\n",
    "        'intermediate_values': optuna.visualization.plot_intermediate_values(study),\n",
    "        'optimization_history': optuna.visualization.plot_optimization_history(study),\n",
    "        'parallel_coordinate' : optuna.visualization.plot_parallel_coordinate(study),\n",
    "        'param_importances': optuna.visualization.plot_param_importances(study),\n",
    "        'slice': optuna.visualization.plot_slice(study)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>arch.attn_dropout</td><td>▁</td></tr><tr><td>arch.d_ff</td><td>▁</td></tr><tr><td>arch.d_model</td><td>▁</td></tr><tr><td>arch.decomposition</td><td>▁</td></tr><tr><td>arch.dropout</td><td>▁</td></tr><tr><td>arch.individual</td><td>▁</td></tr><tr><td>arch.n_heads</td><td>▁</td></tr><tr><td>arch.n_layers</td><td>▁</td></tr><tr><td>best_trial_number</td><td>▁</td></tr><tr><td>best_value</td><td>▁</td></tr><tr><td>init_weights</td><td>▁</td></tr><tr><td>lookback</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>arch.attn_dropout</td><td>0.4</td></tr><tr><td>arch.d_ff</td><td>352</td></tr><tr><td>arch.d_model</td><td>416</td></tr><tr><td>arch.decomposition</td><td>False</td></tr><tr><td>arch.dropout</td><td>0.2</td></tr><tr><td>arch.individual</td><td>False</td></tr><tr><td>arch.n_heads</td><td>8</td></tr><tr><td>arch.n_layers</td><td>1</td></tr><tr><td>best_trial_number</td><td>0</td></tr><tr><td>best_value</td><td>58.26485</td></tr><tr><td>init_weights</td><td>True</td></tr><tr><td>lookback</td><td>144</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /workspaces/sw-driver-forecaster/dev_nbs/wandb/offline-run-20240813_135937-txzllcmu<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20240813_135937-txzllcmu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if run is not None:\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
