# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/losses.ipynb.

# %% auto 0
__all__ = ['Loss', 'MSELoss', 'MAELoss', 'MSLELoss', 'HubberLoss', 'WeightedLoss', 'wMSELoss', 'wMAELoss', 'wMSLELoss',
           'wHubberLoss', 'ClassificationLoss', 'TrendedLoss', 'LossMetrics']

# %% ../nbs/losses.ipynb 0
from abc import ABC, abstractmethod
import torch
import numpy as np
import pandas as pd
from tsai.basics import *

# %% ../nbs/losses.ipynb 2
class Loss(nn.Module, ABC):
    def __init__(self, reduction:str=None):
        super().__init__()
        self.reduction = reduction
    
    def _reduce(self, loss: torch.Tensor) -> torch.Tensor:
        if self.reduction == 'mean': return loss.mean()
        if self.reduction == 'sum': return loss.sum()
        return loss
    
    @abstractmethod
    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        return NotImplementedError
    
    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        loss = self._compute_loss(input, target)
        return self._reduce(loss)

# %% ../nbs/losses.ipynb 3
class MSELoss(Loss):
    def __init__(self, reduction:str=None):
        super().__init__(reduction)

    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        return (target-input)**2

class MAELoss(Loss):
    def __init__(self, reduction:str=None):
        super().__init__(reduction)

    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        return torch.abs(target-input)
    
class MSLELoss(Loss):
    def __init__(self, reduction:str=None):
        super().__init__(reduction)

    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        return (torch.log1p(input) - torch.log1p(target))**2
    
class HubberLoss(Loss):
    def __init__(self, reduction:str=None, delta:float=1.):
        super().__init__(reduction)
        self.delta = delta

    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        error = target - input
        
        is_small_error = error < self.delta
        small_error_loss = (0.5 * (error ** 2))
        large_error_loss = (self.delta * (torch.abs(error) - 0.5 * self.delta))

        return torch.where(is_small_error, small_error_loss, large_error_loss)

# %% ../nbs/losses.ipynb 5
class WeightedLoss(nn.Module, ABC):
    def __init__(self, thresholds:dict, weights:dict):
        super().__init__()

        # Activity levels' weights can be equal across all variables or different,
        # and this should be taken into account during preprocessing. 
        self.all_variables_have_same_weights = len(weights.keys()) == 1
        ranges, weights = self._preprocess_data(thresholds, weights)

        self.register_buffer('ranges', torch.Tensor(ranges))
        self.register_buffer('weights', torch.Tensor(weights))

    def weighted_loss_tensor(self, target: torch.Tensor) -> torch.Tensor:        
        batch, variables, horizon = target.shape  # Example shape (32, 4, 6)
        variable, max_range, interval = self.ranges.shape  # Example shape (4, 4, 2)

        target_shaped = torch.reshape(target, (batch, variables, 1, horizon))  # Example shape (32, 4, 6) -> (32, 4, 1, 6)
        ranges_shaped = torch.reshape(self.ranges, (variable, max_range, 1, interval))  # Example shape (4, 4, 2) -> (4, 4, 1, 2)

        weights_tensor = ((ranges_shaped[..., 0] <= target_shaped) & (target_shaped <= ranges_shaped[..., 1])).float()
             
        if self.all_variables_have_same_weights:
            equation = 'r,bvrh->bvh'
        else:
            equation = 'vr,bvrh->bvh'

        return torch.einsum(equation, self.weights, weights_tensor)
    
    @abstractmethod
    def loss_measure(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:
        return NotImplementedError
    
    def _preprocess_data(self, thresholds, weights):
        # If each variable has its own weights, calculate the maximum size of weights.
        # Padding shorter weights with NaNs prevents heterogeneous tensor errors.
        if (self.all_variables_have_same_weights):
            ranges = np.array(list(thresholds.values())[:])
            weights = np.array(next(iter(weights.values())))
        else:
            def add_padding(x, padding_value, shape):
                result = np.full(shape, padding_value)
                for i, r in enumerate(x):
                    result[i, :len(r)] = r
                return result
            
            max_size = max([len(array) for array in thresholds.values()])

            ranges_raw = thresholds.values()
            ranges = add_padding(ranges_raw, np.nan, (len(ranges_raw), max_size, 2))

            weights_raw = [weights[key] for key in thresholds.keys()]
            weights = add_padding(weights_raw, 0.0, (len(weights_raw), max_size))

        return ranges, weights
    
    def forward(self, y_pred, y_true, reduction='mean'):
        error = self.loss_measure(y_pred, y_true)
        weights = self.weighted_loss_tensor(y_true)

        if reduction == 'mean':
            loss = (error * weights).mean()
        elif reduction == 'sum':
            loss = (error * weights).sum()
        else: 
            loss = error*weights
        
        return loss

# %% ../nbs/losses.ipynb 8
class wMSELoss(WeightedLoss):
    def __init__(self, thresholds, weights):
        super().__init__(thresholds, weights)

    
    def loss_measure(self, input, target):
        return MSELoss()(input, target)

# %% ../nbs/losses.ipynb 9
class wMAELoss(WeightedLoss):
    def __init__(self, thresholds, weights):
        super().__init__(thresholds, weights)

    def loss_measure(self, input, target):
        return MAELoss()(input, target)

# %% ../nbs/losses.ipynb 10
class wMSLELoss(WeightedLoss):
    def __init__(self, thresholds, weights):
        super().__init__(thresholds, weights)
    
    def loss_measure(self, input, target):
        return MSLELoss()(input, target)

# %% ../nbs/losses.ipynb 11
class wHubberLoss(WeightedLoss):
    def __init__(self, thresholds, weights, delta=2.0):
        super().__init__(thresholds, weights)
        self.delta = delta
    
    def loss_measure(self, y_pred, y_true):
        return HubberLoss(self.delta)(y_pred, y_true)

# %% ../nbs/losses.ipynb 12
class ClassificationLoss(WeightedLoss):
    def __init__(self, thresholds, loss):
        n_variables = len(thresholds.keys())
        weights = {'All': np.arange(n_variables)}

        super().__init__(ranges, weights)

        self.loss = loss
    
    def loss_measure(self, input, target):
        return self.loss(input, target)

    def forward(self, input, target, reduction='mean'):
        error = self.loss_measure(input, target)
        weights = 1 + torch.abs(self.weighted_loss_tensor(target) - self.weighted_loss_tensor(input))

        if (error.shape != weights.shape): # To avoid the use of other loss functions as CrossEntropyLoss
            weights = weights.mean(dim=1)
            
        if reduction == 'mean':
            loss = (error * weights).mean()
        elif reduction == 'sum':
            loss = (error * weights).sum()
        
        return loss

# %% ../nbs/losses.ipynb 13
class TrendedLoss(nn.Module):
    def __init__(self, loss: Loss):
        super().__init__()
        self.loss = loss

    @staticmethod
    def _slope(y):
        x = np.arange(len(y))
        slope, _ = np.polyfit(x, y, deg=1)
        return slope

    @staticmethod
    def _calculate_trends(tensor):
        np_tensor = tensor.cpu().detach().numpy()
        trends = np.apply_along_axis(TrendedLoss._slope, 2, np_tensor)
        return torch.Tensor(trends)

    def forward(self, input, target):
        batch, variables, _ = input.shape

        input_trend = TrendedLoss._calculate_trends(input)
        target_trend = TrendedLoss._calculate_trends(target)
        
        trend_diff = 1 + torch.abs(input_trend - target_trend)

        error = self.loss(input, target)
        weights = trend_diff.reshape(batch,variables,1)
        loss = (error * weights).mean()

        return loss

# %% ../nbs/losses.ipynb 16
class LossMetrics:
    def __init__(self, loss_func, solact_levels):
        self.loss_func = loss_func
        self.solact_levels = solact_levels

    # Weighted Regressive Loss Metrics
    def _apply_weighted_loss_by_level(self, input, target, weight_idx):
        loss_copy = deepcopy(self.loss_func)
        
        for idx in range(len(loss_copy.weights)):
            if idx != weight_idx:
                loss_copy.weights[idx] = 0
        
        return loss_copy(input, target)
    
    
    # Classification Loss Metrics
    def _compute_misclassifications(self, predictions, targets):
        classifier = self.loss_func.weighted_loss_tensor
        true_labels = classifier(targets)
        predicted_labels = classifier(predictions)

        misclassified_labels = (true_labels != predicted_labels).int() * predicted_labels

        return misclassified_labels.unique(return_counts=True)

    def _count_misclassifications_by_level(self, predictions, targets, level):
        unique_labels, label_counts = self._compute_misclassifications(predictions, targets)
        label_count_dict = dict(zip(unique_labels.tolist(), label_counts.tolist()))

        return label_count_dict.get(level, 0)
    

    # Metrics functions
    def loss_low(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, 0)
    
    def loss_moderate(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, 1)
    
    def loss_elevated(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, 2)
    
    def loss_high(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, 3)
    
    def missclassifications_low(self, predictions, targets):
        return self._count_misclassifications_by_level(predictions, targets, 1)
    
    def missclassifications_moderate(self, predictions, targets):
        return self._count_misclassifications_by_level(predictions, targets, 2)
    
    def missclassifications_elevated(self, predictions, targets):
        return self._count_misclassifications_by_level(predictions, targets, 3)
    
    def missclassifications_high(self, predictions, targets):
        return self._count_misclassifications_by_level(predictions, targets, 4)
    

    # Metrics retrieval
    def get_metrics(self):
        if not isinstance(self.solact_levels, list):
            def Metrics_Not_Available(input, target): return '_' 
            return [Metrics_Not_Available]
        elif isinstance(self.loss_func, ClassificationLoss):
            return [self.missclassifications_low, self.missclassifications_moderate, self.missclassifications_elevated, self.missclassifications_high]
        
        elif isinstance(self.loss_func, WeightedLoss):
            return [self.loss_low, self.loss_moderate, self.loss_elevated, self.loss_high]
        
        else:
            return []

