# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/metrics.ipynb.

# %% auto 0
__all__ = ['Metrics', 'RegressiveMetrics', 'SOLFMYMetrics', 'GEODSTAPMetrics', 'ClassificationMetrics',
           'SOLFMYClassificationMetrics', 'GEODSTAPClassificationMetrics', 'LossMetrics', 'OutlierDetectionMetrics',
           'F1ScoreMetrics', 'AUPRCMetric', 'KSDifferenceMetric', 'AssociationMetrics',
           'inverse_scale_values_below_threshold', 'AccuracyMetrics', 'BiasMetrics', 'ValidationMetricsHandler']

# %% ../nbs/metrics.ipynb 0
import sys
sys.path.append('..')
from abc import ABC, abstractmethod
import torch
import numpy as np
import pandas as pd
from tsai.basics import *
from .losses import wMAELoss, MSELoss, WeightedLoss, ClassificationLoss
from sklearn.metrics import precision_recall_curve, auc
from optuna.study import StudyDirection




# %% ../nbs/metrics.ipynb 3
class Metrics(ABC):
    def __init__(self):
        super().__init__()

    @abstractmethod
    def get_metrics(self) -> list:
        return NotImplementedError

# %% ../nbs/metrics.ipynb 6
class RegressiveMetrics(Metrics):
    def __init__(self, loss_func):
        super().__init__()
        self.loss_func = loss_func

    def _apply_weighted_loss_by_level(self, input, target, weight_idx):
        loss_copy = deepcopy(self.loss_func)
        
        for idx1 in range(len(loss_copy.weights)):
            if is_iter(loss_copy.weights[0]):
                for idx2 in range(len(loss_copy.weights[idx1])):
                    if (idx1 != weight_idx[0]) | (idx2 != weight_idx[1]):
                        loss_copy.weights[idx1][idx2] = 0
            else:
                if idx1 != weight_idx[1]:
                    loss_copy.weights[idx1] = 0
                
        return loss_copy(input, target)

    @abstractmethod
    def get_metrics(self) -> list:
        return NotImplementedError

# %% ../nbs/metrics.ipynb 8
class SOLFMYMetrics(RegressiveMetrics):
    def __init__(self, loss_func):
        super().__init__(loss_func)
        self.loss_func = loss_func



    # Metrics
    def Loss_Low(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [0,0])
    
    def Loss_Moderate(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [0,1])
    
    def Loss_Elevated(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [0,2])
    
    def Loss_High(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [0,3])
    
    
    # Metrics retrieval function
    def get_metrics(self) -> list:
        return [
                self.Loss_Low, 
                self.Loss_Moderate, 
                self.Loss_Elevated, 
                self.Loss_High
            ]

# %% ../nbs/metrics.ipynb 10
class GEODSTAPMetrics(RegressiveMetrics):
    def __init__(self, loss_func, indices:str='geodstap'):
        super().__init__(loss_func)
        self.indices = indices
        
        
    # Metrics
    def Loss_Low(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [0,0])
    
    def Loss_Medium(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [0,1])
    
    def Loss_Active(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [0,2])
    
    def Loss_G0(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [1,0])
    
    def Loss_G1(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [1,1])
    
    def Loss_G2(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [1,2])
        
    def Loss_G3(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [1,3])
    
    def Loss_G4(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [1,4])
    
    def Loss_G5(self, input, target):
        return self._apply_weighted_loss_by_level(input, target, [1,5])
    

    # Metrics retrieval function
    def get_metrics(self) -> list:
        if self.indices == 'geodst':
            return [
                self.Loss_G0, 
                self.Loss_G1, 
                self.Loss_G2, 
                self.Loss_G3, 
                self.Loss_G4, 
                self.Loss_G5
            ]
        
        elif self.indices == 'geoap':
            return [
                    self.Loss_Low, 
                    self.Loss_Medium, 
                    self.Loss_Active
                ]
        
        return [
                self.Loss_Low, 
                self.Loss_Medium, 
                self.Loss_Active,
                self.Loss_G0, 
                self.Loss_G1, 
                self.Loss_G2, 
                self.Loss_G3, 
                self.Loss_G4, 
                self.Loss_G5
            ]

# %% ../nbs/metrics.ipynb 12
class ClassificationMetrics(Metrics):
    def __init__(self, loss_func):
        super().__init__()
        self.loss_func = loss_func



    def _compute_misclassifications(self, predictions, targets):
        # Use the weighted loss tensor from the provided loss function
        classifier = self.loss_func.weighted_loss_tensor
        
        # Get the true and predicted labels using the classifier
        true_labels = classifier(targets)
        predicted_labels = classifier(predictions)

        # Misclassifications are those where the predicted label does not match the true label
        misclassified_labels = (true_labels != predicted_labels).int() * predicted_labels

        return misclassified_labels

    def _count_misclassifications_by_position(self, predictions, targets, row, col):
        # Calculate misclassifications for a specific (row, column) pair
        misclassified_labels = self._compute_misclassifications(predictions, targets)
        
        # Extract the specific misclassification at the (row, column) position and sum across the time dimension
        if row < misclassified_labels.shape[1] and col < misclassified_labels.shape[2]:
            misclassification_count = misclassified_labels[:, row, col].sum().item()
        else:
            misclassification_count = 0  # Out of bounds, assume no misclassification
        
        return misclassification_count
  
    
    @abstractmethod
    def get_metrics(self) -> list:
        return NotImplementedError

# %% ../nbs/metrics.ipynb 14
class SOLFMYClassificationMetrics(ClassificationMetrics):
    def __init__(self, loss_func):
        super().__init__(loss_func)


    # Metrics
    def Missclassifications_Low(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 0, 1)

    def Missclassifications_Moderate(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 0, 2)

    def Missclassifications_Elevated(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 0, 3)

    def Missclassifications_High(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 0, 4)


    # Metrics retrieval function
    def get_metrics(self) -> list:
        return [
                self.Missclassifications_Low,
                self.Missclassifications_Moderate, 
                self.Missclassifications_Elevated, 
                self.Missclassifications_High
            ]

# %% ../nbs/metrics.ipynb 16
class GEODSTAPClassificationMetrics(ClassificationMetrics):
    def __init__(self, loss_func, indices:str='geodstap'):
        super().__init__(loss_func)
        self.indices = indices


    # Metrics
    def Missclassifications_Low(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 0, 1)

    def Missclassifications_Medium(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 0, 2)

    def Missclassifications_Active(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 0, 3)

    def Missclassifications_G0(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 1, 1)

    def Missclassifications_G1(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 1, 2)

    def Missclassifications_G2(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 1, 3)

    def Missclassifications_G3(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 1, 4)

    def Missclassifications_G4(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 1, 5)

    def Missclassifications_G5(self, predictions, targets):
        return self._count_misclassifications_by_position(predictions, targets, 1, 6)

    # Metrics retrieval function
    def get_metrics(self) -> list:
        if self.indices == 'geodst':
            return [
                    self.Missclassifications_G0,
                    self.Missclassifications_G1, 
                    self.Missclassifications_G2, 
                    self.Missclassifications_G3, 
                    self.Missclassifications_G4, 
                    self.Missclassifications_G5
                ]
        
        elif self.indices == 'geoap':
            return [
                    self.Missclassifications_Low, 
                    self.Missclassifications_Medium, 
                    self.Missclassifications_Active
                ]
        
        return [
                self.Missclassifications_Low, 
                self.Missclassifications_Medium, 
                self.Missclassifications_Active, 
                self.Missclassifications_G0, 
                self.Missclassifications_G1,
                self.Missclassifications_G2,
                self.Missclassifications_G3,
                self.Missclassifications_G4,
                self.Missclassifications_G5
            ]

# %% ../nbs/metrics.ipynb 18
class LossMetrics(Metrics):
    def __init__(self, loss_func, indices:str = ''):
        super().__init__()
        self.loss_func = loss_func
        self.indices = indices

    ## Metrics Not Available
    def Metrics_Not_Available(self, input, target): return np.nan 
    
    # Metrics retrieval
    def get_metrics(self):
        if isinstance(self.loss_func, ClassificationLoss):
            if self.indices.lower() == 'solfsmy':
                return SOLFMYClassificationMetrics(self.loss_func).get_metrics()
            if self.indices.lower() in ['geodstap', 'geoap', 'geodst']:
                return GEODSTAPClassificationMetrics(self.loss_func, self.indices).get_metrics()
        
        if isinstance(self.loss_func, WeightedLoss):
            if self.indices.lower() == 'solfsmy':
                return SOLFMYMetrics(self.loss_func).get_metrics()
            
            if self.indices.lower() in ['geodstap', 'geoap', 'geodst']:
                return GEODSTAPMetrics(self.loss_func, self.indices).get_metrics()
        
        return [self.Metrics_Not_Available]

# %% ../nbs/metrics.ipynb 22
class OutlierDetectionMetrics(Metrics):
    def __init__(self, threshold=3.5):
        super().__init__()
        self.threshold = threshold

    @staticmethod
    def _modified_z_score(x):
        """
        Calculate the Modified Z-Score for each variable in the tensor.
        
        Parameters:
        tensor (torch.Tensor): Input tensor of shape (batch_size, variables, horizon)
        
        Returns:
        torch.Tensor: Modified Z-Score tensor of the same shape as input
        """
        median = torch.median(x, dim=2, keepdim=True).values
        
        mad = torch.median(torch.abs(x - median), dim=2, keepdim=True).values
        mad = torch.where(mad == 0, torch.tensor(1.0, device=x.device), mad)
        
        modified_z_scores = 0.6745 * (x - median) / mad
        
        return modified_z_scores

    def _detect_outliers(self, values):
        """
        Detect outliers based on Modified Z-Scores.
        
        Parameters:
        z_scores (torch.Tensor): Modified Z-Scores tensor
        
        Returns:
        torch.Tensor: Boolean tensor indicating outliers
        """
        z_scores = self._modified_z_score(values)
        return torch.abs(z_scores) > self.threshold
    
    def _evaluate_outlier_predicted(self, y_true, y_pred):
        """
        Evaluate the performance of outlier detection.
        
        Parameters:
        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)
        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true
        
        Returns:
        AttrDict: Dictionary with true/false positives, false negatives, indices of true/predicted outliers
        """    
        # Detect outliers based on the threshold
        true_outliers = self._detect_outliers(y_true)
        pred_outliers = self._detect_outliers(y_pred)
        
        # Evaluate the detection by comparing true outliers and predicted outliers
        tp = torch.sum((pred_outliers & true_outliers).float())  # True Positives
        fp = torch.sum((pred_outliers & ~true_outliers).float()) # False Positives
        fn = torch.sum((~pred_outliers & true_outliers).float()) # False Negatives
        tn = torch.sum((~pred_outliers & ~true_outliers).float()) # True Negatives

        return AttrDict({
            "tp": tp,
            "fp": fp,
            "fn": fn,
            "tn": tn,
            "true_outliers": true_outliers,
            "predicted_outliers": pred_outliers
        })
    
    
    @abstractmethod
    def get_metrics(self) -> list:
        return NotImplementedError

# %% ../nbs/metrics.ipynb 24
class F1ScoreMetrics(OutlierDetectionMetrics):
    def __init__(self, threshold=3.5, metrics='F1_Score'):
        super().__init__(threshold)
        self.metrics = metrics
        self.epsilon = torch.finfo(torch.float32).eps # Used to avoid division per 0

    

    # Metrics
    def Precision(self, y_true, y_pred):
        """
        <p>Calculate the precision metric, which measures the ratio of correctly predicted positive observations to the total predicted positives.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: Precision score<p>
        """
        stats = self._evaluate_outlier_predicted(y_true, y_pred)

        # To avoid divide by 0
        precision = (stats.tp + self.epsilon) / ((stats.tp + stats.fp) + self.epsilon)

        return precision

    
    def Recall(self, y_true, y_pred):
        """
        <p>Calculate the recall metric, which measures the ratio of correctly predicted positive observations to all observations in the actual class.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: Recall score<p>
        """
        stats = self._evaluate_outlier_predicted(y_true, y_pred)

        recall = (stats.tp + self.epsilon) / ((stats.tp + stats.fn) + self.epsilon)
    

        return recall

    
    def F1_Score(self, y_true, y_pred):
        """
        <p>Calculate the F1 score, which is the harmonic mean of precision and recall. It is used as a measure of a model’s accuracy on a dataset.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: F1 score<p>
        """
        precision = self.Precision(y_true, y_pred)
        recall = self.Recall(y_true, y_pred)

        f1_score = ((2 * (precision * recall)) + self.epsilon) / ((precision + recall) + self.epsilon)
 

        return f1_score

    
    def Accuracy_Score(self, y_true, y_pred):
        """
        <p>Calculate the accuracy score, which is the ratio of correctly predicted observations to the total observations.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: Accuracy score<p>
        """
        stats = self._evaluate_outlier_predicted(y_true, y_pred)
            
        ((stats.tp + stats.tn) + self.epsilon) / ((stats.tp + stats.fp + stats.fn + stats.tn) + self.epsilon)


    
    def Specificity(self, y_true, y_pred):
        """
        <p>Calculate the specificity metric, which measures the proportion of true negatives that are correctly identified.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: Specificity score<p>
        """
        stats = self._evaluate_outlier_predicted(y_true, y_pred)
        
        return (stats.tn+ self.epsilon) / ((stats.tn + stats.fp) + self.epsilon)



    def Negative_Predictive_Value(self, y_true, y_pred):
        """
        <p>Calculate the Negative Predictive Value (NPV), which measures the proportion of true negatives among all negative predictions.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: Negative Predictive Value score<p>
        """
        stats = self._evaluate_outlier_predicted(y_true, y_pred)

        return (stats.tn + self.epsilon) / ((stats.tn + stats.fn) + self.epsilon)
  

    
    def Detected_Outliers_Difference (self, y_true, y_pred):
        """
        <p>Calculate the change in detected outliers (Δ Detected Outliers), representing the number of true outliers not predicted as outliers.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: Count of undetected outliers<p>
        """    
        stats = self._evaluate_outlier_predicted(y_true, y_pred)
        
        return torch.sum(stats.true_outliers & ~stats.predicted_outliers)


    # Metrics retrieval function
    def get_metrics(self) -> list:
        if self.metrics == 'F1_Score':
            return [self.F1_Score]
        elif self.metrics == 'All':
            return [self.Precision, self.Recall, self.F1_Score, self.Accuracy_Score, self.Specificity, self.Negative_Predictive_Value, self.Detected_Outliers_Difference ]
        else:
            return [self.Precision, self.Recall, self.F1_Score, self.Detected_Outliers_Difference ]

# %% ../nbs/metrics.ipynb 26
class AUPRCMetric(OutlierDetectionMetrics):
    def __init__(self, threshold=3.5):
        super().__init__(threshold)


    # Metrics
    def AURPC(self, y_true, y_pred):
        """
        <p>Calculate the Area Under the Precision-Recall Curve (AUPRC), a 
        metric used to evaluate the effectiveness of a model in identifying rare, important events (outliers)</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        
        <h3>Returns:</h3>
        <p>torch.Tensor: AUPRC score<p>
        """

        # We check that there are no NaN values as it would product a AURPC of 0.5 
        # (no detected outliers), which is not informative
        if torch.isnan(y_true).any() or torch.isnan(y_pred).any(): 
            return torch.tensor(0.0, device=y_true.device)

        pred_z_scores = self._modified_z_score(y_pred)
        
        pred_z_scores_flat = pred_z_scores.view(-1).cpu().numpy()
        true_outliers_flat = self._detect_outliers(y_true).view(-1).cpu().numpy()
        
        # Use precision_recall_curve to get precision and recall for different thresholds
        precision, recall, _ = precision_recall_curve(true_outliers_flat, pred_z_scores_flat)
        
        auprc_value = auc(recall, precision)
        
        return torch.tensor(auprc_value, device=y_true.device)
    

    # Metrics retrieval function
    def get_metrics(self) -> list:
        return [self.AURPC]

# %% ../nbs/metrics.ipynb 29
class KSDifferenceMetric(Metrics):
    def __init__(self, threshold=3.5):
        super().__init__()
        self.threshold = threshold

    @staticmethod
    def skewness(x):
        # As batches are randomly generated and each variable is independent
        mean = torch.mean(x, dim=2, keepdim=True)
        std_dev = torch.std(x, dim=2, unbiased=True, keepdim=True)
        
        skewness = torch.mean(((x - mean) / std_dev) ** 3, dim=2)
        return skewness

    @staticmethod
    def kurtosis(x):
        mean = torch.mean(x, dim=2, keepdim=True)
        std_dev = torch.std(x, dim=2, unbiased=True, keepdim=True)
        
        kurtosis = torch.mean(((x - mean) / std_dev) ** 4, dim=2)
        return kurtosis
    

    # Metrics
    def Skewness_Difference(self, y_true, y_pred):
        """
        <p>Calculate the absolute difference in skewness between the actual and predicted values, which measures the asymmetry of the data distribution.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: Absolute difference in skewness between y_true and y_pred<p>
        """
        true_skewness = KSDifferenceMetric.skewness(y_true)
        pred_skewness = KSDifferenceMetric.skewness(y_pred)
        
        return torch.mean(torch.abs(true_skewness - pred_skewness), dim=[0, 1])

    
    def Kurtosis_Difference(self, y_true, y_pred):
        """
        <p>Calculate the absolute difference in kurtosis between the actual and predicted values, which measures the tailedness of the data distribution.</p>

        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>

        <h3>Returns:</h3>
        <p>torch.Tensor: Absolute difference in kurtosis between y_true and y_pred<p>
        """
        true_kurtosis = KSDifferenceMetric.kurtosis(y_true)
        pred_kurtosis = KSDifferenceMetric.kurtosis(y_pred)
        
        return torch.mean(torch.abs(true_kurtosis - pred_kurtosis), dim=[0, 1])


    # Metrics retrieval function
    def get_metrics(self) -> list:
        return [self.Skewness_Difference, self.Kurtosis_Difference]

# %% ../nbs/metrics.ipynb 32
class AssociationMetrics(Metrics):
    def __init__(self):
        super().__init__()
        self.epsilon = torch.finfo(torch.float32).eps # Used to avoid division per 0


    # Metrics
    def R_Correlation(self, y_true, y_pred):
        """
        <p>Calculate the Pearson Correlation Coefficient (R Correlation) between true and predicted values.</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>
        
        <h3>Returns:</h3>
        <p>torch.Tensor: R Correlation coefficient<p>
        """
        y_true_flat = y_true.reshape(-1)
        y_pred_flat = y_pred.reshape(-1)
        
        # To be able to use torch.corrcoef, we need to stack the tensors
        stacked = torch.stack([y_true_flat, y_pred_flat])
        
        corr_matrix = torch.corrcoef(stacked)
        
        r_value = corr_matrix[0, 1]
        return r_value


    def R2_Score(self, y_true, y_pred):
        """
        <p>Calculate the R^2 score, which measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>
        
        <h3>Returns:</h3>
        <p>torch.Tensor: R^2 score<p>
        """
        y_true_mean = torch.mean(y_true, dim=2, keepdim=True)
        
        # Total Sum of Squares
        ss_tot = torch.sum((y_true - y_true_mean) ** 2)
        
        # Residual Sum of Squares
        ss_res = torch.sum((y_true - y_pred) ** 2)

        
        r2 = 1 - ((ss_res + self.epsilon) / (ss_tot + self.epsilon))
        
        return r2

    
    # Metrics retrieval function
    def get_metrics(self) -> list:
        return [self.R_Correlation, self.R2_Score]
    
    

# %% ../nbs/metrics.ipynb 34
def inverse_scale_values_below_threshold(tensor, threshold, lower_bound, upper_bound):
    mask = tensor < threshold

    if mask.sum() == 0:
        # If no values are below the threshold, return the original tensor
        return tensor
    
    values_to_scale = tensor[mask]
    min_orig = values_to_scale.min()
    max_orig = values_to_scale.max()
    
    if min_orig == max_orig:
        scaled_values = torch.full_like(tensor, upper_bound)
    else:
        scaled_values = upper_bound - (tensor - min_orig) * (upper_bound - lower_bound
    ) / (max_orig - min_orig)
    
    result_tensor = torch.where(mask, scaled_values, tensor)
    
    return result_tensor

# %% ../nbs/metrics.ipynb 37
class AccuracyMetrics(Metrics):
    def __init__(self):
        super().__init__()
        self.epsilon = torch.finfo(torch.float32).eps # Used to avoid division per 0



    # Metrics
    def sMAPE(self, y_true, y_pred):
        """
        Calculate the Symmetric Mean Absolute Percentage Error (sMAPE).
        
        Parameters:
        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)
        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true
        
        Returns:
        torch.Tensor: sMAPE value
        """
        abs_error = torch.abs(y_true - y_pred)
        symetric_error = ((torch.abs(y_true) + torch.abs(y_pred)) / 2.0) 
        
        smape = torch.mean((abs_error + self.epsilon)/ (symetric_error + self.epsilon)) * 100
        return smape
    
    def MSA(self, y_true, y_pred):
        """
        Calculate the Median Symmetric Accuracy (MSA).
        
        Parameters:
        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)
        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true
        
        Returns:
        torch.Tensor: MSA value
        """
        q = (y_pred + self.epsilon) / (y_true + self.epsilon)

        log_ratio = torch.abs(torch.log(inverse_scale_values_below_threshold(q, 0, 0.9, self.epsilon)))

        msa = (torch.exp(torch.median(log_ratio)) - 1) * 100
        
        return msa
    

    # Metrics retrieval function
    def get_metrics(self) -> list:
        return [self.sMAPE, self.MSA]


# %% ../nbs/metrics.ipynb 40
class BiasMetrics(Metrics):
    def __init__(self):
        super().__init__()
        self.epsilon = torch.finfo(torch.float32).eps # Used to avoid division per 0


    # Metrics
    def SSPB(self, y_true, y_pred):
        """
        <p>Calculate the Symmetric Signed Percentage Bias (SSPB), which measures the percentage bias with consideration for the direction of the bias.</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>
            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>
        </ul>
        
        <h3>Returns:</h3>
        <p>torch.Tensor: SSPB value<p>
        """
        q = (y_pred + self.epsilon) / (y_true + self.epsilon)

        log_ratio = torch.abs(torch.log(inverse_scale_values_below_threshold(q, 0, 0.9, self.epsilon)))
        median_log_ratio = torch.median(log_ratio)

        sign = torch.sign(median_log_ratio)
        
        return sign * (torch.exp(torch.abs(median_log_ratio)) - 1) * 100



    # Metrics retrieval function
    def get_metrics(self) -> list:
        return [self.SSPB]

# %% ../nbs/metrics.ipynb 42
class ValidationMetricsHandler:
    """
    <p>A class to manage validation metrics for model evaluation. It allows listing available metrics, uploading requested metrics, and retrieving study directions and objective values.</p>
    
    <h3>Attributes:</h3>
    <ul>
        <li>available_metrics (list)[<i>Static</i>]: A list of available metrics provided by different metric classes.</li>
        <li>study_directions (dict)[<i>Static</i>]: A dictionary mapping metrics to their respective optimization directions (maximize or minimize).</li>
        <li>requested_metrics (dict): A dictionary storing metrics that have been requested for evaluation.</li>
    </ul>
    """
    
    available_metrics = [
        *F1ScoreMetrics(metrics='All').get_metrics(),
        *AUPRCMetric().get_metrics(),
        *KSDifferenceMetric().get_metrics(),
        *AssociationMetrics().get_metrics(),
        *AccuracyMetrics().get_metrics(),
        *BiasMetrics().get_metrics()
    ]

    study_directions = {
        'precision': StudyDirection.MAXIMIZE,                    # Higher precision is better (Range: [0, 1])
        'recall': StudyDirection.MAXIMIZE,                       # Higher recall is better (Range: [0, 1])
        'f1_score': StudyDirection.MAXIMIZE,                     # Higher F1 score is better (Range: [0, 1])
        'accuracy_score': StudyDirection.MAXIMIZE,               # Higher accuracy is better (Range: [0, 1])
        'specificity': StudyDirection.MAXIMIZE,                  # Higher specificity is better (Range: [0, 1])
        'negative_predictive_value': StudyDirection.MAXIMIZE,    # Higher NPV is better (Range: [0, 1])
        'detected_outliers_difference': StudyDirection.MINIMIZE, # Minimize the difference in detected outliers (Range: [0, ∞))
        'aurpc': StudyDirection.MAXIMIZE,                        # Higher AUPRC is better (Range: [0, 1])
        'skewness_difference': StudyDirection.MINIMIZE,          # Minimize skewness difference to target (Range: [−∞, ∞])
        'kurtosis_difference': StudyDirection.MINIMIZE,          # Minimize kurtosis difference to target (Range: [−∞, ∞])
        'r_correlation': StudyDirection.MAXIMIZE,                # Higher Pearson correlation is better (Range: [−1, 1])
        'r2_score': StudyDirection.MAXIMIZE,                     # Higher R² is better (Range: [−∞, 1])
        'smape': StudyDirection.MINIMIZE,                        # Lower SMAPE is better (Range: [0, ∞))
        'msa': StudyDirection.MAXIMIZE,                          # Higher MSA is better (Range: [0, 1])
        'sspb': StudyDirection.MINIMIZE                          # Minimize absolute SSPB (optimize for bias close to zero) (Range: [−100%, 100%])
    }


    def __init__(self, metrics:list=None, main_metric:str=''):
        self.requested_metrics = {}
        if metrics is not None:
            self.add(metrics)

        self.main_metric = main_metric.lower()


    # Visualization functions
    def list(self):
        """
        <p>Display a list of available metrics along with their descriptions in a table format.</p>
        """
        table_rows = []

        if not bool(self.requested_metrics):
            for metric in ValidationMetricsHandler.available_metrics:
                doc_html = metric.__doc__.strip().replace("\n", " ")
                table_rows.append(f"<tr><td style='text-align: left;'><strong>{metric.__name__}</strong></td><td style='text-align: left;'>{doc_html}</td></tr>")
        else:
            for metric in self.requested_metrics.values():
                doc_html = metric.__doc__.strip().replace("\n", " ")
                metric_name = metric.__name__.lower()
                if metric_name in self.requested_metrics.values():
                    table_rows.append(f"<tr><td style='text-align: left;'><strong>{metric.__name__}</strong></td><td style='text-align: left;'>{doc_html}</td></tr>")
                else:
                    table_rows.append(f"<tr><td style='text-align: left;'>{metric.__name__}</td><td style='text-align: left;'>{doc_html}</td></tr>")
        
        
        table_html = f"""
        <table>
            <thead>
                <tr>
                    <th style='text-align: left;'>Metric Name</th>
                    <th style='text-align: left;'>Description</th>
                </tr>
            </thead>
            <tbody>
                {''.join(table_rows)}
            </tbody>
        </table>
        """
        
        display(HTML(table_html))

    def _show_metrics(self, metrics:List[AvgMetric]) -> None:
        metric_column_width = '250px'  # Fixed width to accommodate 'detected_outliers_difference'
        value_column_width = '100px'   # Fixed width for values up to 12 characters
        table_rows = []
        for metric in metrics:
            value = f"{metric.value:.4f}"
            table_rows.append(f"<tr><td style='padding: 4px; text-align: left; width: {metric_column_width}; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;'><strong>{metric.name}</strong></td><td style='padding: 4px; text-align: right; width: {value_column_width};'>{value}</td></tr>")
        
        table_html = f"""
        <table style='border-collapse: collapse; table-layout: fixed; width: {int(metric_column_width[:-2]) + int(value_column_width[:-2]) + 20}px;'>
            <thead>
                <tr>
                    <th style='padding: 4px; text-align: left; border: 1px solid black; width: {metric_column_width};'>Metric Name</th>
                    <th style='padding: 4px; text-align: right; border: 1px solid black; width: {value_column_width};'>Value</th>
                </tr>
            </thead>
            <tbody>
                {''.join(table_rows)}
            </tbody>
        </table>
        """
        
        display(HTML(table_html))



    # Metrics management functions
    def add(self, metrics:list):
        """
        <p>Upload a list of metrics to the factory for evaluation. The metrics are converted to lowercase for consistency.</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li>metrics (list): A list of metric names to be uploaded for evaluation.</li>
        </ul>
        
        <h3>Raises:</h3>
        <p>ValueError: If any metric in the provided list is not found in the available metrics.</p>
        """
        metrics = [metric.lower() for metric in metrics]

        for metric in ValidationMetricsHandler.available_metrics:
            metric_name = metric.__name__.lower()
            if metric_name in metrics:
                self.requested_metrics[metric_name] = metric
                metrics.remove(metric.__name__.lower())
        
        if len(metrics) > 0:
            raise ValueError(f"Metrics not found: {metrics}. Please use ValidationMetricsFactory.list() to see available metrics.")
        
    def remove(self, metrics:list):
        """
        <p>Remove a list of metrics from the factory that were previously uploaded for evaluation. The metrics are converted to lowercase for consistency.</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li>metrics (list): A list of metric names to be removed from evaluation.</li>
        </ul>
        
        <h3>Raises:</h3>
        <p>ValueError: If any metric in the provided list is not found in the requested metrics.</p>
        """
        metrics = [metric.lower() for metric in metrics]

        for metric in metrics:
            if metric in self.requested_metrics:
                self.requested_metrics.pop(metric)
            else:
                raise ValueError(f"Metric not found: {metric}. Please use ValidationMetricsFactory.get_metrics() to see requested metrics.")
        


    # Metrics utility functions
    def get_metrics(self) -> list:
        """
        <p>Retrieve the list of requested metrics for evaluation.</p>
        
        <h3>Returns:</h3>
        <p>list: A list of requested metric objects.</p>
        """
        return list(self.requested_metrics.values())

    def get_study_directions(self) -> list:    
        """
        <p>Retrieve the study directions (maximize or minimize) for the requested metrics.</p>
        
        <h3>Returns:</h3>
        <p>list: A list of study directions corresponding to the requested metrics.</p>
        """
        return [self.study_directions[metric] for metric in self.requested_metrics.keys()]
    
    def get_objective_values(self, metrics_results:List[AvgMetric], show_metrics=False) -> list:
        """
        <p>Extract the objective values from the results of the requested metrics.</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li>metrics_results (List[AvgMetric]): A list of metric result objects from which to extract values.</li>
        </ul>
        
        <h3>Returns:</h3>
        <p>list: A list of metric values extracted from the provided results.</p>
        """
        if show_metrics:
            self._show_metrics(metrics_results)

        object_values = []
        for metric, requested_metric in zip(metrics_results, self.requested_metrics.keys()):
            metric_name = metric.name.lower()
            if metric_name == requested_metric:
                # As SSPB could be positive or negative, but the better is to be closer to 0
                if metric_name == 'sspb':
                    object_values.append(np.abs(metric.value)) 
                else:
                    object_values.append(metric.value)
            else:
                raise ValueError(f"Unexpected metric found: {metric_name}. Expected: {requested_metric}")
            
            
        return (metric_result.value for metric_result in metrics_results)
    
    def save(self, path:str='tmp/') -> str:
        """
        <p>Save the requested metrics to a file to share with the training notebook.</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li>path (str): The path to save the requested metrics. | <i>Default</i>: tmp</li>
        </ul>
        """
        path = f"{path}metrics.pkl"
        save_object(self, path)

        return path
    

    # Metrics evaluation functions
    @staticmethod
    def _has_improved(trial_value, best_value, direction):
        """
        <p>Determine if the trial value has improved over the best value based on the optimization direction.</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li><b>trial_value</b> (float or int): The value from the current trial to evaluate.</li>
            <li><b>best_value</b> (float or int): The current best value for comparison.</li>
            <li><b>direction</b> (StudyDirection): The direction of optimization, either maximizing or minimizing.</li>
        </ul>
        
        <h3>Returns:</h3>
        <p>bool: True if the trial value represents an improvement over the best value in the given direction, False otherwise.</p>
        """
        if direction == StudyDirection.MAXIMIZE:
            return trial_value > best_value
        elif direction == StudyDirection.MINIMIZE:
            return trial_value < best_value
        return False

    def are_best_values(self, best_values, trial_values) -> bool:
        """
        <p>Determine if the trial values are better than the current best values for a given metric.</p>
        
        <h3>Parameters:</h3>
        <ul>
            <li><b>main_metric</b> (str): The name of the main metric used for comparison.</li>
            <li><b>best_values</b> (list): A list of current best values for various metrics.</li>
            <li><b>trial_values</b> (list): A list of new trial values to compare against the best values.</li>
        </ul>
        
        <h3>Returns:</h3>
        <p>bool: True if the trial values are overall better than the best values, False otherwise.</p>
        """
        cls = ValidationMetricsHandler

        if best_values is None:
            return True

        if len(best_values) == 1:
            return cls._has_improved(
                    trial_value=trial_values[0].value, 
                    best_value=best_values[0].value, 
                    direction=cls.study_directions.get(self.main_metric)
                )

        improvement_count = 0
        comparison_threshold = len(best_values) // 2

        for best_metric, trial_metric in zip(best_values, trial_values):
            metric_name = best_metric.name.lower()
            direction = cls.study_directions.get(metric_name)

            if cls._has_improved(trial_metric.value, best_metric.value, direction):
                if self.main_metric == metric_name:
                    improvement_count += comparison_threshold
                else:
                    improvement_count += 1

        return improvement_count > comparison_threshold
            

