{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp losses\n",
    "\n",
    "#|export\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsai.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Functions\n",
    "\n",
    "---\n",
    "## Index\n",
    "#### <a href=\"#loss-measures-implementation\">1. Loss Measures Implementation</a>\n",
    "  - <a href=\"#mean-squared-error-loss-mse\">1.1 Mean Squared Error Loss (MSE)</a>\n",
    "  - <a href=\"#mean-absolute-error-loss-mae\">1.2 Mean Absolute Error Loss (MAE)</a>\n",
    "  - <a href=\"#mean-squared-logarithmic-error-loss-msle\">1.3 Mean Squared Logarithmic Error Loss (MSLE)</a>\n",
    "  - <a href=\"#root-mean-squared-logarithmic-error-loss-msle\">1.4 Root Mean Squared Logarithmic Error Loss (RMSLE)</a>\n",
    "  - <a href=\"#huber-loss-hl\">1.5 Huber Loss (HL)</a>\n",
    "  - <a href=\"#quantile-loss-ql\">1.6 Quantile Loss (QL)</a>\n",
    "#### <a href=\"#weighted-losses\">2. Weighted Losses</a>\n",
    "  - <a href=\"#weighted-losses-using-the-pre-defined-loss-functions\">2.1 Weighted Losses Using the Pre-defined Loss Functions</a>\n",
    "  - <a href=\"#special-weighted-losses\">2.2 Special Weighted Losses</a>\n",
    "    - <a href=\"#classification-loss\">2.2.1 Classification Loss</a>\n",
    "    - <a href=\"#trended-loss\">2.2.2 Trended Loss</a>\n",
    "#### <a href=\"#loss-factory\">4. Loss Factory</a>\n",
    "#### <a href=\"#tests\">5. Tests</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document contains the implementation of all custom loss functions that can be used during the training process. We opted not to use the pre-existing functions available in `PyTorch`, as they are not specifically tailored to our needs. Implementing these loss functions from scratch allowed us to better control the weighting process and customize them according to our requirements. \n",
    "\n",
    "As a starting point, we implemented an abstract class for the loss function to work as an `nn.Module`. This class provides an option for loss reduction and includes an abstract method for loss computation, which is then used in the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Loss(nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    <p>Base class for loss functions, providing a common interface for different types of losses.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction:str=None):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def _reduce(self, loss: torch.Tensor) -> torch.Tensor:\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum': return loss.sum()\n",
    "        return loss\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, reduction:str=None) -> torch.Tensor:\n",
    "        if reduction is not None:\n",
    "            self.reduction = reduction\n",
    "        loss = self._compute_loss(input, target)\n",
    "        return self._reduce(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Measures Implementation\n",
    "\n",
    "In this section, we provide the implementation of the unweighted loss functions. We have separated the implementations of the unweighted and weighted loss functions **to allow for a direct comparison of their performance**. This separation also enhances the versatility of our evaluation process. The decisions we made regarding which loss functions to implement are based on the work of Jadon, A. et al. (2024).\n",
    "\n",
    "<details>\n",
    "  <summary><u>References</u></summary>\n",
    "\n",
    "Jadon, A., Patil, A. & Jadon, S. A Comprehensive Survey of Regression-Based Loss Functions for Time Series Forecasting. arXiv: [2211.02989 [cs]](https://arxiv.org/abs/2211.02989). (2024). Preprint.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error Loss (MSE)\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "The Mean Squared Error (MSE) is a common loss function for regression problems, including time series forecasting. It calculates the average of the squared differences between predicted and actual values. MSE is popular because it is computationally efficient, differentiable, and works well with various optimization algorithms. However, MSE is highly sensitive to outliers, meaning that large errors can disproportionately influence the model's learning process. This sensitivity can be problematic in time series forecasting, where outliers might represent anomalies or unusual events rather than the underlying data pattern. Therefore, this loss function could be particularly well-suited for **periods where volatility is usual**, such as during high solar activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MSELoss(Loss):\n",
    "    \"\"\"\n",
    "    <p>Mean Squared Error Loss (MSELoss) measures the average squared difference between predicted and actual values.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction:str=None):\n",
    "        super().__init__(reduction)\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return (target-input)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error Loss (MAE)\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "The Mean Absolute Error (MAE) is a widely used loss function in regression tasks, including time series forecasting. It quantifies the average of the absolute differences between predicted and actual values. Unlike Mean Squared Error (MSE), which squares the errors, MAE treats all errors linearly, making it more robust to outliers. However, MAE's linear scoring method can lead to less efficient convergence during optimization, especially when dealing with smaller errors. Despite this potential drawback, MAE remains a valuable loss function, particularly in situations where **minimizing the impact of outliers is crucial**, such as in cases with low activity levels where values are more stable but outliers may still occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MAELoss(Loss):\n",
    "    \"\"\"\n",
    "    <p>Mean Absolute Error Loss (MAELoss) calculates the average absolute differences between predicted and actual values.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction:str=None):\n",
    "        super().__init__(reduction)\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.abs(target-input)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Logarithmic Error Loss (MSLE)\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=0}^{N} (log(y_i' + 1) - log(\\hat{y}_i' + 1))^2 \\qquad \\text{where: } y_i = \n",
    "\\begin{cases} \n",
    "y_i & \\text{if } y_i > -1 \\\\\n",
    "-1 + \\epsilon & \\text{if } y_i \\leq -1 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The Mean Squared Logarithmic Error (MSLE) assesses the **relative difference between predicted and actual values** and is particularly useful when dealing with unscaled quantities. It mitigates the impact of large discrepancies in predictions for large values while remaining sensitive to smaller differences in predictions for smaller values. This characteristic stems from applying a logarithmic transformation to both the actual and predicted values before calculating the squared difference. MSLE is especially well-suited for situations where **underestimating values carries a higher penalty than overestimating them**.\n",
    "\n",
    "Note that we have clipped the value to not be lower than -1 to avoid errors with the $log(1+x)$ function, which tends to infinity as $x$ approaches -1. However, the value is scaled to be closer to -1 if it is more negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MSLELoss(Loss):\n",
    "    \"\"\"\n",
    "    <p>Mean Squared Logarithmic Error Loss (MSLELoss) penalizes underestimations more than overestimations by using logarithms.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction:str=None):\n",
    "        super().__init__(reduction)\n",
    "\n",
    "    @staticmethod\n",
    "    def inverse_scale_values_below_threshold(tensor, threshold, lower_bound, upper_bound):\n",
    "        mask = tensor < threshold\n",
    "\n",
    "        if mask.sum() == 0:\n",
    "            # If no values are below the threshold, return the original tensor\n",
    "            return tensor\n",
    "        \n",
    "        values_to_scale = tensor[mask]\n",
    "        min_orig = values_to_scale.min()\n",
    "        max_orig = values_to_scale.max()\n",
    "        \n",
    "        if min_orig == max_orig:\n",
    "            scaled_values = torch.full_like(tensor, upper_bound)\n",
    "        else:\n",
    "            scaled_values = upper_bound - (tensor - min_orig) * (upper_bound - lower_bound\n",
    "        ) / (max_orig - min_orig)\n",
    "        \n",
    "        result_tensor = torch.where(mask, scaled_values, tensor)\n",
    "        \n",
    "        return result_tensor\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        epsilon = torch.finfo(torch.float32).eps\n",
    "        target_scaled = MSLELoss.inverse_scale_values_below_threshold(target, -1, 0.1, epsilon)\n",
    "        input_scaled = MSLELoss.inverse_scale_values_below_threshold(input, -1, 0.1, epsilon)\n",
    "        \n",
    "        target = torch.where(target <= -1, -1 + target_scaled, target)\n",
    "        input = torch.where(input <= -1, -1 + input_scaled, input)\n",
    "\n",
    "        return (torch.log1p(target) - torch.log1p(input))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Logarithmic Error Loss (MSLE)\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{N} \\sum_{i=0}^{N} \\left(\\log(y_i' + 1) - \\log(\\hat{y}_i' + 1)\\right)^2} \\qquad \\text{where: } y_i = \n",
    "\\begin{cases} \n",
    "y_i & \\text{if } y_i > -1 \\\\\n",
    "-1 + \\epsilon & \\text{if } y_i \\leq -1 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The Root Mean Squared Logarithmic Error Loss (RMSLE) is similar to MSLE but is **scale-invariant** and **less sensitive to outliers**. This loss is preferred when overestimation is acceptable, but underestimation is undesirable. For instance, in the case of DST and AP geomagnetic indices, a slight overestimation might be beneficial, as it can help better predict the significant outliers caused by solar storms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class RMSLELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    <p>Root Mean Squared Logarithmic Error Loss (RMSLELoss) is the square root of MSLE, useful for reducing the impact of outliers.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction:str='mean'):\n",
    "        super().__init__()\n",
    "        self.msle_loss = MSLELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, reduction='mean') -> torch.Tensor:\n",
    "        return torch.sqrt(self.msle_loss(input, target, reduction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss (HL)\n",
    "\n",
    "$$\n",
    "\\text{HuberLoss} = \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y_i - \\hat{y}_i)^2 & \\text{if } |y_i - \\hat{y}_i| < \\delta \\\\\n",
    "\\delta \\times \\left(|y_i - \\hat{y}_i| - \\frac{1}{2} \\delta\\right) & \\text{if } |y_i - \\hat{y}_i| \\geq \\delta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Huber Loss (HL) combines the characteristics of both MSE and MAE, aiming to benefit from their respective strengths while mitigating their limitations. This loss function introduces a parameter called delta (Î´) that acts as a threshold to determine the appropriate method for calculating the loss. For errors smaller than delta, Huber Loss behaves like MSE, employing a quadratic scoring function to facilitate efficient convergence. However, for errors exceeding delta, it transitions to a linear scoring approach akin to MAE, effectively reducing the influence of outliers. This adaptive behavior makes Huber Loss a versatile choice for time series forecasting, as it can handle datasets with **varying degrees of noise and outlier presence**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class HubberLoss(Loss):\n",
    "    \"\"\"\n",
    "    <p>Huber Loss (HL) combines the characteristics of both MSE and MAE, aiming to benefit from their respective strengths while mitigating their limitations.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: 'mean'.</li>\n",
    "        <li>delta (float): Threshold from where the loss changes from MAE to MSE-like functioning.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction:str=None, delta:float=1.):\n",
    "        super().__init__(reduction)\n",
    "        self.delta = delta\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        error = target - input\n",
    "        \n",
    "        is_small_error = error < self.delta\n",
    "        small_error_loss = (0.5 * (error ** 2))\n",
    "        large_error_loss = (self.delta * (torch.abs(error) - 0.5 * self.delta))\n",
    "\n",
    "        return torch.where(is_small_error, small_error_loss, large_error_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss (QL)\n",
    "\n",
    "$$\n",
    "\\text{Quantile Loss} = \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\begin{cases} \n",
    "(\\gamma - 1) \\cdot (y_i - \\hat{y}_i) & \\text{if  } y_i < \\hat{y}_i \\\\\n",
    "\\gamma \\cdot (y_i - \\hat{y}_i) & \\text{if  } y_i \\geq \\hat{y}_i\n",
    "\\end{cases}\n",
    "\n",
    "$$\n",
    "\n",
    "Quantile Loss (QL) is particularly useful when the goal is to predict not just a single point estimate but rather a range of possible outcomes with associated probabilities. This loss function is used in quantile regression, a type of regression analysis that estimates the conditional quantiles of the target variable given a set of predictor variables. Quantile Loss is defined based on the desired quantile ($\\gamma$) and penalizes overestimations and underestimations differently depending on the value of $\\gamma$.\n",
    "\n",
    "For instance, when $\\gamma = 0.5$, the loss function aims to estimate the median, penalizing both overestimations and underestimations equally. However, for other values of $\\gamma$, the penalties are adjusted to reflect the desired quantile. In our case, by **adjusting the quantile to higher percentiles**, we can focus more on **outliers**, such as solar storms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class QuantileLoss(Loss):\n",
    "    \"\"\"\n",
    "    <p>Quantile Loss is used for regression tasks where we want to predict a specific quantile.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "        <li>quantile (float): The quantile to be predicted, usually a value between 0 and 1.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, quantile: float, reduction: str = None):\n",
    "        super().__init__(reduction)\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        errors = target - input\n",
    "        return torch.where(errors >= 0, self.quantile * errors, (self.quantile - 1) * errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Losses\n",
    "\n",
    "Here we implement the addition of a **weights tensor** to the loss functions. As discussed in our paper, we introduce **weighted loss functions** to assign more importance to levels that appear to be **underrepresented** in the training data, thereby giving them greater relevance.\n",
    "\n",
    "As shown below, we have created a **weighted loss superclass** where the weights are calculated and applied. The core method of this class involves calculating the weight tensor. In this superclass, we take the thresholds, reshape the target tensor and thresholds so they can be directly compared, classify the target tensor, and then apply the weights using the `torch.einsum()` function. The final part of the function applies the weights, whether they are **equal for all variables** or **different for each variable**.\n",
    "\n",
    "Additionally, a **data preprocessing method** is used because some thresholds and weights may have different shapes, which can result in errors due to unaligned shapes that do not fit properly with the weight calculation method. To address this, padding is added to variables that have fewer categories. **Reduction is recalculated** as the weights are applied directly to the error tensor, not to the reduced one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class WeightedLoss(nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    <p>Base class for weighted loss functions, where different samples are given different importance.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "        <li>weights (Tensor): Weights assigned to each sample in the batch.</li>\n",
    "        <li>thresholds (Tensor): Threshold values for weighted computation.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, thresholds:dict, weights:dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Activity levels' weights can be equal across all variables or different,\n",
    "        # and this should be taken into account during preprocessing. \n",
    "        self.all_variables_have_same_weights = len(weights.keys()) == 1\n",
    "        ranges, weights = self._preprocess_data(thresholds, weights)\n",
    "\n",
    "        self.register_buffer('ranges', torch.Tensor(ranges))\n",
    "        self.register_buffer('weights', torch.Tensor(weights))\n",
    "\n",
    "\n",
    "    def weighted_loss_tensor(self, target: torch.Tensor) -> torch.Tensor:        \n",
    "        batch, variables, horizon = target.shape  # Example shape (32, 4, 6)\n",
    "        variable, max_range, interval = self.ranges.shape  # Example shape (4, 4, 2)\n",
    "\n",
    "        target_shaped = torch.reshape(target, (batch, variables, 1, horizon))  # Example shape (32, 4, 6) -> (32, 4, 1, 6)\n",
    "        ranges_shaped = torch.reshape(self.ranges, (variable, max_range, 1, interval))  # Example shape (4, 4, 2) -> (4, 4, 1, 2)\n",
    "\n",
    "        weights_tensor = ((ranges_shaped[..., 0] <= target_shaped) & (target_shaped <= ranges_shaped[..., 1])).float()\n",
    "             \n",
    "        if self.all_variables_have_same_weights:\n",
    "            equation = 'r,bvrh->bvh'\n",
    "        else:\n",
    "            equation = 'vr,bvrh->bvh'\n",
    "\n",
    "        return torch.einsum(equation, self.weights, weights_tensor)\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss_measure(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def _preprocess_data(self, thresholds, weights):\n",
    "        # If each variable has its own weights, calculate the maximum size of weights.\n",
    "        # Padding shorter weights with NaNs prevents heterogeneous tensor errors.\n",
    "        if (self.all_variables_have_same_weights):\n",
    "            ranges = np.array(list(thresholds.values())[:])\n",
    "            weights = np.array(next(iter(weights.values())))\n",
    "        else:\n",
    "            def add_padding(x, padding_value, shape):\n",
    "                result = np.full(shape, padding_value)\n",
    "                for i, r in enumerate(x):\n",
    "                    result[i, :len(r)] = r\n",
    "                return result\n",
    "            \n",
    "            max_size = max([len(array) for array in thresholds.values()])\n",
    "\n",
    "            ranges_raw = thresholds.values()\n",
    "            ranges = add_padding(ranges_raw, np.nan, (len(ranges_raw), max_size, 2))\n",
    "\n",
    "            weights_raw = [weights[key] for key in thresholds.keys()]\n",
    "            weights = add_padding(weights_raw, 0.0, (len(weights_raw), max_size))\n",
    "\n",
    "        return ranges, weights\n",
    "    \n",
    "    \n",
    "    def forward(self, y_pred, y_true, reduction='mean'):\n",
    "        error = self.loss_measure(y_pred, y_true)\n",
    "        weights = self.weighted_loss_tensor(y_true)\n",
    "\n",
    "        if reduction == 'mean':\n",
    "            loss = (error * weights).mean()\n",
    "        elif reduction == 'sum':\n",
    "            loss = (error * weights).sum()\n",
    "        else: \n",
    "            loss = error*weights\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Losses Using the Pre-defined Loss Functions\n",
    "\n",
    "Here you can find the weighted versions of the loss functions implemented in this document. All functions follow the structure below when applying the weights:\n",
    "\n",
    "$$ \n",
    "\\text{Weighted Loss} = \\frac{1}{N} \\sum_{i=1}^{N} w^{Bs \\times V \\times H} \\cdot \\mathcal{L}^{Bs \\times V \\times H} \\qquad \\text{where}: \n",
    "\\begin{cases}\n",
    "    w: \\text{Weight tensor} \\\\\n",
    "    Bs: \\text{Batch size} \\\\\n",
    "    V: \\text{Number of variables} \\\\\n",
    "    H: \\text{Horizon length} \\\\\n",
    "    \\mathcal{L}: \\text{Loss measure}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class wMSELoss(WeightedLoss):\n",
    "    \"\"\"\n",
    "    <p>Weighted Mean Squared Error Loss (wMSELoss) is the weighted version of MSE, giving different importance to different samples.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "        <li>weights (Tensor): Weights assigned to each sample in the batch.</li>\n",
    "        <li>thresholds (Tensor): Threshold values for weighted computation.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, thresholds, weights):\n",
    "        super().__init__(thresholds, weights)\n",
    "\n",
    "    \n",
    "    def loss_measure(self, input, target):\n",
    "        return MSELoss()(input, target)\n",
    "    \n",
    "\n",
    "    \n",
    "class wMAELoss(WeightedLoss):\n",
    "    \"\"\"\n",
    "    <p>Weighted Mean Absolute Error Loss (wMAELoss) is the weighted version of MAE, giving different importance to different samples.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "        <li>weights (Tensor): Weights assigned to each sample in the batch.</li>\n",
    "        <li>thresholds (Tensor): Threshold values for weighted computation.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, thresholds, weights):\n",
    "        super().__init__(thresholds, weights)\n",
    "\n",
    "    def loss_measure(self, input, target):\n",
    "        return MAELoss()(input, target)\n",
    "\n",
    "\n",
    "    \n",
    "class wMSLELoss(WeightedLoss):\n",
    "    \"\"\"\n",
    "    <p>Weighted Mean Squared Logarithmic Error Loss (wMSLELoss) is the weighted version of MSLE, penalizing underestimations more than overestimations.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "        <li>weights (Tensor): Weights assigned to each sample in the batch.</li>\n",
    "        <li>thresholds (Tensor): Threshold values for weighted computation.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, thresholds, weights):\n",
    "        super().__init__(thresholds, weights)\n",
    "    \n",
    "    def loss_measure(self, input, target):\n",
    "        return MSLELoss()(input, target)\n",
    "    \n",
    "\n",
    "\n",
    "class wRMSLELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    <p>Weighted Root Mean Squared Logarithmic Error Loss (wRMSLELoss) is the weighted version of RMSLE, useful for reducing the impact of outliers.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "        <li>weights (Tensor): Weights assigned to each sample in the batch.</li>\n",
    "        <li>thresholds (Tensor): Threshold values for weighted computation.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, thresholds, weights):\n",
    "        super().__init__()\n",
    "        self.msle_loss = wMSLELoss(thresholds, weights)\n",
    "        \n",
    "    def forward(self, input, target, reduction='mean'):\n",
    "        return torch.sqrt(self.msle_loss(input, target, reduction))\n",
    "    \n",
    "\n",
    "\n",
    "class wHubberLoss(WeightedLoss):\n",
    "    \"\"\"\n",
    "    <p>Weighted Huber Loss (wHubberLoss) combines the characteristics of both MSE and MAE, with weights for different samples.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: 'mean'.</li>\n",
    "        <li>delta (float): Threshold from where the loss changes from MAE to MSE-like functioning.</li>\n",
    "        <li>weights (Tensor): Weights assigned to each sample in the batch.</li>\n",
    "        <li>thresholds (Tensor): Threshold values for weighted computation.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, thresholds, weights, delta=2.0):\n",
    "        super().__init__(thresholds, weights)\n",
    "        self.delta = delta\n",
    "    \n",
    "    def loss_measure(self, y_pred, y_true):\n",
    "        return HubberLoss(delta=self.delta)(y_pred, y_true)\n",
    "    \n",
    "\n",
    "\n",
    "class wQuantileLoss(WeightedLoss):\n",
    "    \"\"\"\n",
    "    <p>Weighted Quantile Loss is used for regression tasks with weighted samples where we want to predict a specific quantile.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "        <li>quantile (float): The quantile to be predicted, usually a value between 0 and 1.</li>\n",
    "        <li>weights (Tensor): Weights assigned to each sample in the batch.</li>\n",
    "        <li>thresholds (Tensor): Threshold values for weighted computation.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, thresholds, weights, quantile=0.5):\n",
    "        super().__init__(thresholds, weights)\n",
    "        self.quantile = quantile\n",
    "    \n",
    "    def loss_measure(self, y_pred, y_true):\n",
    "        return QuantileLoss(quantile=self.quantile)(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Weighted Losses\n",
    "\n",
    "Here we add two special losses that do not use the predefined loss functions and they do not fully use the `WeightedLoss()` class or have their own implementation of the weights calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Loss\n",
    "\n",
    "$$\n",
    "\\text{Classification Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\; (1 - \\alpha) \\cdot \\mathcal{L}_i^{Bs \\times V \\times H} + \\alpha \\cdot \\mathcal{C}_i^{Bs \\times V}\\qquad \\text{where: } \\mathcal{C}_i = \\frac{1}{H} \\sum_{h=1}^{H} \\left| w_{y_{vh}}^{Bs \\times V \\times H} - w_{\\hat{y}_{vh}}^{Bs \\times V \\times H} \\right|\n",
    "\n",
    "$$\n",
    "\n",
    "This loss function classifies both the input and target tensors to determine how the model has misclassified the category of the forecasted values, penalizing predictions with larger discrepancies between the actual and predicted categories. The purpose of this loss is to penalize the model when it **fails to correctly identify the category of the values based on the available context**. This is particularly important in cases where the volatility of the data, such as FSMY solar indices, varies depending on the activity level (with higher volatility at higher activity levels).\n",
    "\n",
    "The weights are defined as a simple series from 1 to the number of variables, ensuring that the <u>steps between the designated categories are evenly spaced</u>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ClassificationLoss(WeightedLoss):\n",
    "    \"\"\"\n",
    "    <p>Loss function for classification tasks, suitable for handling imbalanced classes and other classification-specific challenges.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>reduction (str): Method for reducing the loss value across batches | <i><u>Default</u></i>: None.</li>\n",
    "        <li>primary_loss (Loss): The base loss function used for classification.</li>\n",
    "        <li>alpha (float): Weighting factor for balancing the importance of different classes.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, thresholds, primary_loss, alpha=0.5):\n",
    "        n_variables = len(thresholds.keys())\n",
    "        weights = {'All': np.arange(n_variables)}\n",
    "        super().__init__(thresholds, weights)\n",
    "\n",
    "        self.loss = primary_loss\n",
    "\n",
    "        if alpha < 0 or alpha > 1:\n",
    "            raise ValueError('Alpha must be between 0 and 1, as it is the weight of the categorical loss against the other loss.')\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def loss_measure(self, input, target):\n",
    "        primary_loss_value = self.loss(input, target, reduction=None)\n",
    "\n",
    "        categorical_error = torch.abs(self.weighted_loss_tensor(target) - self.weighted_loss_tensor(input))\n",
    "        categorical_loss_value = torch.mean(categorical_error, dim=2, keepdim=True)\n",
    "\n",
    "\n",
    "        return (1 - self.alpha) * primary_loss_value + self.alpha * categorical_loss_value\n",
    "\n",
    "\n",
    "    def forward(self, input, target, reduction='mean'):\n",
    "        error = self.loss_measure(input, target)\n",
    "\n",
    "        # if (error.shape != weights.shape): # To properly format the weights tensor in case of multi-variable classification\n",
    "          #   weights = weights.mean(dim=1)\n",
    "            \n",
    "        if reduction == 'mean':\n",
    "            loss = error.mean()\n",
    "        elif reduction == 'sum':\n",
    "            loss = error.sum()\n",
    "        else:\n",
    "            loss = error\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trended Loss\n",
    "\n",
    "$$\n",
    "\\text{Trended Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (1 + |\\tau(y) - \\tau(\\hat{y})|) \\cdot \\mathcal{L} \\qquad \\text{where}\\ \\tau() \\text{ is the trend calculation function}\n",
    "$$\n",
    "\n",
    "The aim of the Trended Loss function is to penalize the model when it **incorrectly detects the trend** of the data. To achieve this, it calculates the trend for both the input and target values, and then measures the difference between these trends. The performance of this loss function can be somewhat limited because the context available to the model is restricted to the horizon, as the batch is generated randomly and <u>cannot capture a larger context</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TrendedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    <p>Trended Loss incorporates trends in the data to adjust the loss computation accordingly.</p>\n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>primary_loss (Loss): The base loss function used in combination with trend adjustments.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    def __init__(self, primary_loss: Loss):\n",
    "        super().__init__()\n",
    "        self.loss = primary_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def _slope(y):\n",
    "        x = np.arange(len(y))\n",
    "        slope, _ = np.polyfit(x, y, deg=1)\n",
    "        return slope\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_trends(tensor):\n",
    "        np_tensor = tensor.cpu().detach().numpy()\n",
    "        trends = np.apply_along_axis(TrendedLoss._slope, 2, np_tensor)\n",
    "        return torch.Tensor(trends)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        batch, variables, _ = input.shape\n",
    "\n",
    "        input_trend = TrendedLoss._calculate_trends(input)\n",
    "        target_trend = TrendedLoss._calculate_trends(target)\n",
    "        \n",
    "        trend_diff = 1 + torch.abs(input_trend - target_trend)\n",
    "\n",
    "        error = self.loss(input, target)\n",
    "        weights = trend_diff.reshape(batch,variables,1)\n",
    "        loss = (error * weights).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Factory\n",
    "This class group and simplifies the creation process of the different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LossFactory:\n",
    "    losses = {\n",
    "        'MSE': MSELoss,\n",
    "        'MAE': MAELoss,\n",
    "        'MSLE': MSELoss,\n",
    "        'RMSLE': RMSLELoss,\n",
    "        'Hubber': HubberLoss,\n",
    "        'Quantile': QuantileLoss,\n",
    "        'wMSE': wMSELoss,\n",
    "        'wMAE': wMAELoss,\n",
    "        'wMSLE': wMSLELoss,\n",
    "        'wRMSLE': wRMSLELoss,\n",
    "        'wHubber': wHubberLoss,\n",
    "        'wQuantile': wQuantileLoss,\n",
    "        'Classification': ClassificationLoss,\n",
    "        'Trended': TrendedLoss\n",
    "    }\n",
    "\n",
    "    def __init__(self, thresholds, weights):\n",
    "        self.thresholds = thresholds\n",
    "        self.weights = weights\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        from IPython.display import HTML, display\n",
    "\n",
    "        table_rows = []\n",
    "        \n",
    "        # Generate rows for the table\n",
    "        for key, value in cls.losses.items():\n",
    "            doc_html = value.__doc__.strip().replace(\"\\n\", \" \")\n",
    "            table_rows.append(f\"<tr><td style='text-align: left;'><strong>{key}</strong></td><td style='text-align: left;'>{doc_html}</td></tr>\")\n",
    "        \n",
    "        # Create the HTML for the table with left-aligned text\n",
    "        table_html = f\"\"\"\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th style='text-align: left;'>Loss Name</th>\n",
    "                    <th style='text-align: left;'>Description</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                {''.join(table_rows)}\n",
    "            </tbody>\n",
    "        </table>\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(table_html))\n",
    "\n",
    "\n",
    "\n",
    "    def create(self, loss_name:str='MSE', **kwargs) -> nn.Module: \n",
    "        if loss_name in LossFactory.losses:\n",
    "            if loss_name.__contains__('w'):\n",
    "\n",
    "                if loss_name == 'hubber':\n",
    "                    delta = kwargs.get('delta', 2.0)\n",
    "                    return wHubberLoss(\n",
    "                            thresholds=self.thresholds, \n",
    "                            weights=self.weights, \n",
    "                            delta=delta\n",
    "                        )\n",
    "                \n",
    "                elif loss_name == 'quantile':\n",
    "                    quantile = kwargs.get('quantile', 0.5)\n",
    "                    return wQuantileLoss(\n",
    "                            thresholds=self.thresholds, \n",
    "                            weights=self.weights, \n",
    "                            quantile=quantile\n",
    "                        )\n",
    "                \n",
    "                else:\n",
    "                    return LossFactory.losses[loss_name](\n",
    "                                thresholds=self.thresholds, \n",
    "                                weights=self.weights\n",
    "                            )\n",
    "                \n",
    "            elif loss_name == 'classification':\n",
    "                alpha = kwargs.get('alpha', 0.5)\n",
    "                primary_loss = kwargs.get('primary_loss', MSELoss())\n",
    "                return ClassificationLoss(\n",
    "                            thresholds=self.thresholds,\n",
    "                            primary_loss=primary_loss,\n",
    "                            alpha=alpha\n",
    "                        )\n",
    "            \n",
    "            elif loss_name == 'trended':\n",
    "                primary_loss = kwargs.get('primary_loss', MSELoss())\n",
    "                return TrendedLoss(primary_loss=primary_loss)\n",
    "            \n",
    "            else:\n",
    "                return LossFactory.losses[loss_name]()\n",
    "        else:\n",
    "            raise ValueError(f'Loss {loss_name} not found. Available losses are: {list(cls.losses.keys())}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "This section contains the tests applied to all tensor loss calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "device = 'cpu'\n",
    "ranges = {'A': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'B': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'C': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'D': np.array([[0, 1], [1, 2], [2, 3], [3, 4]])}\n",
    "\n",
    "weights = {'A': np.array([1, 2, 3, 4])}\n",
    "\n",
    "target = torch.tensor([[[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]],\n",
    "                        [[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]]], device=device, dtype=torch.float32)\n",
    "\n",
    "input = target + 1\n",
    "\n",
    "expected_weights = torch.tensor([[[1, 2, 3, 4, 0, 0],\n",
    "                                 [1, 2, 3, 4, 0, 0],\n",
    "                                 [1, 2, 3, 4, 0, 0],\n",
    "                                 [1, 2, 3, 4, 0, 0]]], device=device, dtype=torch.float32)\n",
    "\n",
    "solact_levels = ['low', 'moderate', 'elevated', 'high']\n",
    "\n",
    "class DummyLoss(WeightedLoss):\n",
    "        def loss_measure(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "            pass\n",
    "\n",
    "def test_LossWeightsTensor():\n",
    "    loss = DummyLoss(ranges, weights).to(device)\n",
    "    result = loss.weighted_loss_tensor(target)\n",
    "\n",
    "    assert torch.equal(result, expected_weights), f\"Expected {expected_weights}, but got {result}\"\n",
    "    print(f\"Loss Tensor test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "thresholds_ne = {\n",
    "    'var1': [[0, 1], [1, 2], [2, 3]],\n",
    "    'var2': [[4, 5], [5, 6]],\n",
    "}\n",
    "\n",
    "weights_ne = {\n",
    "    'var1': [1, 2, 3],\n",
    "    'var2': [3, 4],\n",
    "}\n",
    "\n",
    "target_ne = torch.tensor([[[0.5,0.5,0.5,1.5],\n",
    "                         [4.5,4.5,5.5,4.5]]])\n",
    "\n",
    "expected_weights_ne = torch.tensor([[[1,1,1,2],\n",
    "                                   [3,3, 4, 3]]])\n",
    "\n",
    "def test_LossWeightsTensor_different_weights():\n",
    "    model = DummyLoss(thresholds_ne, weights_ne)\n",
    "    loss_tensor = model.weighted_loss_tensor(target_ne)\n",
    "    assert torch.equal(loss_tensor, expected_weights_ne), f\"Expected {expected_weights}, but got {loss_tensor}\"\n",
    "    print(\"Test for different weights per variable passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "def check_loss_function(loss_class, expected_value, loss_func=None):\n",
    "    if loss_class.__name__ == \"ClassificationLoss\":\n",
    "        loss = loss_class(ranges, loss_func).to(device)\n",
    "    elif loss_class.__name__ == \"TrendedLoss\":\n",
    "        loss = loss_class(loss_func).to(device)\n",
    "    else:\n",
    "        loss = loss_class(ranges, weights).to(device)\n",
    "    \n",
    "    result = loss(input, target)\n",
    "\n",
    "    assert torch.isclose(result, expected_value), f\"Expected {expected_value}, but got {result}\"\n",
    "    print(f\"{type(loss).__name__} test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "def test_wMSELoss():\n",
    "    expected_mse_loss = torch.mean(expected_weights * (target - input) ** 2)\n",
    "    check_loss_function(wMSELoss, expected_mse_loss)\n",
    "\n",
    "def test_wMAELoss():\n",
    "    expected_mae_loss = torch.mean(expected_weights * torch.abs(target - input))\n",
    "    check_loss_function(wMAELoss, expected_mae_loss)\n",
    "\n",
    "def test_wMSLELoss():\n",
    "    expected_msle_loss = torch.mean(expected_weights * ((torch.log1p(target) - torch.log1p(input)) ** 2))\n",
    "    check_loss_function(wMSLELoss, expected_msle_loss)\n",
    "\n",
    "def test_wRMSLELoss():\n",
    "    expected_msle_loss = torch.sqrt(torch.mean(expected_weights * (torch.log1p(target) - torch.log1p(input)) ** 2))\n",
    "    check_loss_function(wRMSLELoss, expected_msle_loss)\n",
    "\n",
    "def test_wHuberLoss():\n",
    "    delta = 1\n",
    "    expected_hubber_loss = torch.mean(expected_weights * \n",
    "                                   torch.where(torch.abs(input - target) < delta, \n",
    "                                                0.5 * (input - target) ** 2,\n",
    "                                                delta * (torch.abs(input - target) - 0.5 * delta)\n",
    "                                                )\n",
    "                                  )\n",
    "    check_loss_function(wHubberLoss, expected_hubber_loss)\n",
    "\n",
    "\n",
    "\n",
    "def test_ClassificationLoss():\n",
    "    expected_classification_loss = MSELoss('mean')(input, target)\n",
    "    check_loss_function(ClassificationLoss, expected_classification_loss, loss_func=MSELoss())\n",
    "\n",
    "def test_TrendedLoss():\n",
    "    expected_loss = torch.mean((target - input) ** 2) # The trend will be the same so the weights will be all 1\n",
    "    check_loss_function(TrendedLoss, expected_loss, loss_func=MSELoss())\n",
    "\n",
    "def test_wQuantileLoss():\n",
    "    quantile = 0.5  # You can change this to any quantile value you want to test\n",
    "    errors = np.abs(target - input)\n",
    "    expected_quantile_loss = torch.mean(expected_weights * torch.where(errors >= 0, quantile * errors, (quantile - 1) * errors))\n",
    "    \n",
    "    check_loss_function(lambda r, w: wQuantileLoss(r, w, quantile), expected_quantile_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wMSELoss test passed!\n",
      "wMAELoss test passed!\n",
      "wMSLELoss test passed!\n",
      "wRMSLELoss test passed!\n",
      "wHubberLoss test passed!\n",
      "wQuantileLoss test passed!\n",
      "ClassificationLoss test passed!\n",
      "TrendedLoss test passed!\n"
     ]
    }
   ],
   "source": [
    "#| Test\n",
    "# test_LossWeightsTensor()\n",
    "# test_LossWeightsTensor_different_weights()\n",
    "test_wMSELoss()\n",
    "test_wMAELoss()\n",
    "test_wMSLELoss()\n",
    "test_wRMSLELoss()\n",
    "test_wHuberLoss()\n",
    "test_wQuantileLoss()\n",
    "\n",
    "test_ClassificationLoss()\n",
    "test_TrendedLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from nbdev import *\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
