{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp losses\n",
    "\n",
    "#|export\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsai.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Functions\n",
    "\n",
    "---\n",
    "## Index\n",
    "- ### <a href=\"#loss-measures-implementation\">1. Loss Measures Implementation</a>\n",
    "  - #### <a href=\"#mean-squared-error-loss-mse\">1.1 Mean Squared Error Loss (MSE)</a>\n",
    "  - #### <a href=\"#mean-absolute-error-loss-mae\">1.2 Mean Absolute Error Loss (MAE)</a>\n",
    "  - #### <a href=\"#mean-squared-logarithmic-error-loss-msle\">1.3 Mean Squared Logarithmic Error Loss (MSLE)</a>\n",
    "  - #### <a href=\"#huber-loss-hl\">1.4 Huber Loss (HL)</a>\n",
    "  - #### <a href=\"#quantile-loss-ql\">1.5 Quantile Loss (QL)</a>\n",
    "- ### <a href=\"#weighted-losses\">2. Weighted Losses</a>\n",
    "  - #### <a href=\"#weighted-losses-using-the-pre-defined-loss-functions\">2.1 Weighted Losses Using the Pre-defined Loss Functions</a>\n",
    "  - #### <a href=\"#special-weighted-losses\">2.2 Special Weighted Losses</a>\n",
    "    - ##### <a href=\"#classification-loss\">2.2.1 Classification Loss</a>\n",
    "    - ##### <a href=\"#trended-loss\">2.2.2 Trended Loss</a>\n",
    "- ### <a href=\"#loss-metrics\">3. Loss Metrics</a>\n",
    "- ### <a href=\"#tests\">4. Tests</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document contains the implementation of all custom loss functions that can be used during the training process. We opted not to use the pre-existing functions available in `PyTorch`, as they are not specifically tailored to our needs. Implementing these loss functions from scratch allowed us to better control the weighting process and customize them according to our requirements. \n",
    "\n",
    "As a starting point, we implemented an abstract class for the loss function to work as an `nn.Module`. This class provides an option for loss reduction and includes an abstract method for loss computation, which is then used in the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Loss(nn.Module, ABC):\n",
    "    def __init__(self, reduction:str=None):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def _reduce(self, loss: torch.Tensor) -> torch.Tensor:\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum': return loss.sum()\n",
    "        return loss\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, reduction:str=None) -> torch.Tensor:\n",
    "        if reduction is not None:\n",
    "            self.reduction = reduction\n",
    "        loss = self._compute_loss(input, target)\n",
    "        return self._reduce(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Measures Implementation\n",
    "\n",
    "In this section, we provide the implementation of the unweighted loss functions. We have separated the implementations of the unweighted and weighted loss functions **to allow for a direct comparison of their performance**. This separation also enhances the versatility of our evaluation process. The decisions we made regarding which loss functions to implement are based on the work of Jadon, A. et al. (2024).\n",
    "\n",
    "<details>\n",
    "  <summary><u>References</u></summary>\n",
    "\n",
    "Jadon, A., Patil, A. & Jadon, S. A Comprehensive Survey of Regression-Based Loss Functions for Time Series Forecasting. arXiv: [2211.02989 [cs]](https://arxiv.org/abs/2211.02989). (2024). Preprint.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error Loss (MSE)\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "The Mean Squared Error (MSE) is a common loss function for regression problems, including time series forecasting. It calculates the average of the squared differences between predicted and actual values. MSE is popular because it is computationally efficient, differentiable, and works well with various optimization algorithms. However, MSE is highly sensitive to outliers, meaning that large errors can disproportionately influence the model's learning process. This sensitivity can be problematic in time series forecasting, where outliers might represent anomalies or unusual events rather than the underlying data pattern. Therefore, this loss function could be particularly well-suited for **periods where volatility is usual**, such as during high solar activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MSELoss(Loss):\n",
    "    def __init__(self, reduction:str=None):\n",
    "        super().__init__(reduction)\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return (target-input)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error Loss (MAE)\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "The Mean Absolute Error (MAE) is a widely used loss function in regression tasks, including time series forecasting. It quantifies the average of the absolute differences between predicted and actual values. Unlike Mean Squared Error (MSE), which squares the errors, MAE treats all errors linearly, making it more robust to outliers. However, MAE's linear scoring method can lead to less efficient convergence during optimization, especially when dealing with smaller errors. Despite this potential drawback, MAE remains a valuable loss function, particularly in situations where **minimizing the impact of outliers is crucial**, such as in cases with low activity levels where values are more stable but outliers may still occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MAELoss(Loss):\n",
    "    def __init__(self, reduction:str=None):\n",
    "        super().__init__(reduction)\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.abs(target-input)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Logarithmic Error Loss (MSLE)\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=0}^{N} (log(y_i + 1) - log(\\hat{y}_i + 1))^2\n",
    "$$\n",
    "\n",
    "The Mean Squared Logarithmic Error (MSLE) assesses the **relative difference between predicted and actual values** and is particularly useful when dealing with unscaled quantities. It mitigates the impact of large discrepancies in predictions for large values while remaining sensitive to smaller differences in predictions for smaller values. This characteristic stems from applying a logarithmic transformation to both the actual and predicted values before calculating the squared difference. MSLE is especially well-suited for situations where **underestimating values carries a higher penalty than overestimating them**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MSLELoss(Loss):\n",
    "    def __init__(self, reduction:str=None):\n",
    "        super().__init__(reduction)\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return (torch.log1p(input) - torch.log1p(target))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss (HL)\n",
    "\n",
    "$$\n",
    "\\text{HuberLoss} = \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y_i - \\hat{y}_i)^2 & \\text{if } |y_i - \\hat{y}_i| < \\delta \\\\\n",
    "\\delta \\times \\left(|y_i - \\hat{y}_i| - \\frac{1}{2} \\delta\\right) & \\text{if } |y_i - \\hat{y}_i| \\geq \\delta\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Huber Loss (HL) combines the characteristics of both MSE and MAE, aiming to benefit from their respective strengths while mitigating their limitations. This loss function introduces a parameter called delta (Î´) that acts as a threshold to determine the appropriate method for calculating the loss. For errors smaller than delta, Huber Loss behaves like MSE, employing a quadratic scoring function to facilitate efficient convergence. However, for errors exceeding delta, it transitions to a linear scoring approach akin to MAE, effectively reducing the influence of outliers. This adaptive behavior makes Huber Loss a versatile choice for time series forecasting, as it can handle datasets with **varying degrees of noise and outlier presence**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "    \n",
    "class HubberLoss(Loss):\n",
    "    def __init__(self, reduction:str=None, delta:float=1.):\n",
    "        super().__init__(reduction)\n",
    "        self.delta = delta\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        error = target - input\n",
    "        \n",
    "        is_small_error = error < self.delta\n",
    "        small_error_loss = (0.5 * (error ** 2))\n",
    "        large_error_loss = (self.delta * (torch.abs(error) - 0.5 * self.delta))\n",
    "\n",
    "        return torch.where(is_small_error, small_error_loss, large_error_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Loss (QL)\n",
    "\n",
    "$$\n",
    "\\text{Quantile Loss} = \\frac{1}{N} \\sum_{i=1}^{N}\n",
    "\\begin{cases} \n",
    "(\\gamma - 1) \\cdot (y_i - \\hat{y}_i) & \\text{if  } y_i < \\hat{y}_i \\\\\n",
    "\\gamma \\cdot (y_i - \\hat{y}_i) & \\text{if  } y_i \\geq \\hat{y}_i\n",
    "\\end{cases}\n",
    "\n",
    "$$\n",
    "\n",
    "Quantile Loss (QL) is particularly useful when the goal is to predict not just a single point estimate but rather a range of possible outcomes with associated probabilities. This loss function is used in quantile regression, a type of regression analysis that estimates the conditional quantiles of the target variable given a set of predictor variables. Quantile Loss is defined based on the desired quantile ($\\gamma$) and penalizes overestimations and underestimations differently depending on the value of $\\gamma$.\n",
    "\n",
    "For instance, when $\\gamma = 0.5$, the loss function aims to estimate the median, penalizing both overestimations and underestimations equally. However, for other values of $\\gamma$, the penalties are adjusted to reflect the desired quantile. In our case, by **adjusting the quantile to higher percentiles**, we can focus more on **outliers**, such as solar storms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class QuantileLoss(Loss):\n",
    "    def __init__(self, quantile: float, reduction: str = None):\n",
    "        super().__init__(reduction)\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def _compute_loss(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        errors = target - input\n",
    "        return torch.where(errors >= 0, self.quantile * errors, (self.quantile - 1) * errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Losses\n",
    "\n",
    "Here we implement the addition of a **weights tensor** to the loss functions. As discussed in our paper, we introduce **weighted loss functions** to assign more importance to levels that appear to be **underrepresented** in the training data, thereby giving them greater relevance.\n",
    "\n",
    "As shown below, we have created a **weighted loss superclass** where the weights are calculated and applied. The core method of this class involves calculating the weight tensor. In this superclass, we take the thresholds, reshape the target tensor and thresholds so they can be directly compared, classify the target tensor, and then apply the weights using the `torch.einsum()` function. The final part of the function applies the weights, whether they are **equal for all variables** or **different for each variable**.\n",
    "\n",
    "Additionally, a **data preprocessing method** is used because some thresholds and weights may have different shapes, which can result in errors due to unaligned shapes that do not fit properly with the weight calculation method. To address this, padding is added to variables that have fewer categories. **Reduction is recalculated** as the weights are applied directly to the error tensor, not to the reduced one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class WeightedLoss(nn.Module, ABC):\n",
    "    def __init__(self, thresholds:dict, weights:dict):\n",
    "        super().__init__()\n",
    "\n",
    "        # Activity levels' weights can be equal across all variables or different,\n",
    "        # and this should be taken into account during preprocessing. \n",
    "        self.all_variables_have_same_weights = len(weights.keys()) == 1\n",
    "        ranges, weights = self._preprocess_data(thresholds, weights)\n",
    "\n",
    "        self.register_buffer('ranges', torch.Tensor(ranges))\n",
    "        self.register_buffer('weights', torch.Tensor(weights))\n",
    "\n",
    "\n",
    "    def weighted_loss_tensor(self, target: torch.Tensor) -> torch.Tensor:        \n",
    "        batch, variables, horizon = target.shape  # Example shape (32, 4, 6)\n",
    "        variable, max_range, interval = self.ranges.shape  # Example shape (4, 4, 2)\n",
    "\n",
    "        print()\n",
    "\n",
    "        target_shaped = torch.reshape(target, (batch, variables, 1, horizon))  # Example shape (32, 4, 6) -> (32, 4, 1, 6)\n",
    "        ranges_shaped = torch.reshape(self.ranges, (variable, max_range, 1, interval))  # Example shape (4, 4, 2) -> (4, 4, 1, 2)\n",
    "\n",
    "        weights_tensor = ((ranges_shaped[..., 0] <= target_shaped) & (target_shaped <= ranges_shaped[..., 1])).float()\n",
    "             \n",
    "        if self.all_variables_have_same_weights:\n",
    "            equation = 'r,bvrh->bvh'\n",
    "        else:\n",
    "            equation = 'vr,bvrh->bvh'\n",
    "\n",
    "        return torch.einsum(equation, self.weights, weights_tensor)\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss_measure(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def _preprocess_data(self, thresholds, weights):\n",
    "        # If each variable has its own weights, calculate the maximum size of weights.\n",
    "        # Padding shorter weights with NaNs prevents heterogeneous tensor errors.\n",
    "        if (self.all_variables_have_same_weights):\n",
    "            ranges = np.array(list(thresholds.values())[:])\n",
    "            weights = np.array(next(iter(weights.values())))\n",
    "        else:\n",
    "            def add_padding(x, padding_value, shape):\n",
    "                result = np.full(shape, padding_value)\n",
    "                for i, r in enumerate(x):\n",
    "                    result[i, :len(r)] = r\n",
    "                return result\n",
    "            \n",
    "            max_size = max([len(array) for array in thresholds.values()])\n",
    "\n",
    "            ranges_raw = thresholds.values()\n",
    "            ranges = add_padding(ranges_raw, np.nan, (len(ranges_raw), max_size, 2))\n",
    "\n",
    "            weights_raw = [weights[key] for key in thresholds.keys()]\n",
    "            weights = add_padding(weights_raw, 0.0, (len(weights_raw), max_size))\n",
    "\n",
    "        return ranges, weights\n",
    "    \n",
    "    \n",
    "    def forward(self, y_pred, y_true, reduction='mean'):\n",
    "        error = self.loss_measure(y_pred, y_true)\n",
    "        weights = self.weighted_loss_tensor(y_true)\n",
    "\n",
    "        if reduction == 'mean':\n",
    "            loss = (error * weights).mean()\n",
    "        elif reduction == 'sum':\n",
    "            loss = (error * weights).sum()\n",
    "        else: \n",
    "            loss = error*weights\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Losses Using the Pre-defined Loss Functions\n",
    "\n",
    "Here you can find the weighted versions of the loss functions implemented in this document. All functions follow the structure below when applying the weights:\n",
    "\n",
    "$$ \n",
    "\\text{Weighted Loss} = \\frac{1}{N} \\sum_{i=1}^{N} w^{Bs \\times V \\times H} \\cdot \\mathcal{L}^{Bs \\times V \\times H} \\qquad \\text{where}: \n",
    "\\begin{cases}\n",
    "    w: \\text{Weight tensor} \\\\\n",
    "    Bs: \\text{Batch size} \\\\\n",
    "    V: \\text{Number of variables} \\\\\n",
    "    H: \\text{Horizon length} \\\\\n",
    "    \\mathcal{L}: \\text{Loss measure}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class wMSELoss(WeightedLoss):\n",
    "    def __init__(self, thresholds, weights):\n",
    "        super().__init__(thresholds, weights)\n",
    "\n",
    "    \n",
    "    def loss_measure(self, input, target):\n",
    "        return MSELoss()(input, target)\n",
    "    \n",
    "\n",
    "    \n",
    "class wMAELoss(WeightedLoss):\n",
    "    def __init__(self, thresholds, weights):\n",
    "        super().__init__(thresholds, weights)\n",
    "\n",
    "    def loss_measure(self, input, target):\n",
    "        return MAELoss()(input, target)\n",
    "\n",
    "\n",
    "    \n",
    "class wMSLELoss(WeightedLoss):\n",
    "    def __init__(self, thresholds, weights):\n",
    "        super().__init__(thresholds, weights)\n",
    "    \n",
    "    def loss_measure(self, input, target):\n",
    "        return MSLELoss()(input, target)\n",
    "    \n",
    "\n",
    "\n",
    "class wHubberLoss(WeightedLoss):\n",
    "    def __init__(self, thresholds, weights, delta=2.0):\n",
    "        super().__init__(thresholds, weights)\n",
    "        self.delta = delta\n",
    "    \n",
    "    def loss_measure(self, y_pred, y_true):\n",
    "        return HubberLoss(delta=self.delta)(y_pred, y_true)\n",
    "    \n",
    "\n",
    "\n",
    "class wQuantileLoss(WeightedLoss):\n",
    "    def __init__(self, thresholds, weights, quantile=0.5):\n",
    "        super().__init__(thresholds, weights)\n",
    "        self.quantile = quantile\n",
    "    \n",
    "    def loss_measure(self, y_pred, y_true):\n",
    "        return QuantileLoss(quantile=self.quantile)(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Weighted Losses\n",
    "\n",
    "Here we add two special losses that do not use the predefined loss functions and they do not fully use the `WeightedLoss()` class or have their own implementation of the weights calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Loss\n",
    "\n",
    "$$\n",
    "\\text{Classification Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (1+|w_{y}^{Bs \\times V \\times H} - w_{\\hat{y}}^{Bs \\times V \\times H}|) \\cdot \\mathcal{L}^{Bs \\times V \\times H}\n",
    "$$\n",
    "\n",
    "This loss function classifies both the input and target tensors to determine how the model has misclassified the category of the forecasted values, penalizing predictions with larger discrepancies between the actual and predicted categories. The purpose of this loss is to penalize the model when it **fails to correctly identify the category of the values based on the available context**. This is particularly important in cases where the volatility of the data, such as FSMY solar indices, varies depending on the activity level (with higher volatility at higher activity levels).\n",
    "\n",
    "The weights are defined as a simple series from 1 to the number of variables, ensuring that the <u>steps between the designated categories are evenly spaced</u>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ClassificationLoss(WeightedLoss):\n",
    "    def __init__(self, thresholds, loss):\n",
    "        n_variables = len(thresholds.keys())\n",
    "        weights = {'All': np.arange(n_variables)}\n",
    "\n",
    "        super().__init__(thresholds, weights)\n",
    "\n",
    "        self.loss = loss\n",
    "    \n",
    "    def loss_measure(self, input, target):\n",
    "        return self.loss(input, target)\n",
    "\n",
    "    def forward(self, input, target, reduction='mean'):\n",
    "        error = self.loss_measure(input, target)\n",
    "        weights = 1 + torch.abs(self.weighted_loss_tensor(target) - self.weighted_loss_tensor(input))\n",
    "\n",
    "        if (error.shape != weights.shape): # To properly format the weights tensor in case of multi-variable classification\n",
    "            weights = weights.mean(dim=1)\n",
    "            \n",
    "        if reduction == 'mean':\n",
    "            loss = (error * weights).mean()\n",
    "        elif reduction == 'sum':\n",
    "            loss = (error * weights).sum()\n",
    "        else:\n",
    "            loss = error * weights\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trended Loss\n",
    "\n",
    "$$\n",
    "\\text{Trended Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (1 + |\\tau(y) - \\tau(\\hat{y})|) \\cdot \\mathcal{L} \\qquad \\text{where}\\ \\tau() \\text{ is the trend calculation function}\n",
    "$$\n",
    "\n",
    "The aim of the Trended Loss function is to penalize the model when it **incorrectly detects the trend** of the data. To achieve this, it calculates the trend for both the input and target values, and then measures the difference between these trends. The performance of this loss function can be somewhat limited because the context available to the model is restricted to the horizon, as the batch is generated randomly and <u>cannot capture a larger context</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class TrendedLoss(nn.Module):\n",
    "    def __init__(self, loss: Loss):\n",
    "        super().__init__()\n",
    "        self.loss = loss\n",
    "\n",
    "    @staticmethod\n",
    "    def _slope(y):\n",
    "        x = np.arange(len(y))\n",
    "        slope, _ = np.polyfit(x, y, deg=1)\n",
    "        return slope\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_trends(tensor):\n",
    "        np_tensor = tensor.cpu().detach().numpy()\n",
    "        trends = np.apply_along_axis(TrendedLoss._slope, 2, np_tensor)\n",
    "        return torch.Tensor(trends)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        batch, variables, _ = input.shape\n",
    "\n",
    "        input_trend = TrendedLoss._calculate_trends(input)\n",
    "        target_trend = TrendedLoss._calculate_trends(target)\n",
    "        \n",
    "        trend_diff = 1 + torch.abs(input_trend - target_trend)\n",
    "\n",
    "        error = self.loss(input, target)\n",
    "        weights = trend_diff.reshape(batch,variables,1)\n",
    "        loss = (error * weights).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Metrics\n",
    "\n",
    "We have implemented a class that generates relevant metrics to better evaluate the performance of the weighted loss function. This class calculates how much of the loss is associated with each activity level, providing **deeper insights into the model's behavior**. The number of methods is elevated because each condition requires its own function to be coded, as dynamically generating these functions can lead to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class LossMetrics:\n",
    "    def __init__(self, loss_func, solact_levels):\n",
    "        self.loss_func = loss_func\n",
    "        self.solact_levels = solact_levels\n",
    "\n",
    "    # Weighted Regressive Loss Metrics\n",
    "    def _apply_weighted_loss_by_level(self, input, target, weight_idx):\n",
    "        loss_copy = deepcopy(self.loss_func)\n",
    "        \n",
    "    \n",
    "        for idx1 in range(len(loss_copy.weights)):\n",
    "            if is_iter(loss_copy.weights[0]):\n",
    "                for idx2 in range(len(loss_copy.weights[idx1])):\n",
    "                    if (idx1 != weight_idx[0]) | (idx2 != weight_idx[1]):\n",
    "                        loss_copy.weights[idx1][idx2] = 0\n",
    "            else:\n",
    "                if idx1 != weight_idx[1]:\n",
    "                    loss_copy.weights[idx1] = 0\n",
    "                \n",
    "        return loss_copy(input, target)\n",
    "    \n",
    "    \n",
    "    # Classification Loss Metrics\n",
    "    def _compute_misclassifications(self, predictions, targets):\n",
    "        classifier = self.loss_func.weighted_loss_tensor\n",
    "        true_labels = classifier(targets)\n",
    "        predicted_labels = classifier(predictions)\n",
    "\n",
    "        misclassified_labels = (true_labels != predicted_labels).int() * predicted_labels\n",
    "\n",
    "        return misclassified_labels.unique(return_counts=True)\n",
    "\n",
    "    def _count_misclassifications_by_level(self, predictions, targets, level):\n",
    "        unique_labels, label_counts = self._compute_misclassifications(predictions, targets)\n",
    "        label_count_dict = dict(zip(unique_labels.tolist(), label_counts.tolist()))\n",
    "\n",
    "        return label_count_dict.get(level, 0)\n",
    "    \n",
    "\n",
    "    # Metrics functions\n",
    "    ## FSMY Metrics\n",
    "    def loss_low(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,0])\n",
    "    \n",
    "    def loss_moderate(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,1])\n",
    "    \n",
    "    def loss_elevated(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,2])\n",
    "    \n",
    "    def loss_high(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,3])\n",
    "    \n",
    "    ## DST-AP Metrics\n",
    "    def loss_Low(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,0])\n",
    "    \n",
    "    def loss_Medium(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,1])\n",
    "    \n",
    "    def loss_Active(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,2])\n",
    "    \n",
    "    def loss_G0(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,0])\n",
    "    \n",
    "    def loss_G1(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,1])\n",
    "    \n",
    "    def loss_G2(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,2])\n",
    "        \n",
    "    def loss_G3(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,3])\n",
    "    \n",
    "    def loss_G4(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,4])\n",
    "    \n",
    "    def loss_G5(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,5])\n",
    "    \n",
    "    ## ClassificationLoss metrics\n",
    "    def missclassifications_low(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_level(predictions, targets, 1)\n",
    "    \n",
    "    def missclassifications_moderate(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_level(predictions, targets, 2)\n",
    "    \n",
    "    def missclassifications_elevated(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_level(predictions, targets, 3)\n",
    "    \n",
    "    def missclassifications_high(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_level(predictions, targets, 4)\n",
    "    \n",
    "    ## Metrics Not Available\n",
    "    def Metrics_Not_Available(self, input, target): return np.nan \n",
    "    \n",
    "\n",
    "    # Metrics retrieval\n",
    "    def get_metrics(self):\n",
    "        if isinstance(self.loss_func, ClassificationLoss):\n",
    "            return [self.missclassifications_low, self.missclassifications_moderate, self.missclassifications_elevated, self.missclassifications_high]\n",
    "        \n",
    "        elif isinstance(self.loss_func, WeightedLoss):\n",
    "            if isinstance(self.solact_levels, list): # FSMY metrics required\n",
    "                return [self.loss_low, self.loss_moderate, self.loss_elevated, self.loss_high]\n",
    "            else: # DST-AP metrics required\n",
    "                return [self.loss_Low, self.loss_Medium, self.loss_Active, self.loss_G0, self.loss_G1, self.loss_G2, self.loss_G3, self.loss_G4, self.loss_G5]\n",
    "        else:\n",
    "            return [self.Metrics_Not_Available]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "This section contains the tests applied to all tensor loss calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "device = 'cpu'\n",
    "ranges = {'A': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'B': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'C': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'D': np.array([[0, 1], [1, 2], [2, 3], [3, 4]])}\n",
    "\n",
    "weights = {'A': np.array([1, 2, 3, 4])}\n",
    "\n",
    "target = torch.tensor([[[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]]], device=device, dtype=torch.float32)\n",
    "\n",
    "input = target + 1\n",
    "\n",
    "expected_weights = torch.tensor([[[1, 2, 3, 4, 0, 0],\n",
    "                                 [1, 2, 3, 4, 0, 0],\n",
    "                                 [1, 2, 3, 4, 0, 0],\n",
    "                                 [1, 2, 3, 4, 0, 0]]], device=device, dtype=torch.float32)\n",
    "\n",
    "solact_levels = ['low', 'moderate', 'elevated', 'high']\n",
    "\n",
    "class DummyLoss(WeightedLoss):\n",
    "        def loss_measure(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "            pass\n",
    "\n",
    "def test_LossWeightsTensor():\n",
    "    loss = DummyLoss(ranges, weights).to(device)\n",
    "    result = loss.weighted_loss_tensor(target)\n",
    "\n",
    "    assert torch.equal(result, expected_weights), f\"Expected {expected_weights}, but got {result}\"\n",
    "    print(f\"Loss Tensor test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "thresholds_ne = {\n",
    "    'var1': [[0, 1], [1, 2], [2, 3]],\n",
    "    'var2': [[4, 5], [5, 6]],\n",
    "}\n",
    "\n",
    "weights_ne = {\n",
    "    'var1': [1, 2, 3],\n",
    "    'var2': [3, 4],\n",
    "}\n",
    "\n",
    "target_ne = torch.tensor([[[0.5,0.5,0.5,1.5],\n",
    "                         [4.5,4.5,5.5,4.5]]])\n",
    "\n",
    "expected_weights_ne = torch.tensor([[[1,1,1,2],\n",
    "                                   [3,3, 4, 3]]])\n",
    "\n",
    "def test_LossWeightsTensor_different_weights():\n",
    "    model = DummyLoss(thresholds_ne, weights_ne)\n",
    "    loss_tensor = model.weighted_loss_tensor(target_ne)\n",
    "    assert torch.equal(loss_tensor, expected_weights_ne), f\"Expected {expected_weights}, but got {loss_tensor}\"\n",
    "    print(\"Test for different weights per variable passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "def check_loss_function(loss_class, expected_value, loss_func=None):\n",
    "    if loss_class.__name__ == \"ClassificationLoss\":\n",
    "        loss = loss_class(ranges, loss_func).to(device)\n",
    "    elif loss_class.__name__ == \"TrendedLoss\":\n",
    "        loss = loss_class(loss_func).to(device)\n",
    "    else:\n",
    "        loss = loss_class(ranges, weights).to(device)\n",
    "    \n",
    "    result = loss(input, target)\n",
    "\n",
    "    assert torch.isclose(result, expected_value), f\"Expected {expected_value}, but got {result}\"\n",
    "    print(f\"{type(loss).__name__} test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "def test_wMSELoss():\n",
    "    expected_mse_loss = torch.mean(expected_weights * (target - input) ** 2)\n",
    "    check_loss_function(wMSELoss, expected_mse_loss)\n",
    "\n",
    "def test_wMAELoss():\n",
    "    expected_mae_loss = torch.mean(expected_weights * torch.abs(target - input))\n",
    "    check_loss_function(wMAELoss, expected_mae_loss)\n",
    "\n",
    "def test_wMSLELoss():\n",
    "    expected_msle_loss = torch.mean(expected_weights * ((torch.log1p(target) - torch.log1p(input)) ** 2))\n",
    "    check_loss_function(wMSLELoss, expected_msle_loss)\n",
    "\n",
    "def test_wHuberLoss():\n",
    "    delta = 1\n",
    "    expected_hubber_loss = torch.mean(expected_weights * \n",
    "                                   torch.where(torch.abs(input - target) < delta, \n",
    "                                                0.5 * (input - target) ** 2,\n",
    "                                                delta * (torch.abs(input - target) - 0.5 * delta)\n",
    "                                                )\n",
    "                                  )\n",
    "    check_loss_function(wHubberLoss, expected_hubber_loss)\n",
    "\n",
    "\n",
    "\n",
    "def test_ClassificationLoss():\n",
    "    expected_classification_loss = MSELoss('mean')(input, target)\n",
    "    check_loss_function(ClassificationLoss, expected_classification_loss, loss_func=MSELoss())\n",
    "\n",
    "def test_TrendedLoss():\n",
    "    expected_loss = torch.mean((target - input) ** 2) # The trend will be the same so the weights will be all 1\n",
    "    check_loss_function(TrendedLoss, expected_loss, loss_func=MSELoss())\n",
    "\n",
    "def test_wQuantileLoss():\n",
    "    quantile = 0.5  # You can change this to any quantile value you want to test\n",
    "    errors = np.abs(target - input)\n",
    "    expected_quantile_loss = torch.mean(expected_weights * torch.where(errors >= 0, quantile * errors, (quantile - 1) * errors))\n",
    "    \n",
    "    check_loss_function(lambda r, w: wQuantileLoss(r, w, quantile), expected_quantile_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test_LossMetrics():\n",
    "    loss = wMAELoss(ranges, weights).to(device)\n",
    "    metrics = LossMetrics(loss, solact_levels).get_metrics()\n",
    "\n",
    "    loss_value = loss(input, target)\n",
    "    metrics_values = [metric(input, target) for metric in metrics]\n",
    "\n",
    "    assert torch.isclose(loss_value, sum(metrics_values)), f\"Expected {loss_value}, but got {sum(metrics_values)} ({metrics_values})\"\n",
    "    print(\"LossMetrics test passed!\")\n",
    "\n",
    "def test_LossMetrics_for_classification():\n",
    "    loss = ClassificationLoss(ranges, MSELoss()).to(device)\n",
    "    metrics = LossMetrics(loss, solact_levels).get_metrics()\n",
    "\n",
    "    total_counts = 0\n",
    "    for i in range(1, 5):\n",
    "        total_counts += LossMetrics(loss, solact_levels)._count_misclassifications_by_level(input, target, i) \n",
    "\n",
    "    metrics_values = [metric(input, target) for metric in metrics]\n",
    "\n",
    "    assert np.isclose(total_counts, sum(metrics_values)), f\"Expected {total_counts}, but got {sum(metrics_values)} ({metrics_values})\"\n",
    "    print(\"LossMetrics for classification loss test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss Tensor test passed!\n",
      "\n",
      "Test for different weights per variable passed!\n",
      "\n",
      "wMSELoss test passed!\n",
      "\n",
      "wMAELoss test passed!\n",
      "\n",
      "wMSLELoss test passed!\n",
      "\n",
      "wHubberLoss test passed!\n",
      "\n",
      "wQuantileLoss test passed!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LossMetrics test passed!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LossMetrics for classification loss test passed!\n",
      "\n",
      "\n",
      "ClassificationLoss test passed!\n",
      "TrendedLoss test passed!\n"
     ]
    }
   ],
   "source": [
    "#| Test\n",
    "test_LossWeightsTensor()\n",
    "test_LossWeightsTensor_different_weights()\n",
    "test_wMSELoss()\n",
    "test_wMAELoss()\n",
    "test_wMSLELoss()\n",
    "test_wHuberLoss()\n",
    "test_wQuantileLoss()\n",
    "test_LossMetrics()\n",
    "test_LossMetrics_for_classification()\n",
    "\n",
    "input -= 1 # To make it equal to target and has no effect on the loss\n",
    "test_ClassificationLoss()\n",
    "test_TrendedLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from nbdev import *\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
