{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastcore.all import *\n",
    "from tsai.basics import SlidingWindow\n",
    "from tsai.utils import load_object\n",
    "from collections import Counter\n",
    "from itertools import combinations, chain\n",
    "import more_itertools as mit\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import papermill as pm\n",
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def get_idxs_per_solar_activity_level(data, thresholds):\n",
    "    # function that splits the data of a variable into 4 different sets, \n",
    "    # one for each solar activity level. The data comes as a numpy array with \n",
    "    # shape (samples, steps), and the split is done along the samples axis. \n",
    "    # The decision is made based on the first column of each sample. The function \n",
    "    # returns a list of 4 numpy arrays, one for each solar activity level. \n",
    "    # But it does not return the values, it returns the indices of the\n",
    "    # samples that belong to each solar activity level.\n",
    "    idxs_per_solar_activity_level = []\n",
    "    for i in range(len(thresholds) + 1):\n",
    "        if i == 0:\n",
    "            idxs = np.where(data[:, 0] <= thresholds[i])[0]\n",
    "        elif i == len(thresholds):\n",
    "            idxs = np.where(data[:, 0] > thresholds[i-1])[0]\n",
    "        else:\n",
    "            idxs = np.where((data[:, 0] > thresholds[i-1]) & (data[:, 0] <= thresholds[i]))[0]\n",
    "        idxs_per_solar_activity_level.append(idxs)\n",
    "    return idxs_per_solar_activity_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Solar Activity Level Indices: [0 1]\n",
      "Moderate Solar Activity Level Indices: [2 3]\n",
      "Elevated Solar Activity Level Indices: [4 5]\n",
      "High Solar Activity Level Indices: [6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# Sample data: 10 samples, each with 5 steps\n",
    "data = np.array([\n",
    "    [10, 1, 2, 3, 4],\n",
    "    [15, 1, 2, 3, 4],\n",
    "    [25, 1, 2, 3, 4],\n",
    "    [35, 1, 2, 3, 4],\n",
    "    [45, 1, 2, 3, 4],\n",
    "    [55, 1, 2, 3, 4],\n",
    "    [65, 1, 2, 3, 4],\n",
    "    [75, 1, 2, 3, 4],\n",
    "    [85, 1, 2, 3, 4],\n",
    "    [95, 1, 2, 3, 4]\n",
    "])\n",
    "\n",
    "# Thresholds to define solar activity levels\n",
    "thresholds = [20, 40, 60]\n",
    "\n",
    "# Call the function with the test data\n",
    "splits = get_idxs_per_solar_activity_level(data, thresholds)\n",
    "\n",
    "# Print the resulting splits\n",
    "activity_levels = ['Low', 'Moderate', 'Elevated', 'High']\n",
    "\n",
    "for i, level in enumerate(activity_levels):\n",
    "    print(f\"{level} Solar Activity Level Indices: {splits[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def convert_uuids_to_indices():\n",
    "    cuda_visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
    "    uuids = re.findall(r\"\\b[0-9a-fA-F]{8}(?:-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}\\b\", cuda_visible_devices)\n",
    "\n",
    "    if uuids:\n",
    "        indices = [str(i) for i in range(len(uuids))]\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified CUDA_VISIBLE_DEVICES: 0,1\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "def test_convert_uuids_to_indices():\n",
    "    # Mock the CUDA_VISIBLE_DEVICES environment variable with UUIDs\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"GPU-123e4567-e89b-12d3-a456-426614174000,GPU-89e4567e-b89b-12d3-a456-426614174111\"\n",
    "    \n",
    "    # Call the function to convert UUIDs to indices\n",
    "    convert_uuids_to_indices()\n",
    "    \n",
    "    # Print the modified CUDA_VISIBLE_DEVICES\n",
    "    print(\"Modified CUDA_VISIBLE_DEVICES:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# Run the test\n",
    "test_convert_uuids_to_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_classified_columns (df: pd.DataFrame, thresholds:dict, activity_levels:dict):\n",
    "    \"\"\"\n",
    "    Creates classified columns based on predefined ranges for specified columns in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with classification classification of each column.\n",
    "\n",
    "    \"\"\"\n",
    "    columns_to_classify = df.columns.intersection(thresholds.keys())\n",
    "\n",
    "    if columns_to_classify.empty:\n",
    "        return df\n",
    "    else:\n",
    "        df_cat = pd.DataFrame()\n",
    "        for column in columns_to_classify:\n",
    "            # ranges tuples come as strings in the yaml file, so we need to convert them to tuples with eval\n",
    "            bins = pd.IntervalIndex.from_tuples(thresholds[column])\n",
    "            df_cat[f'{column}_Cat'] = np.array(activity_levels[column])[pd.cut(df[column], bins=bins).cat.codes]\n",
    "        return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    F10_Cat   S10_Cat   M10_Cat   Y10_Cat\n",
      "0       low       low       low       low\n",
      "1  moderate  moderate  moderate  moderate\n",
      "2  elevated  elevated  elevated  elevated\n",
      "3      high      high      high      high\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "\n",
    "data = {\n",
    "    'F10': [50, 100, 160, 200],\n",
    "    'S10': [30, 70, 170, 220],\n",
    "    'M10': [60, 100, 150, 170],\n",
    "    'Y10': [50, 90, 150, 170]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "thresholds = {\n",
    "    'F10': [(0,75), (76,150), (151,190), (191, df['F10'].max())],\n",
    "    'S10': [(0,65), (66,150), (151,215), (216, df['S10'].max())],\n",
    "    'M10': [(0,72), (73,144), (145,167), (168, df['M10'].max())],\n",
    "    'Y10': [(0,81), (82,148), (149,165), (166, df['Y10'].max())]\n",
    "}\n",
    "\n",
    "activity_levels = {\n",
    "    'F10': ['low', 'moderate', 'elevated', 'high'],\n",
    "    'S10': ['low', 'moderate', 'elevated', 'high'],\n",
    "    'M10': ['low', 'moderate', 'elevated', 'high'],\n",
    "    'Y10': ['low', 'moderate', 'elevated', 'high']\n",
    "}\n",
    "\n",
    "\n",
    "# Expected result\n",
    "expected_data = {\n",
    "    'F10_Cat': ['low', 'moderate', 'elevated', 'high'],\n",
    "    'S10_Cat': ['low', 'moderate', 'elevated', 'high'],\n",
    "    'M10_Cat': ['low', 'moderate', 'elevated', 'high'],\n",
    "    'Y10_Cat': ['low', 'moderate', 'elevated', 'high']\n",
    "}\n",
    "expected_df = pd.DataFrame(expected_data)\n",
    "\n",
    "result_df = get_classified_columns(df, thresholds=thresholds, activity_levels=activity_levels)\n",
    "\n",
    "print(result_df.head())\n",
    "\n",
    "pd.testing.assert_frame_equal(result_df, expected_df)\n",
    "print(\"Test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def euclidean_distance_dict(X:dict, Y:dict):\n",
    "    return math.sqrt(sum((X.get(d,0) - Y.get(d,0))**2 for d in set(X) | set(Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "\n",
    "# Test case 1: Basic test with non-overlapping keys\n",
    "X1 = {'a': 1, 'b': 2, 'c': 3}\n",
    "Y1 = {'d': 4, 'e': 5, 'f': 6}\n",
    "expected_distance1 = np.sqrt(1**2 + 2**2 + 3**2 + 4**2 + 5**2 + 6**2)\n",
    "assert np.isclose(euclidean_distance_dict(X1, Y1), expected_distance1), f\"Test case 1 failed\"\n",
    "\n",
    "# Test case 2: Basic test with overlapping keys\n",
    "X2 = {'a': 1, 'b': 2, 'c': 3}\n",
    "Y2 = {'a': 1, 'b': 2, 'c': 4}\n",
    "expected_distance2 = np.sqrt(0**2 + 0**2 + 1**2)\n",
    "assert np.isclose(euclidean_distance_dict(X2, Y2), expected_distance2), f\"Test case 2 failed\"\n",
    "\n",
    "# Test case 3: Basic test with some overlapping and some non-overlapping keys\n",
    "X3 = {'a': 1, 'b': 2}\n",
    "Y3 = {'b': 2, 'c': 3}\n",
    "expected_distance3 = np.sqrt(1**2 + 0**2 + 3**2)\n",
    "assert np.isclose(euclidean_distance_dict(X3, Y3), expected_distance3), f\"Test case 3 failed\"\n",
    "\n",
    "# Test case 4: Test with empty dictionaries\n",
    "X4 = {}\n",
    "Y4 = {}\n",
    "expected_distance4 = 0\n",
    "assert np.isclose(euclidean_distance_dict(X4, Y4), expected_distance4), f\"Test case 4 failed\"\n",
    "\n",
    "# Test case 5: Test with one empty dictionary\n",
    "X5 = {'a': 1, 'b': 2}\n",
    "Y5 = {}\n",
    "expected_distance5 = np.sqrt(1**2 + 2**2)\n",
    "assert np.isclose(euclidean_distance_dict(X5, Y5), expected_distance5), f\"Test case 5 failed\"\n",
    "\n",
    "print(\"All test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def find_closest_distribution(df_cat, target_distribution, segment_size, val_size):\n",
    "    \"\"\"\n",
    "    Finds the combination of segments in the categorical data that is closest to the target distribution.\n",
    "\n",
    "    Parameters:\n",
    "    df_cat (pd.Series): A pandas Series containing the categorical data.\n",
    "    target_distribution (dict): The target distribution to compare against, given as a dictionary where keys are categories and values are their target proportions.\n",
    "    segment_size (int): The size of each segment to split the data into.\n",
    "    val_size (float): The proportion of the validation split.\n",
    "\n",
    "    Returns:\n",
    "    best_combination (tuple): The indices of the segments that form the closest combination to the target distribution.\n",
    "    segments (list): The list of segments created from the data.\n",
    "    distribution_found (dict): The distribution of categories in the best combination of segments.\n",
    "    \"\"\"\n",
    "    idxs = list(df_cat.index)\n",
    "    segments = np.array_split(idxs, len(df_cat) // segment_size)\n",
    "\n",
    "    value_counts = [df_cat[segments[i]].value_counts().to_dict() for i in range(len(segments))]\n",
    "\n",
    "    num_segments = int(len(segments)*(val_size))\n",
    "    print(f\"Total number of segments:{ len(segments)}, Number of segments for validation: {num_segments} ({num_segments/len(segments)*100:.2f}%)\")\n",
    "\n",
    "    \n",
    "    best_combination = None\n",
    "    best_distance = np.inf\n",
    "    distribution_found = None\n",
    "    comb = combinations(range(len(value_counts)), num_segments)\n",
    "    for c in tqdm(comb):\n",
    "        values = Counter({})\n",
    "        for i in c:\n",
    "            values = values + Counter(value_counts[i])\n",
    "        total = sum(values.values(), 0.0)\n",
    "        distribution = {k: v / total for k, v in values.items()}\n",
    "        \n",
    "        distance = euclidean_distance_dict(distribution, target_distribution)\n",
    "\n",
    "        if distance < best_distance:\n",
    "            best_distance = distance\n",
    "            best_combination = c\n",
    "            distribution_found = distribution\n",
    "    print(\"The closest group of segments to F10.7 categories has an euclidean distance of\", best_distance)\n",
    "    return best_combination, segments, distribution_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of segments:4, Number of segments for validation: 1 (25.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 12018.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest group of segments to F10.7 categories has an euclidean distance of 0.17320508075688773\n",
      "Best combination of segments: [0]\n",
      "Distribution found: {'A': 0.4, 'B': 0.2, 'C': 0.2, 'D': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "target_distribution = {'A': 0.25, 'B': 0.25, 'C': 0.25, 'D': 0.25}\n",
    "\n",
    "data = {\n",
    "    'category': ['A', 'B', 'A', 'C', 'D', 'B', 'A', 'C', 'D', 'B', 'A', 'C', 'D', 'B', 'A', 'C', 'D', 'B', 'A', 'C']\n",
    "}\n",
    "df_cat = pd.Series(data['category'])\n",
    "\n",
    "# Function parameters\n",
    "segment_size = 5\n",
    "val_size = 0.4\n",
    "\n",
    "best_combination, segments, distribution_found = find_closest_distribution(df_cat, target_distribution, segment_size, val_size)\n",
    "\n",
    "print(\"Best combination of segments:\", list(best_combination))\n",
    "print(\"Distribution found:\", distribution_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sliding_window_generator(df, split_start, data_columns, config, comb=None, segments=None):\n",
    "    consecutive_elements, X, y = None, None, None\n",
    "\n",
    "    if comb is not None:\n",
    "        consecutive_elements = [list(group) for group in mit.consecutive_groups(comb)]\n",
    "\n",
    "        df_to_window = []\n",
    "        for elements in consecutive_elements:\n",
    "            best_comb_idxs = [segments[i] for i in elements]\n",
    "            df_to_window.append(df.iloc[chain.from_iterable(best_comb_idxs)])\n",
    "    else:\n",
    "        df_to_window = [df]\n",
    "\n",
    "    X_window, y_window = None, None \n",
    "    for df_window in df_to_window:    \n",
    "        X_window, y_window = SlidingWindow(\n",
    "            window_len=config.lookback,\n",
    "            horizon=config.horizon, \n",
    "            stride=1, \n",
    "            get_x=data_columns, \n",
    "            get_y=data_columns\n",
    "        )(df_window)\n",
    "        X = np.concatenate([X, X_window]) if X is not None else X_window\n",
    "        y = np.concatenate([y, y_window]) if y is not None else y_window\n",
    "    \n",
    "    \n",
    "    splits = L(list(np.arange(split_start, len(X)+split_start)))\n",
    "    return X, y, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[[0 1]\n",
      "  [0 1]]\n",
      "\n",
      " [[6 7]\n",
      "  [6 7]]]\n",
      "y:\n",
      " [[2 2]\n",
      " [8 8]]\n",
      "splits:\n",
      " [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "data = {\n",
    "    'A': range(10),\n",
    "    'B': range(10)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Parameters\n",
    "split_start = 0\n",
    "data_columns = ['A', 'B']\n",
    "comb = [0, 2]\n",
    "segments = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "config = AttrDict({'lookback': 2, 'horizon': 1})\n",
    "\n",
    "# Testing the function\n",
    "X, y, splits = sliding_window_generator(df, split_start, data_columns, config, comb, segments)\n",
    "\n",
    "# Print outputs for verification\n",
    "print(\"X:\\n\", X)\n",
    "print(\"y:\\n\", y)\n",
    "print(\"splits:\\n\", splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_dst_data(start_date: str = '01/1957',\n",
    "                      end_date: str = pd.Timestamp.today(),\n",
    "                      save_folder: str = \"./dst_data\"):\n",
    "    \"\"\"\n",
    "    Downloads Dst index data between the specified start and end dates.\n",
    "\n",
    "    :param start_date: Start date in the format 'MM/YYYY'\n",
    "    :param end_date: End date in the format 'MM/YYYY'\n",
    "    :param save_folder: Folder where the data files should be saved\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Initialize file path\n",
    "    file_name = \"DST_IAGA2002.txt\"\n",
    "    file_path = os.path.join(save_folder, file_name)\n",
    "\n",
    "\n",
    "    # Remove existing file if it exists\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Deleted existing file: {file_path}\")\n",
    "\n",
    "    # Convert input dates to datetime objects\n",
    "    start_dt = pd.to_datetime(start_date, format='%m/%Y')\n",
    "    end_dt = pd.to_datetime(end_date, format='%m/%Y')\n",
    "\n",
    "    # HTTP REQUEST COMPONENTS\n",
    "    current_start = start_dt\n",
    "    while current_start <= end_dt:\n",
    "        current_end = min(current_start + pd.DateOffset(years=24), end_dt)\n",
    "\n",
    "        # Extract year components\n",
    "        SCent = current_start.year // 100\n",
    "        STens = (current_start.year % 100) // 10\n",
    "        SYear = current_start.year % 10\n",
    "        SMonth = current_start.month\n",
    "\n",
    "        ECent = current_end.year // 100\n",
    "        ETens = (current_end.year % 100) // 10\n",
    "        EYear = current_end.year % 10\n",
    "        EMonth = current_end.month\n",
    "\n",
    "        # Construct URL for current chunk\n",
    "        url = f\"https://wdc.kugi.kyoto-u.ac.jp/cgi-bin/dstae-cgi?\" \\\n",
    "              f\"SCent={SCent}&\" \\\n",
    "              f\"STens={STens}&\" \\\n",
    "              f\"SYear={SYear}&\" \\\n",
    "              f\"SMonth={SMonth:02d}&\" \\\n",
    "              f\"ECent={ECent}&\" \\\n",
    "              f\"ETens={ETens}&\" \\\n",
    "              f\"EYear={EYear}&\" \\\n",
    "              f\"EMonth={EMonth:02d}&\" \\\n",
    "              \"Image+Type=GIF&COLOR=COLOR&AE+Sensitivity=0&Dst+Sensitivity=0&Output=DST&Out+format=IAGA2002\"\n",
    "\n",
    "        headers = {\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Referer\": \"https://wdc.kugi.kyoto-u.ac.jp/dstae/index.html\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            session = requests.session()\n",
    "            response = session.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Raise an error for bad responses\n",
    "\n",
    "            # Append or write to file\n",
    "            mode = 'ab' if os.path.exists(file_path) else 'wb'\n",
    "            with open(file_path, mode) as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            print(f\"Downloaded and saved data from {current_start.strftime('%m/%Y')} to {current_end.strftime('%m/%Y')}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to download data: {e}\")\n",
    "\n",
    "        # Move to the next chunk\n",
    "        current_start = current_end + pd.DateOffset(days=1)\n",
    "\n",
    "    print(f\"All data downloaded and saved to {file_path}\")\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved data from 01/1957 to 01/1981\n",
      "Downloaded and saved data from 01/1981 to 01/2005\n",
      "Downloaded and saved data from 01/2005 to 09/2024\n",
      "All data downloaded and saved to ./dst_data/DST_IAGA2002.txt\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import shutil\n",
    "\n",
    "download_dst_data()\n",
    "shutil.rmtree(\"./dst_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def generate_preprocessed_data(config, generate_preproc_pipe=True, generate_exp_pipe=True):\n",
    "    result = []\n",
    "    try:\n",
    "        df = load_object(config.df_save_path)\n",
    " \n",
    "    except FileNotFoundError:\n",
    "        output = './tmp/data_out.ipynb'\n",
    "        print(f\"{config.df_save_path} not found. Executing the notebook to generate the data...\")\n",
    "        \n",
    "        pm.execute_notebook(config.data_nb, output)\n",
    "        os.remove(output)\n",
    "\n",
    "        print(\"Data generated successfully.\")\n",
    "\n",
    "    results = [load_object(config.df_save_path)]\n",
    "\n",
    "    if generate_preproc_pipe:\n",
    "        results.append(load_object(config.preproc_pipe_save_path))\n",
    "\n",
    "    if generate_exp_pipe:\n",
    "        results.append(load_object(config.exp_pipe_save_path))\n",
    "\n",
    "    return *results,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method has been extracted from [tsai library](https://timeseriesai.github.io/tsai/optuna.html) in order to modify it to accept multiobjective studies, without doing a full fork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pathlib import Path\n",
    "from fastcore.script import *\n",
    "import joblib\n",
    "from importlib import import_module\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def run_optuna_study(objective, resume=None, study_type=None, multivariate=True, search_space=None, evaluate=None, seed=None, sampler=None, pruner=None, \n",
    "                     study_name=None, direction='maximize', n_trials=None, timeout=None, gc_after_trial=False, show_progress_bar=True, \n",
    "                     save_study=True, path='optuna', show_plots=True):\n",
    "    r\"\"\"Creates and runs an optuna study.\n",
    "\n",
    "    Args: \n",
    "        objective:          A callable that implements objective function.\n",
    "        resume:             Path to a previously saved study.\n",
    "        study_type:         Type of study selected (bayesian, gridsearch, randomsearch). Based on this a sampler will be build if sampler is None. \n",
    "                            If a sampler is passed, this has no effect.\n",
    "        multivariate:       If this is True, the multivariate TPE is used when suggesting parameters. The multivariate TPE is reported to outperform \n",
    "                            the independent TPE.\n",
    "        search_space:       Search space required when running a gridsearch (if you don't pass a sampler).\n",
    "        evaluate:           Allows you to pass a specific set of hyperparameters that will be evaluated.\n",
    "        seed:               Fixed seed used by samplers.\n",
    "        sampler:            A sampler object that implements background algorithm for value suggestion. If None is specified, TPESampler is used during \n",
    "                            single-objective optimization and NSGAIISampler during multi-objective optimization. See also samplers.\n",
    "        pruner:             A pruner object that decides early stopping of unpromising trials. If None is specified, MedianPruner is used as the default. \n",
    "                            See also pruners.\n",
    "        study_name:         Study’s name. If this argument is set to None, a unique name is generated automatically.\n",
    "        direction:          A sequence of directions during multi-objective optimization.\n",
    "        n_trials:           The number of trials. If this argument is set to None, there is no limitation on the number of trials. If timeout is also set to \n",
    "                            None, the study continues to create trials until it receives a termination signal such as Ctrl+C or SIGTERM.\n",
    "        timeout:            Stop study after the given number of second(s). If this argument is set to None, the study is executed without time limitation. \n",
    "                            If n_trials is also set to None, the study continues to create trials until it receives a termination signal such as \n",
    "                            Ctrl+C or SIGTERM.\n",
    "        gc_after_trial:     Flag to execute garbage collection at the end of each trial. By default, garbage collection is enabled, just in case. \n",
    "                            You can turn it off with this argument if memory is safely managed in your objective function.\n",
    "        show_progress_bar:  Flag to show progress bars or not. To disable progress bar, set this False.\n",
    "        save_study:         Save your study when finished/ interrupted.\n",
    "        path:               Folder where the study will be saved.\n",
    "        show_plots:         Flag to control whether plots are shown at the end of the study.\n",
    "    \"\"\"\n",
    "    \n",
    "    try: import optuna\n",
    "    except ImportError: raise ImportError('You need to install optuna to use run_optuna_study')\n",
    "\n",
    "    # Sampler\n",
    "    if sampler is None:\n",
    "        if study_type is None or \"bayes\" in study_type.lower(): \n",
    "            sampler = optuna.samplers.TPESampler(seed=seed, multivariate=multivariate)\n",
    "        elif \"grid\" in study_type.lower():\n",
    "            assert search_space, f\"you need to pass a search_space dict to run a gridsearch\"\n",
    "            sampler = optuna.samplers.GridSampler(search_space)\n",
    "        elif \"random\" in study_type.lower(): \n",
    "            sampler = optuna.samplers.RandomSampler(seed=seed)\n",
    "    assert sampler, \"you need to either select a study type (bayesian, gridsampler, randomsampler) or pass a sampler\"\n",
    "\n",
    "    # Study\n",
    "    if resume: \n",
    "        try:\n",
    "            study = joblib.load(resume)\n",
    "        except: \n",
    "            print(f\"joblib.load({resume}) couldn't recover any saved study. Check the path.\")\n",
    "            return\n",
    "        print(\"Best trial until now:\")\n",
    "        print(\" Value: \", study.best_trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in study.best_trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "    else: \n",
    "        study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=study_name, directions=direction)\n",
    "    if evaluate: study.enqueue_trial(evaluate)\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=n_trials, timeout=timeout, gc_after_trial=gc_after_trial, show_progress_bar=show_progress_bar)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    # Save\n",
    "    if save_study:\n",
    "        full_path = Path(path)/f'{study.study_name}.pkl'\n",
    "        full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(study, full_path)\n",
    "        print(f'\\nOptuna study saved to {full_path}')\n",
    "        print(f\"To reload the study run: study = joblib.load('{full_path}')\")\n",
    "\n",
    "    # Plots\n",
    "    if show_plots and len(study.trials) > 1:\n",
    "        try: display(optuna.visualization.plot_optimization_history(study))\n",
    "        except: pass\n",
    "        try: display(optuna.visualization.plot_param_importances(study))\n",
    "        except: pass\n",
    "        try: display(optuna.visualization.plot_slice(study))\n",
    "        except: pass\n",
    "        try: display(optuna.visualization.plot_parallel_coordinate(study))\n",
    "        except: pass\n",
    "\n",
    "    # Study stats\n",
    "    try:\n",
    "        pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "        complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        print(f\"\\nStudy statistics    : \")\n",
    "        print(f\"  Study name        : {study.study_name}\")\n",
    "        print(f\"  # finished trials : {len(study.trials)}\")\n",
    "        print(f\"  # pruned trials   : {len(pruned_trials)}\")\n",
    "        print(f\"  # complete trials : {len(complete_trials)}\")\n",
    "        \n",
    "        print(f\"\\nBest trial          :\")\n",
    "        trial = study.best_trial\n",
    "        print(f\"  value             : {trial.value}\")\n",
    "        print(f\"  best_params = {trial.params}\\n\")\n",
    "    except:\n",
    "        print('\\nNo finished trials yet.')\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def filter_nb (path:str, skip_tags:list):\n",
    "    \"\"\"\n",
    "    Filters out cells with tags in skip_tags from a notebook.\n",
    "\n",
    "    Args:\n",
    "    path (str): Path to the notebook file.\n",
    "    skip_tags (list): List of tags to filter out.\n",
    "\n",
    "    Returns:\n",
    "    nb (nbformat.NotebookNode): The filtered notebook.\n",
    "    \"\"\"\n",
    "    nb = nbformat.read(path, as_version=4)\n",
    "\n",
    "    filtered_cells = [cell for cell in nb.cells if not set(skip_tags) & set(cell.metadata.get('tags', []))]\n",
    "    nb.cells = filtered_cells\n",
    "    \n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_vectorized_condition_function(geo_thresholds_dict, sol_thresholds_dict, geo_levels_dict, solact_levels_dict):\n",
    "    \"\"\"\n",
    "    Creates a vectorized function that returns the condition based on geomagnetic and solar values.\n",
    "    \n",
    "    Input:\n",
    "    geo_thresholds_dict: Dictionary of thresholds for the geomagnetic indices (e.g., 'AP', 'DST')\n",
    "    sol_thresholds_dict: Dictionary of thresholds for the solar indices (e.g., 'F10', 'S10')\n",
    "    geo_levels_dict: Dictionary of activity levels for geomagnetic indices\n",
    "    solact_levels_dict: Dictionary of activity levels for solar indices\n",
    "    \n",
    "    Output:\n",
    "    A vectorized function that can be used to compute conditions for arrays of geo_values and sol_values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_combined_condition(geo_index, geo_value, sol_index, sol_value):\n",
    "        \"\"\"\n",
    "        Function that returns the condition based on the geo_value and sol_value.\n",
    "        Input:\n",
    "            geo_index: The name of the geomagnetic index (e.g., 'AP', 'DST')\n",
    "            geo_value: Value of the geomagnetic index (float)\n",
    "            sol_index: The name of the solar index (e.g., 'F10', 'S10')\n",
    "            sol_value: Value of the solar index (float)\n",
    "        Output:\n",
    "            condition: Condition string that combines geomagnetic and solar conditions (string)\n",
    "        \"\"\"\n",
    "        geo_thresholds = geo_thresholds_dict[geo_index]\n",
    "        sol_thresholds = sol_thresholds_dict[sol_index]\n",
    "        geo_levels = geo_levels_dict[geo_index]\n",
    "        solact_levels = solact_levels_dict[sol_index]\n",
    "\n",
    "        geo_condition = None\n",
    "        sol_condition = None\n",
    "\n",
    "        # Determine geomagnetic condition\n",
    "        for i, (lower, upper) in enumerate(geo_thresholds):\n",
    "            if lower < geo_value <= upper:\n",
    "                geo_condition = geo_levels[i]\n",
    "                break\n",
    "\n",
    "        # Determine solar condition\n",
    "        for j, (lower, upper) in enumerate(sol_thresholds):\n",
    "            if lower < sol_value <= upper:\n",
    "                sol_condition = solact_levels[j]\n",
    "                break\n",
    "\n",
    "        # Combine conditions\n",
    "        if geo_condition and sol_condition:\n",
    "            if geo_index == 'AP':\n",
    "                return f'{geo_condition}Geo_{sol_condition.capitalize()}'\n",
    "            else: \n",
    "                return f'{geo_condition}_{sol_condition.capitalize()}'\n",
    "        else:\n",
    "            return 'Unknown'  # Fallback if no condition is found\n",
    "\n",
    "    # Return the vectorized version of the get_combined_condition function\n",
    "    return np.vectorize(get_combined_condition)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G0_Moderate' 'G0_Elevated' 'G0_High']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# Geomagnetic thresholds and levels (e.g., 'AP' and 'DST')\n",
    "geo_thresholds = {\n",
    "    \"AP\": [\n",
    "        (-np.inf, 10),  # Low\n",
    "        (10, 50),       # Moderate\n",
    "        (50, np.inf)    # Active\n",
    "    ],\n",
    "    \"DST\": [\n",
    "        (-30, np.inf),  # G0\n",
    "        (-50, -30),     # G1\n",
    "        (-90, -50),     # G2\n",
    "        (-130, -90),    # G3\n",
    "        (-350, -130),   # G4\n",
    "        (-np.inf, -350) # G5\n",
    "    ]\n",
    "}\n",
    "\n",
    "geo_levels = {\n",
    "    'AP': [\"Low\", \"Moderate\", \"Active\"],\n",
    "    'DST': [\"G0\", \"G1\", \"G2\", \"G3\", \"G4\", \"G5\"]\n",
    "}\n",
    "\n",
    "# Solar thresholds and levels (e.g., 'F10')\n",
    "sol_thresholds = {\n",
    "    'F10': [(0,75), (76,150), (151,190), (191, 99999)]\n",
    "}\n",
    "\n",
    "solact_levels = {\n",
    "    'F10': ['low', 'moderate', 'elevated', 'high']\n",
    "}\n",
    "\n",
    "# Create the vectorized function with parameters loaded\n",
    "condition_function = create_vectorized_condition_function(geo_thresholds, sol_thresholds, geo_levels, solact_levels)\n",
    "\n",
    "# Example data (for geomagnetic index AP and solar index F10)\n",
    "geo_values = np.array([5, 20, 60])  # AP values\n",
    "sol_values = np.array([80, 160, 200])  # F10 values\n",
    "\n",
    "# Now execute the preloaded vectorized function\n",
    "conditions = condition_function('DST', geo_values, 'F10', sol_values)\n",
    "print(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from nbdev import *\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
