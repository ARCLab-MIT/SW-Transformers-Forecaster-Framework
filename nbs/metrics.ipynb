{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp metrics\n",
    "\n",
    "#|export\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsai.basics import *\n",
    "from swdf.losses import wMAELoss, MSELoss, WeightedLoss, ClassificationLoss\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from optuna.study import StudyDirection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "---\n",
    "## Index\n",
    "#### [1 Loss Metrics](#loss-metrics)\n",
    "  - [1.1 Regression Metrics](#regression-metrics)\n",
    "    - [1.1.1 Solar Indices FSMY 10.7 Metrics](#solar-indices-fsmy-107-metrics)\n",
    "    - [1.1.2 Geomagnetic Indices DST and AP Metrics](#geomagnetic-indices-dst-and-ap-metrics)\n",
    "  - [1.2 Classification Metrics](#classification-metrics)\n",
    "    - [1.2.1 Solar Indices FSMY 10.7 Metrics](#solar-indices-fsmy-107-metrics)\n",
    "    - [1.2.2 Geomagnetic Indices DST and AP Metrics](#geomagnetic-indices-dst-and-ap-metrics)\n",
    "  - [1.3 Loss Metrics Retrieval](#loss-metrics-retrieval)\n",
    "#### [2 Validation Metrics](#validation-metrics)\n",
    "  - [2.1 Outliers Evaluation Metrics](#outliers-evaluation-metrics)\n",
    "    - [2.1.1 F1 Score Metric](#f1-score-metric)\n",
    "    - [2.1.2 Area Under the Precision-Recall Curve (AUPRC)](#area-under-the-precision-recall-curve-auprc)\n",
    "    - [2.1.3 Kurtois and Skewness Difference (KSD)](#kurtois-and-skewness-difference-ksd)\n",
    "  - [2.2 Association Metrics](#association-metrics)\n",
    "    - [2.2.1 Pearson Linear Correlation Coefficient ($R$)](#pearson-linear-correlation-coefficient-r)\n",
    "    - [2.2.2 Coefficient of Determination Metric ($R^2$)](#coefficient-of-determination-metric-r2)\n",
    "  - [2.3 Accuracy Metrics](#accuracy-metrics)\n",
    "    - [2.3.1 Symmetric Mean Absolute Percentage Error Metric (sMAPE)](#symmetric-mean-absolute-percentage-error-metric-smape)\n",
    "    - [2.3.1 Median Symmetric Accuracy Metric (MSA)](#median-symmetric-accuracy-metric-msa)\n",
    "  - [2.4 Bias Metrics](#bias-metrics)\n",
    "    - [2.4.1 Symmetric Signed Percentage Bias Metric (SSPB)](#symmetric-signed-percentage-bias-metric-sspb)\n",
    "  - [2.5 Validation Metrics Handler](#validation-metrics-handler)\n",
    "#### [3 Tests](#tests)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have implemented metrics to be used within the `TSForecaster()` to provide relevant insights into the training process. Additionally, we have compiled a selection of metrics that can be used to compare the performance of each model in the forecasting task. All the classes follow the pattern of the `Metrics()` class, which is implemented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class Metrics(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Metrics\n",
    "\n",
    "We have implemented a class that generates relevant metrics to better evaluate the performance of the weighted loss function. This class calculates how much of the loss is associated with each activity level, providing **deeper insights into the model's behavior**. The number of methods is elevated because each condition requires its own function to be coded, as dynamically generating these functions can lead to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics\n",
    "\n",
    "Here we have implemented metrics to determine the portion of the loss represented by each of the categories in the weighted losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class RegressiveMetrics(Metrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def _apply_weighted_loss_by_level(self, input, target, weight_idx):\n",
    "        loss_copy = deepcopy(self.loss_func)\n",
    "        \n",
    "        for idx1 in range(len(loss_copy.weights)):\n",
    "            if is_iter(loss_copy.weights[0]):\n",
    "                for idx2 in range(len(loss_copy.weights[idx1])):\n",
    "                    if (idx1 != weight_idx[0]) | (idx2 != weight_idx[1]):\n",
    "                        loss_copy.weights[idx1][idx2] = 0\n",
    "            else:\n",
    "                if idx1 != weight_idx[1]:\n",
    "                    loss_copy.weights[idx1] = 0\n",
    "                \n",
    "        return loss_copy(input, target)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solar Indices FSMY 10.7 Metrics\n",
    "\n",
    "Here we calculate how each solar activity category contributes to the overall loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class SOLFMYMetrics(RegressiveMetrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__(loss_func)\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Loss_Low(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,0])\n",
    "    \n",
    "    def Loss_Moderate(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,1])\n",
    "    \n",
    "    def Loss_Elevated(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,2])\n",
    "    \n",
    "    def Loss_High(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,3])\n",
    "    \n",
    "    \n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [\n",
    "                self.Loss_Low, \n",
    "                self.Loss_Moderate, \n",
    "                self.Loss_Elevated, \n",
    "                self.Loss_High\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geomagnetic Indices DST and AP Metrics\n",
    "\n",
    "Below, we compile the performance of each category within the geomagnetic indices studied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class GEODSTAPMetrics(RegressiveMetrics):\n",
    "    def __init__(self, loss_func, indices:str='geodstap'):\n",
    "        super().__init__(loss_func)\n",
    "        self.indices = indices\n",
    "        \n",
    "        \n",
    "    # Metrics\n",
    "    def Loss_Low(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,0])\n",
    "    \n",
    "    def Loss_Medium(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,1])\n",
    "    \n",
    "    def Loss_Active(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,2])\n",
    "    \n",
    "    def Loss_G0(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,0])\n",
    "    \n",
    "    def Loss_G1(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,1])\n",
    "    \n",
    "    def Loss_G2(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,2])\n",
    "        \n",
    "    def Loss_G3(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,3])\n",
    "    \n",
    "    def Loss_G4(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,4])\n",
    "    \n",
    "    def Loss_G5(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,5])\n",
    "    \n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        if self.indices == 'geodst':\n",
    "            return [\n",
    "                self.Loss_G0, \n",
    "                self.Loss_G1, \n",
    "                self.Loss_G2, \n",
    "                self.Loss_G3, \n",
    "                self.Loss_G4, \n",
    "                self.Loss_G5\n",
    "            ]\n",
    "        \n",
    "        elif self.indices == 'geoap':\n",
    "            return [\n",
    "                    self.Loss_Low, \n",
    "                    self.Loss_Medium, \n",
    "                    self.Loss_Active\n",
    "                ]\n",
    "        \n",
    "        return [\n",
    "                self.Loss_Low, \n",
    "                self.Loss_Medium, \n",
    "                self.Loss_Active,\n",
    "                self.Loss_G0, \n",
    "                self.Loss_G1, \n",
    "                self.Loss_G2, \n",
    "                self.Loss_G3, \n",
    "                self.Loss_G4, \n",
    "                self.Loss_G5\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "\n",
    "These metrics are used to assess the average misclassifications detected by the `ClassificationLoss()`, providing insights into how the model is improving when the solar or geomagnetic categories of the predictions impact the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class ClassificationMetrics(Metrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "\n",
    "    def _compute_misclassifications(self, predictions, targets):\n",
    "        # Use the weighted loss tensor from the provided loss function\n",
    "        classifier = self.loss_func.weighted_loss_tensor\n",
    "        \n",
    "        # Get the true and predicted labels using the classifier\n",
    "        true_labels = classifier(targets)\n",
    "        predicted_labels = classifier(predictions)\n",
    "\n",
    "        # Misclassifications are those where the predicted label does not match the true label\n",
    "        misclassified_labels = (true_labels != predicted_labels).int() * predicted_labels\n",
    "\n",
    "        return misclassified_labels\n",
    "\n",
    "    def _count_misclassifications_by_position(self, predictions, targets, row, col):\n",
    "        # Calculate misclassifications for a specific (row, column) pair\n",
    "        misclassified_labels = self._compute_misclassifications(predictions, targets)\n",
    "        \n",
    "        # Extract the specific misclassification at the (row, column) position and sum across the time dimension\n",
    "        if row < misclassified_labels.shape[1] and col < misclassified_labels.shape[2]:\n",
    "            misclassification_count = misclassified_labels[:, row, col].sum().item()\n",
    "        else:\n",
    "            misclassification_count = 0  # Out of bounds, assume no misclassification\n",
    "        \n",
    "        return misclassification_count\n",
    "  \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solar Indices FSMY 10.7 Metrics\n",
    "\n",
    "This metrics calculate the average of missclasifications for each of the solar activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class SOLFMYClassificationMetrics(ClassificationMetrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__(loss_func)\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Missclassifications_Low(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 1)\n",
    "\n",
    "    def Missclassifications_Moderate(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 2)\n",
    "\n",
    "    def Missclassifications_Elevated(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 3)\n",
    "\n",
    "    def Missclassifications_High(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 4)\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [\n",
    "                self.Missclassifications_Low,\n",
    "                self.Missclassifications_Moderate, \n",
    "                self.Missclassifications_Elevated, \n",
    "                self.Missclassifications_High\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geomagnetic Indices DST and AP Metrics\n",
    "\n",
    "This metrics calculate the average of missclasifications for each of DST and AP activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class GEODSTAPClassificationMetrics(ClassificationMetrics):\n",
    "    def __init__(self, loss_func, indices:str='geodstap'):\n",
    "        super().__init__(loss_func)\n",
    "        self.indices = indices\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Missclassifications_Low(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 1)\n",
    "\n",
    "    def Missclassifications_Medium(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 2)\n",
    "\n",
    "    def Missclassifications_Active(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 3)\n",
    "\n",
    "    def Missclassifications_G0(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 1)\n",
    "\n",
    "    def Missclassifications_G1(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 2)\n",
    "\n",
    "    def Missclassifications_G2(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 3)\n",
    "\n",
    "    def Missclassifications_G3(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 4)\n",
    "\n",
    "    def Missclassifications_G4(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 5)\n",
    "\n",
    "    def Missclassifications_G5(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 6)\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        if self.indices == 'geodst':\n",
    "            return [\n",
    "                    self.Missclassifications_G0,\n",
    "                    self.Missclassifications_G1, \n",
    "                    self.Missclassifications_G2, \n",
    "                    self.Missclassifications_G3, \n",
    "                    self.Missclassifications_G4, \n",
    "                    self.Missclassifications_G5\n",
    "                ]\n",
    "        \n",
    "        elif self.indices == 'geoap':\n",
    "            return [\n",
    "                    self.Missclassifications_Low, \n",
    "                    self.Missclassifications_Medium, \n",
    "                    self.Missclassifications_Active\n",
    "                ]\n",
    "        \n",
    "        return [\n",
    "                self.Missclassifications_Low, \n",
    "                self.Missclassifications_Medium, \n",
    "                self.Missclassifications_Active, \n",
    "                self.Missclassifications_G0, \n",
    "                self.Missclassifications_G1,\n",
    "                self.Missclassifications_G2,\n",
    "                self.Missclassifications_G3,\n",
    "                self.Missclassifications_G4,\n",
    "                self.Missclassifications_G5\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Metrics Retrieval\n",
    "\n",
    "This class consolidates all the logic needed to retrieve the appropriate metrics, depending on the type of data used in the training and the specific loss function applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class LossMetrics(Metrics):\n",
    "    def __init__(self, loss_func, indices:str = ''):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.indices = indices\n",
    "\n",
    "    ## Metrics Not Available\n",
    "    def Metrics_Not_Available(self, input, target): return np.nan \n",
    "    \n",
    "    # Metrics retrieval\n",
    "    def get_metrics(self):\n",
    "        if isinstance(self.loss_func, ClassificationLoss):\n",
    "            if self.indices.lower() == 'solfsmy':\n",
    "                return SOLFMYClassificationMetrics(self.loss_func).get_metrics()\n",
    "            if self.indices.lower() in ['geodstap', 'geoap', 'geodst']:\n",
    "                return GEODSTAPClassificationMetrics(self.loss_func, self.indices).get_metrics()\n",
    "        \n",
    "        if isinstance(self.loss_func, WeightedLoss):\n",
    "            if self.indices.lower() == 'solfsmy':\n",
    "                return SOLFMYMetrics(self.loss_func).get_metrics()\n",
    "            \n",
    "            if self.indices.lower() in ['geodstap', 'geoap', 'geodst']:\n",
    "                return GEODSTAPMetrics(self.loss_func, self.indices).get_metrics()\n",
    "        \n",
    "        return [self.Metrics_Not_Available]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Metrics\n",
    "\n",
    "Here we have implemented several metrics to assess various aspects of our models. These metrics are crucial for comparing model performance, enabling us to select the most appropriate loss functions and hyperparameters. This evaluation approach is especially valuable during the Optuna study to identify the best-performing models.\n",
    "\n",
    "<details>\n",
    "<summary><u>References</u></summary>\n",
    "<ul>\n",
    "    <li>M. Steurer, R. J. Hill, and N. Pfeifer, “Metrics for evaluating the performance of machine learning based automated valuation models,” Journal of Property Research, vol. 38, Art. no. 2, Apr. 2021. doi: <a href=\"https://doi.org/10.1080/09599916.2020.1858937\">https://doi.org/10.1080/09599916.2020.1858937</a></li>\n",
    "    <li>S. K. Morley, T. V. Brito, and D. T. Welling, “Measures of model performance based on the log accuracy ratio,” Space Weather, vol. 16, Art. no. 1, 2018. doi: <a href=\"https://doi.org/10.1002/2017SW001669\">https://doi.org/10.1002/2017SW001669</a></li>\n",
    "    <li>M. W. Liemohn, A. D. Shane, A. R. Azari, A. K. Petersen, B. M. Swiger, and A. Mukhopadhyay, “RMSE is not enough: Guidelines to robust data-model comparisons for magnetospheric physics,” Journal of Atmospheric and Solar-Terrestrial Physics, vol. 218, p. 105624, Jul. 2021. doi: <a href=\"https://doi.org/10.1016/j.jastp.2021.105624\">https://doi.org/10.1016/j.jastp.2021.105624</a></li>\n",
    "    <li>Matthew, L. H. Hansen, H. Zhang, G. Angelotti, and J. Gallifant, “A Closer Look at AUROC and AUPRC under Class Imbalance,” arXiv.org, 2024. <a href=\"https://arxiv.org/abs/2401.06091v1\">https://arxiv.org/abs/2401.06091v1</a> (accessed Jul. 2024)</li>\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "> **Note:** While many of these functions are available in ML libraries, we have reimplemented them to better suit our specific use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Evaluation Metrics\n",
    "\n",
    "Metrics in this category focus on outliers and are particularly helpful in studies where outlier detection is crucial. This is especially important in our situation, as outliers are associated with solar storms, a significant phenomenon for our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have implemented a general class for Z-score calculation, which is useful for the detection of outliers.\n",
    "\n",
    "> **Note:** The choice of 3.5 as a threshold comes from empirical observations that in normally distributed data, approximately 99.7% of data points should fall within a Z-Score of 3. If a point has a Z-Score greater than 3.5, it is considered significantly deviant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class OutlierDetectionMetrics(Metrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def _modified_z_score(x):\n",
    "        \"\"\"\n",
    "        Calculate the Modified Z-Score for each variable in the tensor.\n",
    "        \n",
    "        Parameters:\n",
    "        tensor (torch.Tensor): Input tensor of shape (batch_size, variables, horizon)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Modified Z-Score tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        median = torch.median(x, dim=2, keepdim=True).values\n",
    "        \n",
    "        mad = torch.median(torch.abs(x - median), dim=2, keepdim=True).values\n",
    "        mad = torch.where(mad == 0, torch.tensor(1.0, device=x.device), mad)\n",
    "        \n",
    "        modified_z_scores = 0.6745 * (x - median) / mad\n",
    "        \n",
    "        return modified_z_scores\n",
    "\n",
    "    def _detect_outliers(self, values):\n",
    "        \"\"\"\n",
    "        Detect outliers based on Modified Z-Scores.\n",
    "        \n",
    "        Parameters:\n",
    "        z_scores (torch.Tensor): Modified Z-Scores tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Boolean tensor indicating outliers\n",
    "        \"\"\"\n",
    "        z_scores = self._modified_z_score(values)\n",
    "        return torch.abs(z_scores) > self.threshold\n",
    "    \n",
    "    def _evaluate_outlier_predicted(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of outlier detection.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        AttrDict: Dictionary with true/false positives, false negatives, indices of true/predicted outliers\n",
    "        \"\"\"    \n",
    "        # Detect outliers based on the threshold\n",
    "        true_outliers = self._detect_outliers(y_true)\n",
    "        pred_outliers = self._detect_outliers(y_pred)\n",
    "        \n",
    "        # Evaluate the detection by comparing true outliers and predicted outliers\n",
    "        tp = torch.sum((pred_outliers & true_outliers).float())  # True Positives\n",
    "        fp = torch.sum((pred_outliers & ~true_outliers).float()) # False Positives\n",
    "        fn = torch.sum((~pred_outliers & true_outliers).float()) # False Negatives\n",
    "        tn = torch.sum((~pred_outliers & ~true_outliers).float()) # True Negatives\n",
    "\n",
    "        return AttrDict({\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "            \"true_outliers\": true_outliers,\n",
    "            \"predicted_outliers\": pred_outliers\n",
    "        })\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score Metric\n",
    "\n",
    "$$\n",
    "\\text{F1\\_Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\qquad \\text{where: }\n",
    "\\begin{cases}\n",
    "    \\text{Precision} = \\frac{TP}{TP + FP} \\\\\n",
    "    \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The F1 Score is a classification metric that we will use to evaluate how well the model is forecasting outliers in the dataset. We have expanded this implementation to include additional metrics related to the F1 Score, using true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). These metrics include:\n",
    "$$\n",
    "\\text{} \\\\\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN} \\\\ \n",
    "\\text{} \\\\\n",
    "\\text{Negative Predictive Value (NPV)} = \\frac{TN}{TN + FN} \\\\\n",
    "\\text{} \\\\\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP} \\\\\n",
    "\\text{} \\\\\n",
    "\\Delta_\\text{Detected Outliers} = |y_{outliers} - \\hat{y}_{outliers}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class F1ScoreMetrics(OutlierDetectionMetrics):\n",
    "    def __init__(self, threshold=3.5, metrics='F1_Score'):\n",
    "        super().__init__(threshold)\n",
    "        self.metrics = metrics\n",
    "        self.epsilon = torch.finfo(torch.float32).eps # Used to avoid division per 0\n",
    "\n",
    "    \n",
    "\n",
    "    # Metrics\n",
    "    def Precision(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the precision metric, which measures the ratio of correctly predicted positive observations to the total predicted positives.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: Precision score<p>\n",
    "        \"\"\"\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "\n",
    "        # To avoid divide by 0\n",
    "        precision = (stats.tp + self.epsilon) / ((stats.tp + stats.fp) + self.epsilon)\n",
    "\n",
    "        return precision\n",
    "\n",
    "    \n",
    "    def Recall(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the recall metric, which measures the ratio of correctly predicted positive observations to all observations in the actual class.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: Recall score<p>\n",
    "        \"\"\"\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "\n",
    "        recall = (stats.tp + self.epsilon) / ((stats.tp + stats.fn) + self.epsilon)\n",
    "    \n",
    "\n",
    "        return recall\n",
    "\n",
    "    \n",
    "    def F1_Score(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the F1 score, which is the harmonic mean of precision and recall. It is used as a measure of a model’s accuracy on a dataset.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: F1 score<p>\n",
    "        \"\"\"\n",
    "        precision = self.Precision(y_true, y_pred)\n",
    "        recall = self.Recall(y_true, y_pred)\n",
    "\n",
    "        f1_score = ((2 * (precision * recall)) + self.epsilon) / ((precision + recall) + self.epsilon)\n",
    " \n",
    "\n",
    "        return f1_score\n",
    "\n",
    "    \n",
    "    def Accuracy_Score(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the accuracy score, which is the ratio of correctly predicted observations to the total observations.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: Accuracy score<p>\n",
    "        \"\"\"\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "            \n",
    "        ((stats.tp + stats.tn) + self.epsilon) / ((stats.tp + stats.fp + stats.fn + stats.tn) + self.epsilon)\n",
    "\n",
    "\n",
    "    \n",
    "    def Specificity(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the specificity metric, which measures the proportion of true negatives that are correctly identified.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: Specificity score<p>\n",
    "        \"\"\"\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "        \n",
    "        return (stats.tn+ self.epsilon) / ((stats.tn + stats.fp) + self.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "    def Negative_Predictive_Value(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the Negative Predictive Value (NPV), which measures the proportion of true negatives among all negative predictions.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: Negative Predictive Value score<p>\n",
    "        \"\"\"\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "\n",
    "        return (stats.tn + self.epsilon) / ((stats.tn + stats.fn) + self.epsilon)\n",
    "  \n",
    "\n",
    "    \n",
    "    def Detected_Outliers_Difference (self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the change in detected outliers (Δ Detected Outliers), representing the number of true outliers not predicted as outliers.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: Count of undetected outliers<p>\n",
    "        \"\"\"    \n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "        \n",
    "        return torch.sum(stats.true_outliers & ~stats.predicted_outliers)\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        if self.metrics == 'F1_Score':\n",
    "            return [self.F1_Score]\n",
    "        elif self.metrics == 'All':\n",
    "            return [self.Precision, self.Recall, self.F1_Score, self.Accuracy_Score, self.Specificity, self.Negative_Predictive_Value, self.Detected_Outliers_Difference ]\n",
    "        else:\n",
    "            return [self.Precision, self.Recall, self.F1_Score, self.Detected_Outliers_Difference ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under the Precision-Recall Curve (AUPRC)\n",
    "\n",
    "$$\n",
    "\\text{AUPRC} = \\int_{0}^{1} \\text{Precision}(\\text{Recall}) \\, d(\\text{Recall})\n",
    "$$\n",
    "\n",
    "The Area Under the Precision-Recall Curve (AUPRC) is a metric used to evaluate the effectiveness of a model in identifying rare, important events (outliers) in imbalanced datasets. In time series forecasting, especially when detecting outliers like solar storms, AUPRC is particularly useful. Since solar storms are rare compared to normal solar activity, traditional metrics like accuracy may not provide a clear picture of the model’s performance. However, AUPRC focuses on how well the model balances identifying true solar storms (high recall) while minimizing false positives (high precision).\n",
    "\n",
    "A higher AUPRC value suggests that the model is effective in detecting solar storms without generating too many false alarms, making it a crucial metric for evaluating the model’s ability to correctly identify these rare but significant events in your forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class AUPRCMetric(OutlierDetectionMetrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__(threshold)\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def AURPC(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the Area Under the Precision-Recall Curve (AUPRC), a \n",
    "        metric used to evaluate the effectiveness of a model in identifying rare, important events (outliers)</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: AUPRC score<p>\n",
    "        \"\"\"\n",
    "        pred_z_scores = self._modified_z_score(y_pred)\n",
    "        \n",
    "        pred_z_scores_flat = pred_z_scores.view(-1).cpu().numpy()\n",
    "        true_outliers_flat = self._detect_outliers(y_true).view(-1).cpu().numpy()\n",
    "        \n",
    "        # Use precision_recall_curve to get precision and recall for different thresholds\n",
    "        precision, recall, _ = precision_recall_curve(true_outliers_flat, pred_z_scores_flat)\n",
    "        \n",
    "        auprc_value = auc(recall, precision)\n",
    "        \n",
    "        return torch.tensor(auprc_value, device=y_true.device)\n",
    "    \n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.AURPC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurtois and Skewness Difference (KSD)\n",
    "\n",
    "$$\n",
    "\\text{Skewness}(S) = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^3}{\\left(\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right)^{3/2}} \\qquad\n",
    "\\text{Kurtois}(K) = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^4}{\\left(\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "Differences in skewness (S), which measures the \"tailedness\" of a distribution, and kurtosis (K), which quantifies the asymmetry of a distribution, are valuable metrics for evaluating the ability of the model to detect outliers because they assess the tails of distributions. Traditional metrics often focus on central tendencies, such as the mean or median, and might not adequately capture the presence or absence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class KSDifferenceMetric(Metrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def skewness(x):\n",
    "        # As batches are randomly generated and each variable is independent\n",
    "        mean = torch.mean(x, dim=2, keepdim=True)\n",
    "        std_dev = torch.std(x, dim=2, unbiased=True, keepdim=True)\n",
    "        \n",
    "        skewness = torch.mean(((x - mean) / std_dev) ** 3, dim=2)\n",
    "        return skewness\n",
    "\n",
    "    @staticmethod\n",
    "    def kurtosis(x):\n",
    "        mean = torch.mean(x, dim=2, keepdim=True)\n",
    "        std_dev = torch.std(x, dim=2, unbiased=True, keepdim=True)\n",
    "        \n",
    "        kurtosis = torch.mean(((x - mean) / std_dev) ** 4, dim=2)\n",
    "        return kurtosis\n",
    "    \n",
    "\n",
    "    # Metrics\n",
    "    def Skewness_Difference(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the absolute difference in skewness between the actual and predicted values, which measures the asymmetry of the data distribution.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: Absolute difference in skewness between y_true and y_pred<p>\n",
    "        \"\"\"\n",
    "        true_skewness = KSDifferenceMetric.skewness(y_true)\n",
    "        pred_skewness = KSDifferenceMetric.skewness(y_pred)\n",
    "        \n",
    "        return torch.mean(torch.abs(true_skewness - pred_skewness), dim=[0, 1])\n",
    "\n",
    "    \n",
    "    def Kurtosis_Difference(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the absolute difference in kurtosis between the actual and predicted values, which measures the tailedness of the data distribution.</p>\n",
    "\n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "\n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: Absolute difference in kurtosis between y_true and y_pred<p>\n",
    "        \"\"\"\n",
    "        true_kurtosis = KSDifferenceMetric.kurtosis(y_true)\n",
    "        pred_kurtosis = KSDifferenceMetric.kurtosis(y_pred)\n",
    "        \n",
    "        return torch.mean(torch.abs(true_kurtosis - pred_kurtosis), dim=[0, 1])\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.Skewness_Difference, self.Kurtosis_Difference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Metrics\n",
    "Metrics in this category quantify how well a model captures the trends in observed data. We will use two from this category:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson Linear Correlation Coefficient ($R$)\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}}\n",
    "$$\n",
    "\n",
    "The most commonly used metric for correlation is the **Pearson Linear Correlation Coefficient ($R$)**. It is important to note that a good $R$-value should be both statistically significant and above an appropriate threshold for the study.\n",
    "\n",
    "#### Coefficient of Determination Metric ($R^2$)\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\qquad \\text{where: }\n",
    "\\begin{cases}\n",
    "    \\text{Residual Sum of Squares (RSS)} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\\\\n",
    "    \\text{Total Sum of Squares (TSS)} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The **Coefficient of Determination Metric ($R^2$)** quantifies the proportion of variance in the observed solar indices that is explained by the model's predictions. This measure is particularly useful because it provides a direct indication of how well the model captures the underlying patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class AssociationMetrics(Metrics):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = torch.finfo(torch.float32).eps # Used to avoid division per 0\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def R_Correlation(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the Pearson Correlation Coefficient (R Correlation) between true and predicted values.</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: R Correlation coefficient<p>\n",
    "        \"\"\"\n",
    "        y_true_flat = y_true.reshape(-1)\n",
    "        y_pred_flat = y_pred.reshape(-1)\n",
    "        \n",
    "        # To be able to use torch.corrcoef, we need to stack the tensors\n",
    "        stacked = torch.stack([y_true_flat, y_pred_flat])\n",
    "        \n",
    "        corr_matrix = torch.corrcoef(stacked)\n",
    "        \n",
    "        r_value = corr_matrix[0, 1]\n",
    "        return r_value\n",
    "\n",
    "\n",
    "    def R2_Score(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the R^2 score, which measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: R^2 score<p>\n",
    "        \"\"\"\n",
    "        y_true_mean = torch.mean(y_true, dim=2, keepdim=True)\n",
    "        \n",
    "        # Total Sum of Squares\n",
    "        ss_tot = torch.sum((y_true - y_true_mean) ** 2)\n",
    "        \n",
    "        # Residual Sum of Squares\n",
    "        ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "\n",
    "        \n",
    "        r2 = 1 - ((ss_res + self.epsilon) / (ss_tot + self.epsilon))\n",
    "        \n",
    "        return r2\n",
    "\n",
    "    \n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.R_Correlation, self.R2_Score]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Values scalation when they go under the log asymptote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def inverse_scale_values_below_threshold(tensor, threshold, lower_bound, upper_bound):\n",
    "    mask = tensor < threshold\n",
    "\n",
    "    if mask.sum() == 0:\n",
    "        # If no values are below the threshold, return the original tensor\n",
    "        return tensor\n",
    "    \n",
    "    values_to_scale = tensor[mask]\n",
    "    min_orig = values_to_scale.min()\n",
    "    max_orig = values_to_scale.max()\n",
    "    \n",
    "    if min_orig == max_orig:\n",
    "        scaled_values = torch.full_like(tensor, upper_bound)\n",
    "    else:\n",
    "        scaled_values = upper_bound - (tensor - min_orig) * (upper_bound - lower_bound\n",
    "    ) / (max_orig - min_orig)\n",
    "    \n",
    "    result_tensor = torch.where(mask, scaled_values, tensor)\n",
    "    \n",
    "    return result_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metrics\n",
    "\n",
    "The metrics in this category are used to quantify the closeness of model predictions to actual observations, providing a measure of how well a model reproduces real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric Mean Absolute Percentage Error Metric (sMAPE)\n",
    "\n",
    "$$\n",
    "\\text{sMAPE} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|y_i - \\hat{y}_i|}{\\frac{|y_i| + |\\hat{y}_i|}{2}} \\times 100\n",
    "$$\n",
    "\n",
    "The Symmetric Mean Absolute Percentage Error (sMAPE) offers an alternative to traditional metrics like MAE (Mean Absolute Error), which are sensitive to larger values. By scaling the absolute error to the average magnitude of the data and model pair, sMAPE mitigates the undue influence of outliers. However, as an average-based metric, it remains susceptible to extreme values within the dataset. \n",
    "\n",
    "Since sMAPE is not directly aligned with either of the losses used, it may not fully capture the strengths or weaknesses of models trained with these loss functions. For example, a model optimized for MAE might perform poorly on sMAPE if the predictions are close in absolute terms but relatively inaccurate for small actual values.\n",
    "\n",
    "#### Median Symmetric Accuracy Metric (MSA)\n",
    "\n",
    "$$\n",
    "\\text{MSA} = 100 \\times (e^{\\tilde{x}}-1) \\qquad \\text{where: } x = \\left|\\ln\\left(\\frac{\\hat{y_i}}{y_i}\\right)\\right|\n",
    "$$\n",
    "\n",
    "The Median Symmetric Accuracy (MSA) provides a more robust accuracy assessment, particularly for datasets prone to outliers. It leverages the median of the log-transformed ratios between model and observed values, minimizing the impact of extreme deviations. This characteristic makes MSA particularly suitable for characterizing model performance in capturing the overall distribution and minimizing the effect of outliers inherent in observational data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class AccuracyMetrics(Metrics):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = torch.finfo(torch.float32).eps # Used to avoid division per 0\n",
    "\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def sMAPE(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Symmetric Mean Absolute Percentage Error (sMAPE).\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: sMAPE value\n",
    "        \"\"\"\n",
    "        abs_error = torch.abs(y_true - y_pred)\n",
    "        symetric_error = ((torch.abs(y_true) + torch.abs(y_pred)) / 2.0) \n",
    "        \n",
    "        smape = torch.mean((abs_error + self.epsilon)/ (symetric_error + self.epsilon)) * 100\n",
    "        return smape\n",
    "    \n",
    "    def MSA(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Median Symmetric Accuracy (MSA).\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: MSA value\n",
    "        \"\"\"\n",
    "        q = (y_pred + self.epsilon) / (y_true + self.epsilon)\n",
    "\n",
    "        log_ratio = torch.abs(torch.log(inverse_scale_values_below_threshold(q, 0, 0.9, self.epsilon)))\n",
    "\n",
    "        msa = (torch.exp(torch.median(log_ratio)) - 1) * 100\n",
    "        \n",
    "        return msa\n",
    "    \n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.sMAPE, self.MSA]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Metrics\n",
    "\n",
    "This category examines the systematic overestimation or underestimation of observations by a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric Signed Percentage Bias Metric (SSPB)\n",
    "\n",
    "$$\n",
    "\\text{SSPB} = \\text{sign}(\\tilde{x}) \\times \\left|e^{\\tilde{x}} - 1\\right| \\times 100 \\qquad \\text{where: }\n",
    "\\begin{cases}\n",
    "    x = \\ln\\left(\\frac{\\hat{y_i}}{y_i}\\right) \\\\\n",
    "    \\\\\n",
    "    \\text{sign}(x) = \n",
    "    \\begin{cases} \n",
    "        -1 & \\text{if } x < 0, \\\\\n",
    "        0 & \\text{if } x = 0, \\\\\n",
    "        1 & \\text{if } x > 0.\n",
    "    \\end{cases}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The Symmetric Signed Percentage Bias (SSPB) is particularly well-suited for assessing bias in time series data characterized by high variability spanning several orders of magnitude. Unlike conventional bias metrics like Mean Error (ME), which are sensitive to extreme values, SSPB leverages the logarithm of the model-to-observation ratio. This logarithmic transformation effectively mitigates the disproportionate influence of outliers, providing a more balanced assessment of systematic overestimation or underestimation by the model.\n",
    "\n",
    "This characteristic is especially valuable when evaluating models dealing with data such as radiation belt electron fluxes, which exhibit significant fluctuations. By using the median of these logarithmic ratios, SSPB further enhances its robustness, offering a stable measure of central tendency even in the presence of non-normal error distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BiasMetrics(Metrics):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = torch.finfo(torch.float32).eps # Used to avoid division per 0\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def SSPB(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        <p>Calculate the Symmetric Signed Percentage Bias (SSPB), which measures the percentage bias with consideration for the direction of the bias.</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)</li>\n",
    "            <li>y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>torch.Tensor: SSPB value<p>\n",
    "        \"\"\"\n",
    "        q = (y_pred + self.epsilon) / (y_true + self.epsilon)\n",
    "\n",
    "        log_ratio = torch.abs(torch.log(inverse_scale_values_below_threshold(q, 0, 0.9, self.epsilon)))\n",
    "        median_log_ratio = torch.median(log_ratio)\n",
    "\n",
    "        sign = torch.sign(median_log_ratio)\n",
    "        \n",
    "        return sign * (torch.exp(torch.abs(median_log_ratio)) - 1) * 100\n",
    "\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.SSPB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Metrics Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ValidationMetricsHandler:\n",
    "    \"\"\"\n",
    "    <p>A class to manage validation metrics for model evaluation. It allows listing available metrics, uploading requested metrics, and retrieving study directions and objective values.</p>\n",
    "    \n",
    "    <h3>Attributes:</h3>\n",
    "    <ul>\n",
    "        <li>available_metrics (list)[<i>Static</i>]: A list of available metrics provided by different metric classes.</li>\n",
    "        <li>study_directions (dict)[<i>Static</i>]: A dictionary mapping metrics to their respective optimization directions (maximize or minimize).</li>\n",
    "        <li>requested_metrics (dict): A dictionary storing metrics that have been requested for evaluation.</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    available_metrics = [\n",
    "        *F1ScoreMetrics(metrics='All').get_metrics(),\n",
    "        *AUPRCMetric().get_metrics(),\n",
    "        *KSDifferenceMetric().get_metrics(),\n",
    "        *AssociationMetrics().get_metrics(),\n",
    "        *AccuracyMetrics().get_metrics(),\n",
    "        *BiasMetrics().get_metrics()\n",
    "    ]\n",
    "\n",
    "    study_directions = {\n",
    "        'precision': StudyDirection.MAXIMIZE,                    # Higher precision is better (Range: [0, 1])\n",
    "        'recall': StudyDirection.MAXIMIZE,                       # Higher recall is better (Range: [0, 1])\n",
    "        'f1_score': StudyDirection.MAXIMIZE,                     # Higher F1 score is better (Range: [0, 1])\n",
    "        'accuracy_score': StudyDirection.MAXIMIZE,               # Higher accuracy is better (Range: [0, 1])\n",
    "        'specificity': StudyDirection.MAXIMIZE,                  # Higher specificity is better (Range: [0, 1])\n",
    "        'negative_predictive_value': StudyDirection.MAXIMIZE,    # Higher NPV is better (Range: [0, 1])\n",
    "        'detected_outliers_difference': StudyDirection.MINIMIZE, # Minimize the difference in detected outliers (Range: [0, ∞))\n",
    "        'aurpc': StudyDirection.MAXIMIZE,                        # Higher AUPRC is better (Range: [0, 1])\n",
    "        'skewness_difference': StudyDirection.MINIMIZE,          # Minimize skewness difference to target (Range: [−∞, ∞])\n",
    "        'kurtosis_difference': StudyDirection.MINIMIZE,          # Minimize kurtosis difference to target (Range: [−∞, ∞])\n",
    "        'r_correlation': StudyDirection.MAXIMIZE,                # Higher Pearson correlation is better (Range: [−1, 1])\n",
    "        'r2_score': StudyDirection.MAXIMIZE,                     # Higher R² is better (Range: [−∞, 1])\n",
    "        'smape': StudyDirection.MINIMIZE,                        # Lower SMAPE is better (Range: [0, ∞))\n",
    "        'msa': StudyDirection.MAXIMIZE,                          # Higher MSA is better (Range: [0, 1])\n",
    "        'sspb': StudyDirection.MINIMIZE                          # Minimize absolute SSPB (optimize for bias close to zero) (Range: [−100%, 100%])\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, metrics:list=None, main_metric:str=''):\n",
    "        self.requested_metrics = {}\n",
    "        if metrics is not None:\n",
    "            self.add(metrics)\n",
    "\n",
    "        self.main_metric = main_metric.lower()\n",
    "\n",
    "\n",
    "    # Visualization functions\n",
    "    def list(self):\n",
    "        \"\"\"\n",
    "        <p>Display a list of available metrics along with their descriptions in a table format.</p>\n",
    "        \"\"\"\n",
    "        table_rows = []\n",
    "\n",
    "        if not bool(self.requested_metrics):\n",
    "            for metric in ValidationMetricsHandler.available_metrics:\n",
    "                doc_html = metric.__doc__.strip().replace(\"\\n\", \" \")\n",
    "                table_rows.append(f\"<tr><td style='text-align: left;'><strong>{metric.__name__}</strong></td><td style='text-align: left;'>{doc_html}</td></tr>\")\n",
    "        else:\n",
    "            for metric in self.requested_metrics.values():\n",
    "                doc_html = metric.__doc__.strip().replace(\"\\n\", \" \")\n",
    "                metric_name = metric.__name__.lower()\n",
    "                if metric_name in self.requested_metrics.values():\n",
    "                    table_rows.append(f\"<tr><td style='text-align: left;'><strong>{metric.__name__}</strong></td><td style='text-align: left;'>{doc_html}</td></tr>\")\n",
    "                else:\n",
    "                    table_rows.append(f\"<tr><td style='text-align: left;'>{metric.__name__}</td><td style='text-align: left;'>{doc_html}</td></tr>\")\n",
    "        \n",
    "        \n",
    "        table_html = f\"\"\"\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th style='text-align: left;'>Metric Name</th>\n",
    "                    <th style='text-align: left;'>Description</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                {''.join(table_rows)}\n",
    "            </tbody>\n",
    "        </table>\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(table_html))\n",
    "\n",
    "    def _show_metrics(self, metrics:List[AvgMetric]) -> None:\n",
    "        metric_column_width = '250px'  # Fixed width to accommodate 'detected_outliers_difference'\n",
    "        value_column_width = '100px'   # Fixed width for values up to 12 characters\n",
    "        table_rows = []\n",
    "        for metric in metrics:\n",
    "            value = f\"{metric.value:.4f}\"\n",
    "            table_rows.append(f\"<tr><td style='padding: 4px; text-align: left; width: {metric_column_width}; white-space: nowrap; overflow: hidden; text-overflow: ellipsis;'><strong>{metric.name}</strong></td><td style='padding: 4px; text-align: right; width: {value_column_width};'>{value}</td></tr>\")\n",
    "        \n",
    "        table_html = f\"\"\"\n",
    "        <table style='border-collapse: collapse; table-layout: fixed; width: {int(metric_column_width[:-2]) + int(value_column_width[:-2]) + 20}px;'>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th style='padding: 4px; text-align: left; border: 1px solid black; width: {metric_column_width};'>Metric Name</th>\n",
    "                    <th style='padding: 4px; text-align: right; border: 1px solid black; width: {value_column_width};'>Value</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "                {''.join(table_rows)}\n",
    "            </tbody>\n",
    "        </table>\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(table_html))\n",
    "\n",
    "\n",
    "\n",
    "    # Metrics management functions\n",
    "    def add(self, metrics:list):\n",
    "        \"\"\"\n",
    "        <p>Upload a list of metrics to the factory for evaluation. The metrics are converted to lowercase for consistency.</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>metrics (list): A list of metric names to be uploaded for evaluation.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Raises:</h3>\n",
    "        <p>ValueError: If any metric in the provided list is not found in the available metrics.</p>\n",
    "        \"\"\"\n",
    "        metrics = [metric.lower() for metric in metrics]\n",
    "\n",
    "        for metric in ValidationMetricsHandler.available_metrics:\n",
    "            metric_name = metric.__name__.lower()\n",
    "            if metric_name in metrics:\n",
    "                self.requested_metrics[metric_name] = metric\n",
    "                metrics.remove(metric.__name__.lower())\n",
    "        \n",
    "        if len(metrics) > 0:\n",
    "            raise ValueError(f\"Metrics not found: {metrics}. Please use ValidationMetricsFactory.list() to see available metrics.\")\n",
    "        \n",
    "    def remove(self, metrics:list):\n",
    "        \"\"\"\n",
    "        <p>Remove a list of metrics from the factory that were previously uploaded for evaluation. The metrics are converted to lowercase for consistency.</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>metrics (list): A list of metric names to be removed from evaluation.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Raises:</h3>\n",
    "        <p>ValueError: If any metric in the provided list is not found in the requested metrics.</p>\n",
    "        \"\"\"\n",
    "        metrics = [metric.lower() for metric in metrics]\n",
    "\n",
    "        for metric in metrics:\n",
    "            if metric in self.requested_metrics:\n",
    "                self.requested_metrics.pop(metric)\n",
    "            else:\n",
    "                raise ValueError(f\"Metric not found: {metric}. Please use ValidationMetricsFactory.get_metrics() to see requested metrics.\")\n",
    "        \n",
    "\n",
    "\n",
    "    # Metrics utility functions\n",
    "    def get_metrics(self) -> list:\n",
    "        \"\"\"\n",
    "        <p>Retrieve the list of requested metrics for evaluation.</p>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>list: A list of requested metric objects.</p>\n",
    "        \"\"\"\n",
    "        return list(self.requested_metrics.values())\n",
    "\n",
    "    def get_study_directions(self) -> list:    \n",
    "        \"\"\"\n",
    "        <p>Retrieve the study directions (maximize or minimize) for the requested metrics.</p>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>list: A list of study directions corresponding to the requested metrics.</p>\n",
    "        \"\"\"\n",
    "        return [self.study_directions[metric] for metric in self.requested_metrics.keys()]\n",
    "    \n",
    "    def get_objective_values(self, metrics_results:List[AvgMetric], show_metrics=False) -> list:\n",
    "        \"\"\"\n",
    "        <p>Extract the objective values from the results of the requested metrics.</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>metrics_results (List[AvgMetric]): A list of metric result objects from which to extract values.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>list: A list of metric values extracted from the provided results.</p>\n",
    "        \"\"\"\n",
    "        if show_metrics:\n",
    "            self._show_metrics(metrics_results)\n",
    "\n",
    "        object_values = []\n",
    "        for metric, requested_metric in zip(metrics_results, self.requested_metrics.keys()):\n",
    "            metric_name = metric.name.lower()\n",
    "            if metric_name == requested_metric:\n",
    "                # As SSPB could be positive or negative, but the better is to be closer to 0\n",
    "                if metric_name == 'sspb':\n",
    "                    object_values.append(np.abs(metric.value)) \n",
    "                else:\n",
    "                    object_values.append(metric.value)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected metric found: {metric_name}. Expected: {requested_metric}\")\n",
    "            \n",
    "            \n",
    "        return (metric_result.value for metric_result in metrics_results)\n",
    "    \n",
    "    def save(self, path:str='tmp/') -> str:\n",
    "        \"\"\"\n",
    "        <p>Save the requested metrics to a file to share with the training notebook.</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li>path (str): The path to save the requested metrics. | <i>Default</i>: tmp</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "        path = f\"{path}metrics.pkl\"\n",
    "        save_object(self, path)\n",
    "\n",
    "        return path\n",
    "    \n",
    "\n",
    "    # Metrics evaluation functions\n",
    "    @staticmethod\n",
    "    def _has_improved(trial_value, best_value, direction):\n",
    "        \"\"\"\n",
    "        <p>Determine if the trial value has improved over the best value based on the optimization direction.</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li><b>trial_value</b> (float or int): The value from the current trial to evaluate.</li>\n",
    "            <li><b>best_value</b> (float or int): The current best value for comparison.</li>\n",
    "            <li><b>direction</b> (StudyDirection): The direction of optimization, either maximizing or minimizing.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>bool: True if the trial value represents an improvement over the best value in the given direction, False otherwise.</p>\n",
    "        \"\"\"\n",
    "        if direction == StudyDirection.MAXIMIZE:\n",
    "            return trial_value > best_value\n",
    "        elif direction == StudyDirection.MINIMIZE:\n",
    "            return trial_value < best_value\n",
    "        return False\n",
    "\n",
    "    def are_best_values(self, best_values, trial_values) -> bool:\n",
    "        \"\"\"\n",
    "        <p>Determine if the trial values are better than the current best values for a given metric.</p>\n",
    "        \n",
    "        <h3>Parameters:</h3>\n",
    "        <ul>\n",
    "            <li><b>main_metric</b> (str): The name of the main metric used for comparison.</li>\n",
    "            <li><b>best_values</b> (list): A list of current best values for various metrics.</li>\n",
    "            <li><b>trial_values</b> (list): A list of new trial values to compare against the best values.</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Returns:</h3>\n",
    "        <p>bool: True if the trial values are overall better than the best values, False otherwise.</p>\n",
    "        \"\"\"\n",
    "        cls = ValidationMetricsHandler\n",
    "\n",
    "        if best_values is None:\n",
    "            return True\n",
    "\n",
    "        if len(best_values) == 1:\n",
    "            return cls._has_improved(\n",
    "                    trial_value=trial_values[0].value, \n",
    "                    best_value=best_values[0].value, \n",
    "                    direction=cls.study_directions.get(self.main_metric)\n",
    "                )\n",
    "\n",
    "        improvement_count = 0\n",
    "        comparison_threshold = len(best_values) // 2\n",
    "\n",
    "        for best_metric, trial_metric in zip(best_values, trial_values):\n",
    "            metric_name = best_metric.name.lower()\n",
    "            direction = cls.study_directions.get(metric_name)\n",
    "\n",
    "            if cls._has_improved(trial_metric.value, best_metric.value, direction):\n",
    "                if self.main_metric == metric_name:\n",
    "                    improvement_count += comparison_threshold\n",
    "                else:\n",
    "                    improvement_count += 1\n",
    "\n",
    "        return improvement_count > comparison_threshold\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "device = 'cpu'\n",
    "ranges = {'A': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'B': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'C': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'D': np.array([[0, 1], [1, 2], [2, 3], [3, 4]])}\n",
    "\n",
    "weights = {'A': np.array([1, 2, 3, 4])}\n",
    "\n",
    "target = torch.tensor([[[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]]], device=device, dtype=torch.float32)\n",
    "\n",
    "input = target + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "def test_LossMetrics():\n",
    "    loss = wMAELoss(ranges, weights).to(device)\n",
    "    metrics = LossMetrics(loss, 'SolFSMY').get_metrics()\n",
    "\n",
    "    loss_value = loss(input, target)\n",
    "    metrics_values = [metric(input, target) for metric in metrics]\n",
    "\n",
    "    assert torch.isclose(loss_value, sum(metrics_values)), f\"Expected {loss_value}, but got {sum(metrics_values)} ({metrics_values})\"\n",
    "    print(\"LossMetrics test passed!\")\n",
    "\n",
    "def test_LossMetrics_for_classification():\n",
    "    loss = ClassificationLoss(ranges, MSELoss()).to(device)\n",
    "    metrics = SOLFMYClassificationMetrics(loss)\n",
    "\n",
    "    # Compute the total misclassifications manually for all specific positions\n",
    "    total_counts = 0\n",
    "    total_counts += metrics.Missclassifications_Low(input, target)\n",
    "    total_counts += metrics.Missclassifications_Moderate(input, target)\n",
    "    total_counts += metrics.Missclassifications_Elevated(input, target)\n",
    "    total_counts += metrics.Missclassifications_High(input, target)\n",
    "\n",
    "    # Use the generate_metrics method to retrieve and calculate all defined metrics\n",
    "    metrics_functions = LossMetrics(loss, 'SolFSMY').get_metrics()\n",
    "    metrics_values = [metric(input, target) for metric in metrics_functions]\n",
    "\n",
    "    # Assert that the total manually calculated matches the sum of individual metrics\n",
    "    assert np.isclose(total_counts, sum(metrics_values)), f\"Expected {total_counts}, but got {sum(metrics_values)} ({metrics_values})\"\n",
    "    print(\"LossMetrics for classification loss test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor([\n",
    "    [[10, 12, 12, 13, 12, 12, 12, 14, 12, 100], [20, 22, 23, 20, 22, 20, 21, 22, 23, 200]],\n",
    "    [[-11, -12, -13, -13, -14, -14, -12, -13, -14, -105], [-22, -23, -25, -23, -22, -24, -23, -22, -25, -210]]\n",
    "], dtype=torch.float)\n",
    "\n",
    "y_pred = torch.tensor([\n",
    "    [[11, 12, 12, 13, 12, 12, 13, 14, 13, 90], [21, 22, 23, 21, 22, 21, 22, 22, 23, 195]],\n",
    "    [[-12, -13, -14, -14, -15, -14, -13, -14, -14, -10], [-23, -24, -26, -24, -23, -25, -24, -23, -26, -205]]\n",
    "], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test_OutlierDetectionMetrics():\n",
    "    metrics = F1ScoreMetrics(metrics=' ').get_metrics()\n",
    "    metrics_precision = metrics[0](y_true, y_pred) \n",
    "    metrics_recall = metrics[1](y_true, y_pred)\n",
    "    metrics_f1_score = metrics[2](y_true, y_pred)\n",
    "    metrics_outliers_difference = metrics[3](y_true, y_pred)\n",
    "\n",
    "    f1 = 2 * (metrics_precision * metrics_recall) / (metrics_precision + metrics_recall)\n",
    "\n",
    "    assert metrics_precision == 1.0, f\"Expected 1.0, but got {metrics_precision}\"\n",
    "    assert metrics_recall == 0.75, f\"Expected 0.75, but got {metrics_recall}\"\n",
    "    assert metrics_f1_score == f1, f\"Expected {f1}, but got {metrics_f1_score}\"\n",
    "    assert metrics_outliers_difference == 1, f\"Expected 2, but got {metrics_outliers_difference}\"\n",
    "\n",
    "    print(\"OutlierDetectionMetrics test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def evaluate_get_metrics (metrics:list):\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.__name__}: {metric(y_true, y_pred)}\")\n",
    "\n",
    "def test_F1ScoreMetrics():\n",
    "    metrics = F1ScoreMetrics(metrics='All').get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"F1ScoreMetrics test passed!\")\n",
    "\n",
    "def test_AUPRCMetric():\n",
    "    metrics = AUPRCMetric().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"AUPRCMetric test passed!\")\n",
    "\n",
    "def test_KSDifferenceMetric():\n",
    "    metrics = KSDifferenceMetric().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"KSDifferenceMetric test passed!\")\n",
    "\n",
    "def test_AssociationMetrics():\n",
    "    metrics = AssociationMetrics().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"AssociationMetrics test passed!\")\n",
    "\n",
    "def test_AccuracyMetrics():\n",
    "    metrics = AccuracyMetrics().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"AccuracyMetrics test passed!\")\n",
    "\n",
    "def test_BiasMetrics():\n",
    "    metrics = BiasMetrics().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"BiasMetrics test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_upload_valid_metrics():\n",
    "    valid_metrics = ['precision', 'recall']\n",
    "    factory = ValidationMetricsHandler(valid_metrics)\n",
    "    try:\n",
    "        factory.add(valid_metrics)\n",
    "        assert len(factory.requested_metrics) == len(valid_metrics)\n",
    "        print(\"test_upload_valid_metrics passed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"test_upload_valid_metrics failed: {e}\")\n",
    "\n",
    "def test_upload_invalid_metric():\n",
    "    try:\n",
    "        invalid_metric = ['non_existing_metric']\n",
    "        factory = ValidationMetricsHandler(invalid_metric)    \n",
    "    except ValueError:\n",
    "        print(\"test_upload_invalid_metric passed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"test_upload_invalid_metric failed: {e}\")\n",
    "\n",
    "\n",
    "def test_get_study_directions():\n",
    "    valid_metrics = ['precision', 'recall']\n",
    "    factory = ValidationMetricsHandler(valid_metrics)\n",
    "    try:\n",
    "        factory.upload(valid_metrics)\n",
    "        directions = factory.get_study_directions()\n",
    "        expected_directions = [ValidationMetricsHandler.study_directions[m] for m in valid_metrics]\n",
    "        assert directions == expected_directions\n",
    "        print(\"test_get_study_directions passed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"test_get_study_directions failed: {e}\")\n",
    "\n",
    "def test_get_objective_values():\n",
    "    from collections import namedtuple\n",
    "    AvgMetric = namedtuple('AvgMetric', ['value'])\n",
    "    \n",
    "    valid_metrics = ['precision', 'recall']\n",
    "    factory = ValidationMetricsHandler(valid_metrics)\n",
    "    try:\n",
    "        factory.upload(valid_metrics)\n",
    "        mock_results = [AvgMetric(value=i) for i in range(len(valid_metrics))]\n",
    "        values = factory.get_objective_values(mock_results)\n",
    "        assert values == list(range(len(valid_metrics)))\n",
    "        print(\"test_get_objective_values passed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"test_get_objective_values failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LossMetrics test passed!\n",
      "LossMetrics for classification loss test passed!\n",
      "OutlierDetectionMetrics test passed!\n",
      "Precision: 1.0\n",
      "Recall: 0.75\n",
      "F1_Score: 0.8571428656578064\n",
      "Accuracy_Score: None\n",
      "Specificity: 1.0\n",
      "Negative_Predictive_Value: 0.9729729890823364\n",
      "Detected_Outliers_Difference: 1\n",
      "F1ScoreMetrics test passed!\n",
      "AURPC: 0.7721153846153846\n",
      "AUPRCMetric test passed!\n",
      "Skewness_Difference: 0.8450393676757812\n",
      "Kurtosis_Difference: 0.8302615284919739\n",
      "KSDifferenceMetric test passed!\n",
      "R_Correlation: 0.9610453844070435\n",
      "R2_Score: 0.8769017457962036\n",
      "AssociationMetrics test passed!\n",
      "sMAPE: 7.933315753936768\n",
      "MSA: 4.347825050354004\n",
      "AccuracyMetrics test passed!\n",
      "SSPB: 4.347825050354004\n",
      "BiasMetrics test passed!\n",
      "test_upload_valid_metrics passed!\n",
      "test_upload_invalid_metric passed!\n",
      "test_get_study_directions failed: 'ValidationMetricsHandler' object has no attribute 'upload'\n",
      "test_get_objective_values failed: 'ValidationMetricsHandler' object has no attribute 'upload'\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_LossMetrics()\n",
    "test_LossMetrics_for_classification()\n",
    "test_OutlierDetectionMetrics()\n",
    "test_F1ScoreMetrics()\n",
    "test_AUPRCMetric()\n",
    "test_KSDifferenceMetric()\n",
    "test_AssociationMetrics()\n",
    "test_AccuracyMetrics()\n",
    "test_BiasMetrics()\n",
    "\n",
    "# Factory tests\n",
    "test_upload_valid_metrics()\n",
    "test_upload_invalid_metric()\n",
    "test_get_study_directions()\n",
    "test_get_objective_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from nbdev import *\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
