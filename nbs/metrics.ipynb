{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp metrics\n",
    "\n",
    "#|export\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsai.basics import *\n",
    "from swdf.losses import wMAELoss, MSELoss, WeightedLoss, ClassificationLoss\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "---\n",
    "## Index\n",
    "#### [1 Loss Metrics](#loss-metrics)\n",
    "  - [1.1 Regression Metrics](#regression-metrics)\n",
    "    - [1.1.1 Solar Indices FSMY 10.7 Metrics](#solar-indices-fsmy-107-metrics)\n",
    "    - [1.1.2 Geomagnetic Indices DST and AP Metrics](#geomagnetic-indices-dst-and-ap-metrics)\n",
    "  - [1.2 Classification Metrics](#classification-metrics)\n",
    "    - [1.2.1 Solar Indices FSMY 10.7 Metrics](#solar-indices-fsmy-107-metrics)\n",
    "    - [1.2.2 Geomagnetic Indices DST and AP Metrics](#geomagnetic-indices-dst-and-ap-metrics)\n",
    "  - [1.3 Loss Metrics Retrieval](#loss-metrics-retrieval)\n",
    "#### [2 Validation Metrics](#validation-metrics)\n",
    "  - [2.1 Outliers Evaluation Metrics](#outliers-evaluation-metrics)\n",
    "    - [2.1.1 F1 Score Metric](#f1-score-metric)\n",
    "    - [2.1.2 Area Under the Precision-Recall Curve (AUPRC)](#area-under-the-precision-recall-curve-auprc)\n",
    "    - [2.1.3 Kurtois and Skewness Difference (KSD)](#kurtois-and-skewness-difference-ksd)\n",
    "  - [2.2 Association Metrics](#association-metrics)\n",
    "    - [2.2.1 Pearson Linear Correlation Coefficient ($R$)](#pearson-linear-correlation-coefficient-r)\n",
    "    - [2.2.2 Coefficient of Determination Metric ($R^2$)](#coefficient-of-determination-metric-r2)\n",
    "  - [2.3 Accuracy Metrics](#accuracy-metrics)\n",
    "    - [2.3.1 Symmetric Mean Absolute Percentage Error Metric (sMAPE)](#symmetric-mean-absolute-percentage-error-metric-smape)\n",
    "    - [2.3.1 Median Symmetric Accuracy Metric (MSA)](#median-symmetric-accuracy-metric-msa)\n",
    "  - [2.4 Bias Metrics](#bias-metrics)\n",
    "    - [2.4.1 Symmetric Signed Percentage Bias Metric (SSPB)](#symmetric-signed-percentage-bias-metric-sspb)\n",
    "#### [3 Tests](#tests)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have implemented metrics to be used within the `TSForecaster()` to provide relevant insights into the training process. Additionally, we have compiled a selection of metrics that can be used to compare the performance of each model in the forecasting task. All the classes follow the pattern of the `Metrics()` class, which is implemented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class Metrics(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Metrics\n",
    "\n",
    "We have implemented a class that generates relevant metrics to better evaluate the performance of the weighted loss function. This class calculates how much of the loss is associated with each activity level, providing **deeper insights into the model's behavior**. The number of methods is elevated because each condition requires its own function to be coded, as dynamically generating these functions can lead to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics\n",
    "\n",
    "Here we have implemented metrics to determine the portion of the loss represented by each of the categories in the weighted losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class RegressiveMetrics(Metrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def _apply_weighted_loss_by_level(self, input, target, weight_idx):\n",
    "        loss_copy = deepcopy(self.loss_func)\n",
    "        \n",
    "        for idx1 in range(len(loss_copy.weights)):\n",
    "            if is_iter(loss_copy.weights[0]):\n",
    "                for idx2 in range(len(loss_copy.weights[idx1])):\n",
    "                    if (idx1 != weight_idx[0]) | (idx2 != weight_idx[1]):\n",
    "                        loss_copy.weights[idx1][idx2] = 0\n",
    "            else:\n",
    "                if idx1 != weight_idx[1]:\n",
    "                    loss_copy.weights[idx1] = 0\n",
    "                \n",
    "        return loss_copy(input, target)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solar Indices FSMY 10.7 Metrics\n",
    "\n",
    "Here we calculate how each solar activity category contributes to the overall loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class SOLFMYMetrics(RegressiveMetrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__(loss_func)\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Loss_Low(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,0])\n",
    "    \n",
    "    def Loss_Moderate(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,1])\n",
    "    \n",
    "    def Loss_Elevated(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,2])\n",
    "    \n",
    "    def Loss_High(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,3])\n",
    "    \n",
    "    \n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [\n",
    "                self.Loss_Low, \n",
    "                self.Loss_Moderate, \n",
    "                self.Loss_Elevated, \n",
    "                self.Loss_High\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geomagnetic Indices DST and AP Metrics\n",
    "\n",
    "Below, we compile the performance of each category within the geomagnetic indices studied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class GEODSTAPMetrics(RegressiveMetrics):\n",
    "    def __init__(self, loss_func, indices:str='geodstap'):\n",
    "        super().__init__(loss_func)\n",
    "        self.indices = indices\n",
    "        \n",
    "        \n",
    "    # Metrics\n",
    "    def Loss_Low(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,0])\n",
    "    \n",
    "    def Loss_Medium(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,1])\n",
    "    \n",
    "    def Loss_Active(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,2])\n",
    "    \n",
    "    def Loss_G0(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,0])\n",
    "    \n",
    "    def Loss_G1(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,1])\n",
    "    \n",
    "    def Loss_G2(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,2])\n",
    "        \n",
    "    def Loss_G3(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,3])\n",
    "    \n",
    "    def Loss_G4(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,4])\n",
    "    \n",
    "    def Loss_G5(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,5])\n",
    "    \n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        if self.indices == 'geodst':\n",
    "            return [\n",
    "                self.Loss_G0, \n",
    "                self.Loss_G1, \n",
    "                self.Loss_G2, \n",
    "                self.Loss_G3, \n",
    "                self.Loss_G4, \n",
    "                self.Loss_G5\n",
    "            ]\n",
    "        \n",
    "        elif self.indices == 'geoap':\n",
    "            return [\n",
    "                    self.Loss_Low, \n",
    "                    self.Loss_Medium, \n",
    "                    self.Loss_Active\n",
    "                ]\n",
    "        \n",
    "        return [\n",
    "                self.Loss_Low, \n",
    "                self.Loss_Medium, \n",
    "                self.Loss_Active,\n",
    "                self.Loss_G0, \n",
    "                self.Loss_G1, \n",
    "                self.Loss_G2, \n",
    "                self.Loss_G3, \n",
    "                self.Loss_G4, \n",
    "                self.Loss_G5\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "\n",
    "These metrics are used to assess the average misclassifications detected by the `ClassificationLoss()`, providing insights into how the model is improving when the solar or geomagnetic categories of the predictions impact the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class ClassificationMetrics(Metrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "\n",
    "    def _compute_misclassifications(self, predictions, targets):\n",
    "        # Use the weighted loss tensor from the provided loss function\n",
    "        classifier = self.loss_func.weighted_loss_tensor\n",
    "        \n",
    "        # Get the true and predicted labels using the classifier\n",
    "        true_labels = classifier(targets)\n",
    "        predicted_labels = classifier(predictions)\n",
    "\n",
    "        # Misclassifications are those where the predicted label does not match the true label\n",
    "        misclassified_labels = (true_labels != predicted_labels).int() * predicted_labels\n",
    "\n",
    "        return misclassified_labels\n",
    "\n",
    "    def _count_misclassifications_by_position(self, predictions, targets, row, col):\n",
    "        # Calculate misclassifications for a specific (row, column) pair\n",
    "        misclassified_labels = self._compute_misclassifications(predictions, targets)\n",
    "        \n",
    "        # Extract the specific misclassification at the (row, column) position and sum across the time dimension\n",
    "        if row < misclassified_labels.shape[1] and col < misclassified_labels.shape[2]:\n",
    "            misclassification_count = misclassified_labels[:, row, col].sum().item()\n",
    "        else:\n",
    "            misclassification_count = 0  # Out of bounds, assume no misclassification\n",
    "        \n",
    "        return misclassification_count\n",
    "  \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solar Indices FSMY 10.7 Metrics\n",
    "\n",
    "This metrics calculate the average of missclasifications for each of the solar activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class SOLFMYClassificationMetrics(ClassificationMetrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__(loss_func)\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Missclassifications_Low(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 1)\n",
    "\n",
    "    def Missclassifications_Moderate(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 2)\n",
    "\n",
    "    def Missclassifications_Elevated(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 3)\n",
    "\n",
    "    def Missclassifications_High(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 4)\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [\n",
    "                self.Missclassifications_Low,\n",
    "                self.Missclassifications_Moderate, \n",
    "                self.Missclassifications_Elevated, \n",
    "                self.Missclassifications_High\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geomagnetic Indices DST and AP Metrics\n",
    "\n",
    "This metrics calculate the average of missclasifications for each of DST and AP activity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class GEODSTAPClassificationMetrics(ClassificationMetrics):\n",
    "    def __init__(self, loss_func, indices:str='geodstap'):\n",
    "        super().__init__(loss_func)\n",
    "        self.indices = indices\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Missclassifications_Low(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 1)\n",
    "\n",
    "    def Missclassifications_Medium(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 2)\n",
    "\n",
    "    def Missclassifications_Active(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 3)\n",
    "\n",
    "    def Missclassifications_G0(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 1)\n",
    "\n",
    "    def Missclassifications_G1(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 2)\n",
    "\n",
    "    def Missclassifications_G2(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 3)\n",
    "\n",
    "    def Missclassifications_G3(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 4)\n",
    "\n",
    "    def Missclassifications_G4(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 5)\n",
    "\n",
    "    def Missclassifications_G5(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 6)\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        if self.indices == 'geodst':\n",
    "            return [\n",
    "                    self.Missclassifications_G0,\n",
    "                    self.Missclassifications_G1, \n",
    "                    self.Missclassifications_G2, \n",
    "                    self.Missclassifications_G3, \n",
    "                    self.Missclassifications_G4, \n",
    "                    self.Missclassifications_G5\n",
    "                ]\n",
    "        \n",
    "        elif self.indices == 'geoap':\n",
    "            return [\n",
    "                    self.Missclassifications_Low, \n",
    "                    self.Missclassifications_Medium, \n",
    "                    self.Missclassifications_Active\n",
    "                ]\n",
    "        \n",
    "        return [\n",
    "                self.Missclassifications_Low, \n",
    "                self.Missclassifications_Medium, \n",
    "                self.Missclassifications_Active, \n",
    "                self.Missclassifications_G0, \n",
    "                self.Missclassifications_G1,\n",
    "                self.Missclassifications_G2,\n",
    "                self.Missclassifications_G3,\n",
    "                self.Missclassifications_G4,\n",
    "                self.Missclassifications_G5\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Metrics Retrieval\n",
    "\n",
    "This class consolidates all the logic needed to retrieve the appropriate metrics, depending on the type of data used in the training and the specific loss function applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class LossMetrics(Metrics):\n",
    "    def __init__(self, loss_func, indices:str = ''):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.indices = indices\n",
    "\n",
    "    ## Metrics Not Available\n",
    "    def Metrics_Not_Available(self, input, target): return np.nan \n",
    "    \n",
    "    # Metrics retrieval\n",
    "    def get_metrics(self):\n",
    "        if isinstance(self.loss_func, ClassificationLoss):\n",
    "            if self.indices.lower() == 'solfsmy':\n",
    "                return SOLFMYClassificationMetrics(self.loss_func).get_metrics()\n",
    "            if self.indices.lower() in ['geodstap', 'geoap', 'geodst']:\n",
    "                return GEODSTAPClassificationMetrics(self.loss_func, self.indices).get_metrics()\n",
    "        \n",
    "        if isinstance(self.loss_func, WeightedLoss):\n",
    "            if self.indices.lower() == 'solfsmy':\n",
    "                return SOLFMYMetrics(self.loss_func).get_metrics()\n",
    "            \n",
    "            if self.indices.lower() in ['geodstap', 'geoap', 'geodst']:\n",
    "                return GEODSTAPMetrics(self.loss_func, self.indices).get_metrics()\n",
    "        \n",
    "        return [self.Metrics_Not_Available]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Metrics\n",
    "\n",
    "Here we have implemented several metrics to assess various aspects of our models. These metrics are crucial for comparing model performance, enabling us to select the most appropriate loss functions and hyperparameters. This evaluation approach is especially valuable during the Optuna study to identify the best-performing models.\n",
    "\n",
    "<details>\n",
    "<summary><u>References</u></summary>\n",
    "<ul>\n",
    "    <li>M. Steurer, R. J. Hill, and N. Pfeifer, “Metrics for evaluating the performance of machine learning based automated valuation models,” Journal of Property Research, vol. 38, Art. no. 2, Apr. 2021. doi: <a href=\"https://doi.org/10.1080/09599916.2020.1858937\">https://doi.org/10.1080/09599916.2020.1858937</a></li>\n",
    "    <li>S. K. Morley, T. V. Brito, and D. T. Welling, “Measures of model performance based on the log accuracy ratio,” Space Weather, vol. 16, Art. no. 1, 2018. doi: <a href=\"https://doi.org/10.1002/2017SW001669\">https://doi.org/10.1002/2017SW001669</a></li>\n",
    "    <li>M. W. Liemohn, A. D. Shane, A. R. Azari, A. K. Petersen, B. M. Swiger, and A. Mukhopadhyay, “RMSE is not enough: Guidelines to robust data-model comparisons for magnetospheric physics,” Journal of Atmospheric and Solar-Terrestrial Physics, vol. 218, p. 105624, Jul. 2021. doi: <a href=\"https://doi.org/10.1016/j.jastp.2021.105624\">https://doi.org/10.1016/j.jastp.2021.105624</a></li>\n",
    "    <li>Matthew, L. H. Hansen, H. Zhang, G. Angelotti, and J. Gallifant, “A Closer Look at AUROC and AUPRC under Class Imbalance,” arXiv.org, 2024. <a href=\"https://arxiv.org/abs/2401.06091v1\">https://arxiv.org/abs/2401.06091v1</a> (accessed Jul. 2024)</li>\n",
    "</ul>\n",
    "</details>\n",
    "\n",
    "> **Note:** While many of these functions are available in ML libraries, we have reimplemented them to better suit our specific use cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Evaluation Metrics\n",
    "\n",
    "Metrics in this category focus on outliers and are particularly helpful in studies where outlier detection is crucial. This is especially important in our situation, as outliers are associated with solar storms, a significant phenomenon for our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have implemented a general class for Z-score calculation, which is useful for the detection of outliers.\n",
    "\n",
    "> **Note:** The choice of 3.5 as a threshold comes from empirical observations that in normally distributed data, approximately 99.7% of data points should fall within a Z-Score of 3. If a point has a Z-Score greater than 3.5, it is considered significantly deviant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class OutlierDetectionMetrics(Metrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def _modified_z_score(x):\n",
    "        \"\"\"\n",
    "        Calculate the Modified Z-Score for each variable in the tensor.\n",
    "        \n",
    "        Parameters:\n",
    "        tensor (torch.Tensor): Input tensor of shape (batch_size, variables, horizon)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Modified Z-Score tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        median = torch.median(x, dim=2, keepdim=True).values\n",
    "        \n",
    "        mad = torch.median(torch.abs(x - median), dim=2, keepdim=True).values\n",
    "        mad = torch.where(mad == 0, torch.tensor(1.0, device=x.device), mad)\n",
    "        \n",
    "        modified_z_scores = 0.6745 * (x - median) / mad\n",
    "        \n",
    "        return modified_z_scores\n",
    "\n",
    "    def _detect_outliers(self, values):\n",
    "        \"\"\"\n",
    "        Detect outliers based on Modified Z-Scores.\n",
    "        \n",
    "        Parameters:\n",
    "        z_scores (torch.Tensor): Modified Z-Scores tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Boolean tensor indicating outliers\n",
    "        \"\"\"\n",
    "        z_scores = self._modified_z_score(values)\n",
    "        return torch.abs(z_scores) > self.threshold\n",
    "    \n",
    "    def _evaluate_outlier_predicted(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of outlier detection.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        AttrDict: Dictionary with true/false positives, false negatives, indices of true/predicted outliers\n",
    "        \"\"\"    \n",
    "        # Detect outliers based on the threshold\n",
    "        true_outliers = self._detect_outliers(y_true)\n",
    "        pred_outliers = self._detect_outliers(y_pred)\n",
    "        \n",
    "        # Evaluate the detection by comparing true outliers and predicted outliers\n",
    "        tp = torch.sum((pred_outliers & true_outliers).float())  # True Positives\n",
    "        fp = torch.sum((pred_outliers & ~true_outliers).float()) # False Positives\n",
    "        fn = torch.sum((~pred_outliers & true_outliers).float()) # False Negatives\n",
    "        tn = torch.sum((~pred_outliers & ~true_outliers).float()) # True Negatives\n",
    "\n",
    "        return AttrDict({\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "            \"true_outliers\": true_outliers,\n",
    "            \"predicted_outliers\": pred_outliers\n",
    "        })\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score Metric\n",
    "\n",
    "$$\n",
    "\\text{F1\\_Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\qquad \\text{where: }\n",
    "\\begin{cases}\n",
    "    \\text{Precision} = \\frac{TP}{TP + FP} \\\\\n",
    "    \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The F1 Score is a classification metric that we will use to evaluate how well the model is forecasting outliers in the dataset. We have expanded this implementation to include additional metrics related to the F1 Score, using true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). These metrics include:\n",
    "$$\n",
    "\\text{} \\\\\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN} \\\\ \n",
    "\\text{} \\\\\n",
    "\\text{Negative Predictive Value (NPV)} = \\frac{TN}{TN + FN} \\\\\n",
    "\\text{} \\\\\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP} \\\\\n",
    "\\text{} \\\\\n",
    "\\Delta_\\text{Detected Outliers} = |y_{outliers} - \\hat{y}_{outliers}|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class F1ScoreMetrics(OutlierDetectionMetrics):\n",
    "    def __init__(self, threshold=3.5, metrics='F1_Score'):\n",
    "        super().__init__(threshold)\n",
    "        self.metrics = metrics\n",
    "    \n",
    "\n",
    "    # Metrics\n",
    "    def Precision(self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "\n",
    "        # To avoid divide by 0\n",
    "        if (stats.tp + stats.fp) > 0:\n",
    "            precision = stats.tp / (stats.tp + stats.fp)  \n",
    "        else: \n",
    "            precision = torch.tensor(0.0)\n",
    "\n",
    "        return precision\n",
    "    \n",
    "    def Recall(self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "\n",
    "        if (stats.tp + stats.fn) > 0:\n",
    "            recall = stats.tp / (stats.tp + stats.fn)\n",
    "        else: \n",
    "            recall = torch.tensor(0.0)\n",
    "\n",
    "        return recall\n",
    "    \n",
    "    def F1_Score(self, y_true, y_pred):\n",
    "        precision = self.Precision(y_true, y_pred)\n",
    "        recall = self.Recall(y_true, y_pred)\n",
    "\n",
    "        if (precision + recall) > 0:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        else: \n",
    "            f1_score = torch.tensor(0.0)\n",
    "\n",
    "        return f1_score\n",
    "    \n",
    "    def Accuracy_Score(self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "        \n",
    "        if (stats.tp + stats.fp + stats.fn + stats.tn) > 0:\n",
    "            return (stats.tp + stats.tn) / (stats.tp + stats.fp + stats.fn + stats.tn)\n",
    "        else:\n",
    "            return torch.tensor(0.0)\n",
    "    \n",
    "    def Specificity(self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "        \n",
    "        if (stats.tn + stats.fp) > 0:\n",
    "            return stats.tn / (stats.tn + stats.fp)\n",
    "        else:\n",
    "            return torch.tensor(0.0)\n",
    "\n",
    "    def Negative_Predictive_Value(self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "\n",
    "        if (stats.tn + stats.fn) > 0:\n",
    "            return stats.tn / (stats.tn + stats.fn)\n",
    "        else:\n",
    "            return torch.tensor(0.0)\n",
    "    \n",
    "    def Δ_Detected_Outliers (self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "        \n",
    "        return torch.sum(stats.true_outliers & ~stats.predicted_outliers)\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        if self.metrics == 'F1_Score':\n",
    "            return [self.F1_Score]\n",
    "        elif self.metrics == 'All':\n",
    "            return [self.Precision, self.Recall, self.F1_Score, self.Accuracy_Score, self.Specificity, self.Negative_Predictive_Value, self.Δ_Detected_Outliers ]\n",
    "        else:\n",
    "            return [self.Precision, self.Recall, self.F1_Score, self.Δ_Detected_Outliers ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area Under the Precision-Recall Curve (AUPRC)\n",
    "\n",
    "$$\n",
    "\\text{AUPRC} = \\int_{0}^{1} \\text{Precision}(\\text{Recall}) \\, d(\\text{Recall})\n",
    "$$\n",
    "\n",
    "The Area Under the Precision-Recall Curve (AUPRC) is a metric used to evaluate the effectiveness of a model in identifying rare, important events (outliers) in imbalanced datasets. In time series forecasting, especially when detecting outliers like solar storms, AUPRC is particularly useful. Since solar storms are rare compared to normal solar activity, traditional metrics like accuracy may not provide a clear picture of the model’s performance. However, AUPRC focuses on how well the model balances identifying true solar storms (high recall) while minimizing false positives (high precision).\n",
    "\n",
    "A higher AUPRC value suggests that the model is effective in detecting solar storms without generating too many false alarms, making it a crucial metric for evaluating the model’s ability to correctly identify these rare but significant events in your forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class AUPRCMetric(OutlierDetectionMetrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__(threshold)\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def AURPC(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Area Under the Precision-Recall Curve (AUPRC).\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: AUPRC score\n",
    "        \"\"\"\n",
    "        pred_z_scores = self._modified_z_score(y_pred)\n",
    "        \n",
    "        pred_z_scores_flat = pred_z_scores.view(-1).cpu().numpy()\n",
    "        true_outliers_flat = self._detect_outliers(y_true).view(-1).cpu().numpy()\n",
    "        \n",
    "        # Use precision_recall_curve to get precision and recall for different thresholds\n",
    "        precision, recall, _ = precision_recall_curve(true_outliers_flat, pred_z_scores_flat)\n",
    "        \n",
    "        auprc_value = auc(recall, precision)\n",
    "        \n",
    "        return torch.tensor(auprc_value, device=y_true.device)\n",
    "    \n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.AURPC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurtois and Skewness Difference (KSD)\n",
    "\n",
    "$$\n",
    "\\text{Skewness}(S) = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^3}{\\left(\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right)^{3/2}} \\qquad\n",
    "\\text{Kurtois}(K) = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^4}{\\left(\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\right)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "Differences in skewness (S), which measures the \"tailedness\" of a distribution, and kurtosis (K), which quantifies the asymmetry of a distribution, are valuable metrics for evaluating the ability of the model to detect outliers because they assess the tails of distributions. Traditional metrics often focus on central tendencies, such as the mean or median, and might not adequately capture the presence or absence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class KSDifferenceMetric(Metrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def skewness(x):\n",
    "        mean = torch.mean(x)\n",
    "        std_dev = torch.std(x, unbiased=False)\n",
    "        \n",
    "        skewness = torch.mean(((x - mean) / std_dev) ** 3)\n",
    "        return skewness\n",
    "\n",
    "    @staticmethod\n",
    "    def kurtosis(x):\n",
    "        mean = torch.mean(x)\n",
    "        std_dev = torch.std(x, unbiased=False)\n",
    "        \n",
    "        kurtosis = torch.mean(((x - mean) / std_dev) ** 4)\n",
    "        return kurtosis\n",
    "    \n",
    "\n",
    "    # Metrics\n",
    "    def Δ_Skewness(self, y_true, y_pred):\n",
    "        true_skewness = KSDifferenceMetric.skewness(y_true)\n",
    "        pred_skewness = KSDifferenceMetric.skewness(y_pred)\n",
    "        \n",
    "        return torch.abs(true_skewness - pred_skewness)\n",
    "    \n",
    "    def Δ_Kurtosis(self, y_true, y_pred):\n",
    "        true_kurtosis = KSDifferenceMetric.kurtosis(y_true)\n",
    "        pred_kurtosis = KSDifferenceMetric.kurtosis(y_pred)\n",
    "        \n",
    "        return torch.abs(true_kurtosis - pred_kurtosis)\n",
    "        \n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.Δ_Skewness, self.Δ_Kurtosis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Metrics\n",
    "Metrics in this category quantify how well a model captures the trends in observed data. We will use two from this category:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson Linear Correlation Coefficient ($R$)\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}}\n",
    "$$\n",
    "\n",
    "The most commonly used metric for correlation is the **Pearson Linear Correlation Coefficient ($R$)**. It is important to note that a good $R$-value should be both statistically significant and above an appropriate threshold for the study.\n",
    "\n",
    "#### Coefficient of Determination Metric ($R^2$)\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\qquad \\text{where: }\n",
    "\\begin{cases}\n",
    "    \\text{Residual Sum of Squares (RSS)} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\\\\n",
    "    \\text{Total Sum of Squares (TSS)} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The **Coefficient of Determination Metric ($R^2$)** quantifies the proportion of variance in the observed solar indices that is explained by the model's predictions. This measure is particularly useful because it provides a direct indication of how well the model captures the underlying patterns in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class AssociationMetrics(Metrics):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Metrics\n",
    "    def R_Correlation(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Pearson Correlation Coefficient (R Correlation) between true and predicted values.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: R Correlation coefficient\n",
    "        \"\"\"\n",
    "        y_true_flat = y_true.view(-1)\n",
    "        y_pred_flat = y_pred.view(-1)\n",
    "        \n",
    "        # To be able to use torch.corrcoef, we need to stack the tensors\n",
    "        stacked = torch.stack([y_true_flat, y_pred_flat])\n",
    "        \n",
    "        corr_matrix = torch.corrcoef(stacked)\n",
    "        \n",
    "        r_value = corr_matrix[0, 1]\n",
    "        return r_value\n",
    "\n",
    "    def R2_Score(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the R^2 score.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: R^2 score\n",
    "        \"\"\"\n",
    "        y_true_mean = torch.mean(y_true, dim=2, keepdim=True)\n",
    "        print(y_true_mean.shape, y_true.shape)\n",
    "        \n",
    "        # Total Sum of Squares\n",
    "        ss_tot = torch.sum((y_true - y_true_mean) ** 2)\n",
    "        \n",
    "        # Residual Sum of Squares\n",
    "        ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "        \n",
    "        r2 = 1 - ss_res / ss_tot\n",
    "        \n",
    "        return r2\n",
    "    \n",
    "    \n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.R_Correlation, self.R2_Score]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metrics\n",
    "\n",
    "The metrics in this category are used to quantify the closeness of model predictions to actual observations, providing a measure of how well a model reproduces real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric Mean Absolute Percentage Error Metric (sMAPE)\n",
    "\n",
    "$$\n",
    "\\text{sMAPE} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{|y_i - \\hat{y}_i|}{\\frac{|y_i| + |\\hat{y}_i|}{2}} \\times 100\n",
    "$$\n",
    "\n",
    "The Symmetric Mean Absolute Percentage Error (sMAPE) offers an alternative to traditional metrics like MAE (Mean Absolute Error), which are sensitive to larger values. By scaling the absolute error to the average magnitude of the data and model pair, sMAPE mitigates the undue influence of outliers. However, as an average-based metric, it remains susceptible to extreme values within the dataset. \n",
    "\n",
    "Since sMAPE is not directly aligned with either of the losses used, it may not fully capture the strengths or weaknesses of models trained with these loss functions. For example, a model optimized for MAE might perform poorly on sMAPE if the predictions are close in absolute terms but relatively inaccurate for small actual values.\n",
    "\n",
    "#### Median Symmetric Accuracy Metric (MSA)\n",
    "\n",
    "$$\n",
    "\\text{MSA} = 100 \\times (e^{\\tilde{x}}-1) \\qquad \\text{where: } x = \\left|\\ln\\left(\\frac{\\hat{y_i}}{y_i}\\right)\\right|\n",
    "$$\n",
    "\n",
    "The Median Symmetric Accuracy (MSA) provides a more robust accuracy assessment, particularly for datasets prone to outliers. It leverages the median of the log-transformed ratios between model and observed values, minimizing the impact of extreme deviations. This characteristic makes MSA particularly suitable for characterizing model performance in capturing the overall distribution and minimizing the effect of outliers inherent in observational data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class AccuracyMetrics(Metrics):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def sMAPE(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Symmetric Mean Absolute Percentage Error (sMAPE).\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: sMAPE value\n",
    "        \"\"\"\n",
    "        epsilon = 1e-8  # small constant to prevent division by zero\n",
    "        abs_error = torch.abs(y_true - y_pred)\n",
    "        symetric_error = ((torch.abs(y_true) + torch.abs(y_pred)) / 2.0) + epsilon\n",
    "        \n",
    "        smape = torch.mean(abs_error / symetric_error) * 100\n",
    "        return smape\n",
    "    \n",
    "    def MSA(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Mean Scaled Absolute Error (MSA).\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: MSA value\n",
    "        \"\"\"\n",
    "        # Calculate the natural logarithm of the ratio\n",
    "        log_ratio = torch.abs(torch.log(y_pred / y_true))\n",
    "\n",
    "        msa = (torch.exp(torch.median(log_ratio)) - 1) * 100\n",
    "        \n",
    "        return msa\n",
    "    \n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.sMAPE, self.MSA]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Metrics\n",
    "\n",
    "This category examines the systematic overestimation or underestimation of observations by a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric Signed Percentage Bias Metric (SSPB)\n",
    "\n",
    "$$\n",
    "\\text{SSPB} = \\text{sign}(\\tilde{x}) \\times \\left|e^{\\tilde{x}} - 1\\right| \\qquad \\text{where: }\n",
    "\\begin{cases}\n",
    "    x = \\ln\\left(\\frac{\\hat{y_i}}{y_i}\\right) \\\\\n",
    "    \\\\\n",
    "    \\text{sign}(x) = \n",
    "    \\begin{cases} \n",
    "        -1 & \\text{if } x < 0, \\\\\n",
    "        0 & \\text{if } x = 0, \\\\\n",
    "        1 & \\text{if } x > 0.\n",
    "    \\end{cases}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The Symmetric Signed Percentage Bias (SSPB) is particularly well-suited for assessing bias in time series data characterized by high variability spanning several orders of magnitude. Unlike conventional bias metrics like Mean Error (ME), which are sensitive to extreme values, SSPB leverages the logarithm of the model-to-observation ratio. This logarithmic transformation effectively mitigates the disproportionate influence of outliers, providing a more balanced assessment of systematic overestimation or underestimation by the model.\n",
    "\n",
    "This characteristic is especially valuable when evaluating models dealing with data such as radiation belt electron fluxes, which exhibit significant fluctuations. By using the median of these logarithmic ratios, SSPB further enhances its robustness, offering a stable measure of central tendency even in the presence of non-normal error distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BiasMetrics(Metrics):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Metrics\n",
    "    def SSPB(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Symmetric Signed Percentage Bias (SSPB).\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: SSPB value\n",
    "        \"\"\"\n",
    "        log_ratio = torch.log(y_pred / y_true)\n",
    "        median_log_ratio = torch.median(log_ratio)\n",
    "\n",
    "        sign = torch.sign(median_log_ratio)\n",
    "        \n",
    "        return sign * (torch.exp(torch.abs(median_log_ratio)) - 1) * 100\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.SSPB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "device = 'cpu'\n",
    "ranges = {'A': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'B': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'C': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'D': np.array([[0, 1], [1, 2], [2, 3], [3, 4]])}\n",
    "\n",
    "weights = {'A': np.array([1, 2, 3, 4])}\n",
    "\n",
    "target = torch.tensor([[[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]]], device=device, dtype=torch.float32)\n",
    "\n",
    "input = target + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "def test_LossMetrics():\n",
    "    loss = wMAELoss(ranges, weights).to(device)\n",
    "    metrics = LossMetrics(loss, 'SolFSMY').get_metrics()\n",
    "\n",
    "    loss_value = loss(input, target)\n",
    "    metrics_values = [metric(input, target) for metric in metrics]\n",
    "\n",
    "    assert torch.isclose(loss_value, sum(metrics_values)), f\"Expected {loss_value}, but got {sum(metrics_values)} ({metrics_values})\"\n",
    "    print(\"LossMetrics test passed!\")\n",
    "\n",
    "def test_LossMetrics_for_classification():\n",
    "    loss = ClassificationLoss(ranges, MSELoss()).to(device)\n",
    "    metrics = SOLFMYClassificationMetrics(loss)\n",
    "\n",
    "    # Compute the total misclassifications manually for all specific positions\n",
    "    total_counts = 0\n",
    "    total_counts += metrics.Missclassifications_Low(input, target)\n",
    "    total_counts += metrics.Missclassifications_Moderate(input, target)\n",
    "    total_counts += metrics.Missclassifications_Elevated(input, target)\n",
    "    total_counts += metrics.Missclassifications_High(input, target)\n",
    "\n",
    "    # Use the generate_metrics method to retrieve and calculate all defined metrics\n",
    "    metrics_functions = LossMetrics(loss, 'SolFSMY').get_metrics()\n",
    "    metrics_values = [metric(input, target) for metric in metrics_functions]\n",
    "\n",
    "    # Assert that the total manually calculated matches the sum of individual metrics\n",
    "    assert np.isclose(total_counts, sum(metrics_values)), f\"Expected {total_counts}, but got {sum(metrics_values)} ({metrics_values})\"\n",
    "    print(\"LossMetrics for classification loss test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor([\n",
    "    [[10, 12, 12, 13, 12, 12, 12, 14, 12, 100], [20, 22, 23, 20, 22, 20, 21, 22, 23, 200]],\n",
    "    [[-11, -12, -13, -13, -14, -14, -12, -13, -14, -105], [-22, -23, -25, -23, -22, -24, -23, -22, -25, -210]]\n",
    "], dtype=torch.float)\n",
    "\n",
    "y_pred = torch.tensor([\n",
    "    [[11, 12, 12, 13, 12, 12, 13, 14, 13, 90], [21, 22, 23, 21, 22, 21, 22, 22, 23, 195]],\n",
    "    [[-12, -13, -14, -14, -15, -14, -13, -14, -14, -10], [-23, -24, -26, -24, -23, -25, -24, -23, -26, -205]]\n",
    "], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test_OutlierDetectionMetrics():\n",
    "    metrics = F1ScoreMetrics(metrics=' ').get_metrics()\n",
    "    metrics_precision = metrics[0](y_true, y_pred) \n",
    "    metrics_recall = metrics[1](y_true, y_pred)\n",
    "    metrics_f1_score = metrics[2](y_true, y_pred)\n",
    "    metrics_outliers_difference = metrics[3](y_true, y_pred)\n",
    "\n",
    "    f1 = 2 * (metrics_precision * metrics_recall) / (metrics_precision + metrics_recall)\n",
    "\n",
    "    assert metrics_precision == 1.0, f\"Expected 1.0, but got {metrics_precision}\"\n",
    "    assert metrics_recall == 0.75, f\"Expected 0.75, but got {metrics_recall}\"\n",
    "    assert metrics_f1_score == f1, f\"Expected {f1}, but got {metrics_f1_score}\"\n",
    "    assert metrics_outliers_difference == 1, f\"Expected 2, but got {metrics_outliers_difference}\"\n",
    "\n",
    "    print(\"OutlierDetectionMetrics test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def evaluate_get_metrics (metrics:list):\n",
    "    for metric in metrics:\n",
    "        print(f\"{metric.__name__}: {metric(y_true, y_pred)}\")\n",
    "\n",
    "def test_F1ScoreMetrics():\n",
    "    metrics = F1ScoreMetrics(metrics='All').get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"F1ScoreMetrics test passed!\")\n",
    "\n",
    "def test_AUPRCMetric():\n",
    "    metrics = AUPRCMetric().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"AUPRCMetric test passed!\")\n",
    "\n",
    "def test_KSDifferenceMetric():\n",
    "    metrics = KSDifferenceMetric().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"KSDifferenceMetric test passed!\")\n",
    "\n",
    "def test_AssociationMetrics():\n",
    "    metrics = AssociationMetrics().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"AssociationMetrics test passed!\")\n",
    "\n",
    "def test_AccuracyMetrics():\n",
    "    metrics = AccuracyMetrics().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"AccuracyMetrics test passed!\")\n",
    "\n",
    "def test_BiasMetrics():\n",
    "    metrics = BiasMetrics().get_metrics()\n",
    "    evaluate_get_metrics(metrics)\n",
    "    print(\"BiasMetrics test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LossMetrics test passed!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LossMetrics for classification loss test passed!\n",
      "OutlierDetectionMetrics test passed!\n",
      "Precision: 1.0\n",
      "Recall: 0.75\n",
      "F1_Score: 0.8571428656578064\n",
      "Accuracy_Score: 0.9750000238418579\n",
      "Specificity: 1.0\n",
      "Negative_Predictive_Value: 0.9729729890823364\n",
      "Δ_Detected_Outliers: 1\n",
      "F1ScoreMetrics test passed!\n",
      "AURPC: 0.7721153846153846\n",
      "AUPRCMetric test passed!\n",
      "Δ_Skewness: 0.018215760588645935\n",
      "Δ_Kurtosis: 1.8897819519042969\n",
      "KSDifferenceMetric test passed!\n",
      "R_Correlation: 0.9610453844070435\n",
      "torch.Size([2, 2, 1]) torch.Size([2, 2, 10])\n",
      "R2_Score: 0.8769017457962036\n",
      "AssociationMetrics test passed!\n",
      "sMAPE: 7.933314800262451\n",
      "MSA: 4.347825050354004\n",
      "AccuracyMetrics test passed!\n",
      "SSPB: 4.166662693023682\n",
      "BiasMetrics test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_LossMetrics()\n",
    "test_LossMetrics_for_classification()\n",
    "test_OutlierDetectionMetrics()\n",
    "test_F1ScoreMetrics()\n",
    "test_AUPRCMetric()\n",
    "test_KSDifferenceMetric()\n",
    "test_AssociationMetrics()\n",
    "test_AccuracyMetrics()\n",
    "test_BiasMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from nbdev import *\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
