{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp metrics\n",
    "\n",
    "#|export\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsai.basics import *\n",
    "from swdf.losses import wMAELoss, MSELoss, WeightedLoss, ClassificationLoss\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "---\n",
    "## Index\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Intro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class Metrics(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Metrics\n",
    "\n",
    "We have implemented a class that generates relevant metrics to better evaluate the performance of the weighted loss function. This class calculates how much of the loss is associated with each activity level, providing **deeper insights into the model's behavior**. The number of methods is elevated because each condition requires its own function to be coded, as dynamically generating these functions can lead to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class RegressiveMetrics(Metrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def _apply_weighted_loss_by_level(self, input, target, weight_idx):\n",
    "        loss_copy = deepcopy(self.loss_func)\n",
    "        \n",
    "        for idx1 in range(len(loss_copy.weights)):\n",
    "            if is_iter(loss_copy.weights[0]):\n",
    "                for idx2 in range(len(loss_copy.weights[idx1])):\n",
    "                    if (idx1 != weight_idx[0]) | (idx2 != weight_idx[1]):\n",
    "                        loss_copy.weights[idx1][idx2] = 0\n",
    "            else:\n",
    "                if idx1 != weight_idx[1]:\n",
    "                    loss_copy.weights[idx1] = 0\n",
    "                \n",
    "        return loss_copy(input, target)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solar Indices FSMY 10.7 Metrics\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class SOLFMYMetrics(RegressiveMetrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__(loss_func)\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Loss_Low(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,0])\n",
    "    \n",
    "    def Loss_Moderate(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,1])\n",
    "    \n",
    "    def Loss_Elevated(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,2])\n",
    "    \n",
    "    def Loss_High(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,3])\n",
    "    \n",
    "    \n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [\n",
    "                self.Loss_Low, \n",
    "                self.Loss_Moderate, \n",
    "                self.Loss_Elevated, \n",
    "                self.Loss_High\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geomagnetic Indices DST and AP Metrics\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class GEODSTAPMetrics(RegressiveMetrics):\n",
    "    def __init__(self, loss_func, indices:str='geodstap'):\n",
    "        super().__init__(loss_func)\n",
    "        self.indices = indices\n",
    "        \n",
    "        \n",
    "    # Metrics\n",
    "    def Loss_Low(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,0])\n",
    "    \n",
    "    def Loss_Medium(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,1])\n",
    "    \n",
    "    def Loss_Active(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [0,2])\n",
    "    \n",
    "    def Loss_G0(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,0])\n",
    "    \n",
    "    def Loss_G1(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,1])\n",
    "    \n",
    "    def Loss_G2(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,2])\n",
    "        \n",
    "    def Loss_G3(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,3])\n",
    "    \n",
    "    def Loss_G4(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,4])\n",
    "    \n",
    "    def Loss_G5(self, input, target):\n",
    "        return self._apply_weighted_loss_by_level(input, target, [1,5])\n",
    "    \n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        if self.indices == 'geodst':\n",
    "            return [\n",
    "                self.Loss_G0, \n",
    "                self.Loss_G1, \n",
    "                self.Loss_G2, \n",
    "                self.Loss_G3, \n",
    "                self.Loss_G4, \n",
    "                self.Loss_G5\n",
    "            ]\n",
    "        \n",
    "        elif self.indices == 'geoap':\n",
    "            return [\n",
    "                    self.Loss_Low, \n",
    "                    self.Loss_Medium, \n",
    "                    self.Loss_Active\n",
    "                ]\n",
    "        \n",
    "        return [\n",
    "                self.Loss_Low, \n",
    "                self.Loss_Medium, \n",
    "                self.Loss_Active,\n",
    "                self.Loss_G0, \n",
    "                self.Loss_G1, \n",
    "                self.Loss_G2, \n",
    "                self.Loss_G3, \n",
    "                self.Loss_G4, \n",
    "                self.Loss_G5\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class ClassificationMetrics(Metrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "\n",
    "\n",
    "    def _compute_misclassifications(self, predictions, targets):\n",
    "        # Use the weighted loss tensor from the provided loss function\n",
    "        classifier = self.loss_func.weighted_loss_tensor\n",
    "        \n",
    "        # Get the true and predicted labels using the classifier\n",
    "        true_labels = classifier(targets)\n",
    "        predicted_labels = classifier(predictions)\n",
    "\n",
    "        # Misclassifications are those where the predicted label does not match the true label\n",
    "        misclassified_labels = (true_labels != predicted_labels).int() * predicted_labels\n",
    "\n",
    "        return misclassified_labels\n",
    "\n",
    "    def _count_misclassifications_by_position(self, predictions, targets, row, col):\n",
    "        # Calculate misclassifications for a specific (row, column) pair\n",
    "        misclassified_labels = self._compute_misclassifications(predictions, targets)\n",
    "        \n",
    "        # Extract the specific misclassification at the (row, column) position and sum across the time dimension\n",
    "        if row < misclassified_labels.shape[1] and col < misclassified_labels.shape[2]:\n",
    "            misclassification_count = misclassified_labels[:, row, col].sum().item()\n",
    "        else:\n",
    "            misclassification_count = 0  # Out of bounds, assume no misclassification\n",
    "        \n",
    "        return misclassification_count\n",
    "  \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solar Indices FSMY 10.7 Metrics\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class SOLFMYClassificationMetrics(ClassificationMetrics):\n",
    "    def __init__(self, loss_func):\n",
    "        super().__init__(loss_func)\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Missclassifications_Low(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 1)\n",
    "\n",
    "    def Missclassifications_Moderate(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 2)\n",
    "\n",
    "    def Missclassifications_Elevated(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 3)\n",
    "\n",
    "    def Missclassifications_High(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 4)\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [\n",
    "                self.Missclassifications_Low,\n",
    "                self.Missclassifications_Moderate, \n",
    "                self.Missclassifications_Elevated, \n",
    "                self.Missclassifications_High\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geomagnetic Indices DST and AP Metrics\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class GEODSTAPClassificationMetrics(ClassificationMetrics):\n",
    "    def __init__(self, loss_func, indices:str='geodstap'):\n",
    "        super().__init__(loss_func)\n",
    "        self.indices = indices\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    def Missclassifications_Low(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 1)\n",
    "\n",
    "    def Missclassifications_Medium(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 2)\n",
    "\n",
    "    def Missclassifications_Active(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 0, 3)\n",
    "\n",
    "    def Missclassifications_G0(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 1)\n",
    "\n",
    "    def Missclassifications_G1(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 2)\n",
    "\n",
    "    def Missclassifications_G2(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 3)\n",
    "\n",
    "    def Missclassifications_G3(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 4)\n",
    "\n",
    "    def Missclassifications_G4(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 5)\n",
    "\n",
    "    def Missclassifications_G5(self, predictions, targets):\n",
    "        return self._count_misclassifications_by_position(predictions, targets, 1, 6)\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        if self.indices == 'geodst':\n",
    "            return [\n",
    "                    self.Missclassifications_G0,\n",
    "                    self.Missclassifications_G1, \n",
    "                    self.Missclassifications_G2, \n",
    "                    self.Missclassifications_G3, \n",
    "                    self.Missclassifications_G4, \n",
    "                    self.Missclassifications_G5\n",
    "                ]\n",
    "        \n",
    "        elif self.indices == 'geoap':\n",
    "            return [\n",
    "                    self.Missclassifications_Low, \n",
    "                    self.Missclassifications_Medium, \n",
    "                    self.Missclassifications_Active\n",
    "                ]\n",
    "        \n",
    "        return [\n",
    "                self.Missclassifications_Low, \n",
    "                self.Missclassifications_Medium, \n",
    "                self.Missclassifications_Active, \n",
    "                self.Missclassifications_G0, \n",
    "                self.Missclassifications_G1,\n",
    "                self.Missclassifications_G2,\n",
    "                self.Missclassifications_G3,\n",
    "                self.Missclassifications_G4,\n",
    "                self.Missclassifications_G5\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Metrics Retrieval\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class LossMetrics(Metrics):\n",
    "    def __init__(self, loss_func, indices:str = ''):\n",
    "        super().__init__()\n",
    "        self.loss_func = loss_func\n",
    "        self.indices = indices\n",
    "\n",
    "    ## Metrics Not Available\n",
    "    def Metrics_Not_Available(self, input, target): return np.nan \n",
    "    \n",
    "    # Metrics retrieval\n",
    "    def get_metrics(self):\n",
    "        if isinstance(self.loss_func, ClassificationLoss):\n",
    "            if self.indices.lower() == 'solfsmy':\n",
    "                return SOLFMYClassificationMetrics(self.loss_func).get_metrics()\n",
    "            if self.indices.lower() in ['geodstap', 'geoap', 'geodst']:\n",
    "                return GEODSTAPClassificationMetrics(self.loss_func, self.indices).get_metrics()\n",
    "        \n",
    "        if isinstance(self.loss_func, WeightedLoss):\n",
    "            if self.indices.lower() == 'solfsmy':\n",
    "                return SOLFMYMetrics(self.loss_func).get_metrics()\n",
    "            \n",
    "            if self.indices.lower() in ['geodstap', 'geoap', 'geodst']:\n",
    "                return GEODSTAPMetrics(self.loss_func, self.indices).get_metrics()\n",
    "        \n",
    "        return [self.Metrics_Not_Available]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Metrics\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Metrics\n",
    "[Text]\n",
    "\n",
    "> The choice of 3.5 comes from empirical observations that in normally distributed data, approximately 99.7% of data should fall within a Z-Score of 3. If a point has a Z-Score greater than 3.5, it is considered significantly deviant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class OutlierDetectionMetrics(Metrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def _modified_z_score(x):\n",
    "        \"\"\"\n",
    "        Calculate the Modified Z-Score for each variable in the tensor.\n",
    "        \n",
    "        Parameters:\n",
    "        tensor (torch.Tensor): Input tensor of shape (batch_size, variables, horizon)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Modified Z-Score tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        median = torch.median(x, dim=2, keepdim=True).values\n",
    "        \n",
    "        mad = torch.median(torch.abs(x - median), dim=2, keepdim=True).values\n",
    "        mad = torch.where(mad == 0, torch.tensor(1.0, device=x.device), mad)\n",
    "        \n",
    "        modified_z_scores = 0.6745 * (x - median) / mad\n",
    "        \n",
    "        return modified_z_scores\n",
    "\n",
    "    def _detect_outliers(self, values):\n",
    "        \"\"\"\n",
    "        Detect outliers based on Modified Z-Scores.\n",
    "        \n",
    "        Parameters:\n",
    "        z_scores (torch.Tensor): Modified Z-Scores tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Boolean tensor indicating outliers\n",
    "        \"\"\"\n",
    "        z_scores = self._modified_z_score(values)\n",
    "        return torch.abs(z_scores) > self.threshold\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> list:\n",
    "        return NotImplementedError\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1ScoreMetrics(OutlierDetectionMetrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__(threshold)\n",
    "\n",
    "    def _evaluate_outlier_predicted(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Evaluate the performance of outlier detection.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        AttrDict: Dictionary with true/false positives, false negatives, indices of true/predicted outliers\n",
    "        \"\"\"    \n",
    "        # Detect outliers based on the threshold\n",
    "        true_outliers = self._detect_outliers(y_true)\n",
    "        pred_outliers = self._detect_outliers(y_pred)\n",
    "        \n",
    "        # Evaluate the detection by comparing true outliers and predicted outliers\n",
    "        tp = torch.sum((pred_outliers & true_outliers).float())  # True Positives\n",
    "        fp = torch.sum((pred_outliers & ~true_outliers).float()) # False Positives\n",
    "        fn = torch.sum((~pred_outliers & true_outliers).float()) # False Negatives\n",
    "\n",
    "        return AttrDict({\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"true_outliers\": true_outliers,\n",
    "            \"predicted_outliers\": pred_outliers\n",
    "        })\n",
    "    \n",
    "\n",
    "    # Metrics\n",
    "    def Precision(self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "\n",
    "        if (stats.tp + stats.fp) > 0:\n",
    "            precision = stats.tp / (stats.tp + stats.fp)  \n",
    "        else: \n",
    "            precision = torch.tensor(0.0)\n",
    "\n",
    "        return precision\n",
    "    \n",
    "    def Recall(self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "\n",
    "        if (stats.tp + stats.fn) > 0:\n",
    "            recall = stats.tp / (stats.tp + stats.fn)\n",
    "        else: \n",
    "            recall = torch.tensor(0.0)\n",
    "\n",
    "        return recall\n",
    "    \n",
    "    def F1_Score(self, y_true, y_pred):\n",
    "        precision = self.Precision(y_true, y_pred)\n",
    "        recall = self.Recall(y_true, y_pred)\n",
    "\n",
    "        if (precision + recall) > 0:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        else: \n",
    "            f1_score = torch.tensor(0.0)\n",
    "\n",
    "        return f1_score\n",
    "    \n",
    "    def Outliers_Difference(self, y_true, y_pred):\n",
    "        stats = self._evaluate_outlier_predicted(y_true, y_pred)\n",
    "        \n",
    "        return torch.sum(stats.true_outliers & ~stats.predicted_outliers)\n",
    "\n",
    "\n",
    "    # Metrics retrieval function\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.Precision, self.Recall, self.F1_Score, self.Outliers_Difference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under the Precision-Recall Curve (AURPC)\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class AUPRCMetric(OutlierDetectionMetrics):\n",
    "    def __init__(self, threshold=3.5):\n",
    "        super().__init__(threshold=threshold)\n",
    "\n",
    "    def AURPC(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the Area Under the Precision-Recall Curve (AUPRC).\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: AUPRC score\n",
    "        \"\"\"\n",
    "        pred_z_scores = self._modified_z_score(y_pred)\n",
    "        \n",
    "        pred_z_scores_flat = pred_z_scores.view(-1).cpu().numpy()\n",
    "        true_outliers_flat = self._detect_outliers(y_true).view(-1).cpu().numpy()\n",
    "        \n",
    "        # Use precision_recall_curve to get precision and recall for different thresholds\n",
    "        precision, recall, _ = precision_recall_curve(true_outliers_flat, pred_z_scores_flat)\n",
    "        \n",
    "        print(type(precision), type(recall))\n",
    "        auprc_value = auc(recall, precision)\n",
    "        \n",
    "        return torch.tensor(auprc_value, device=y_true.device)\n",
    "\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.AURPC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Metric\n",
    "[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# TODO: Revisar\n",
    "class R2Score(Metrics):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def R2(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the R^2 score.\n",
    "        \n",
    "        Parameters:\n",
    "        y_true (torch.Tensor): Actual values tensor of shape (batch_size, variables, horizon)\n",
    "        y_pred (torch.Tensor): Predicted values tensor of the same shape as y_true\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: R^2 score\n",
    "        \"\"\"\n",
    "        # Calculate the mean of the actual values\n",
    "        y_true_mean = torch.mean(y_true, dim=2, keepdim=True)\n",
    "        \n",
    "        # Calculate the total sum of squares\n",
    "        ss_tot = torch.sum((y_true - y_true_mean) ** 2)\n",
    "        \n",
    "        # Calculate the residual sum of squares\n",
    "        ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "        \n",
    "        # Calculate the R^2 score\n",
    "        r2 = 1 - ss_res / ss_tot\n",
    "        \n",
    "        return r2\n",
    "\n",
    "    def get_metrics(self) -> list:\n",
    "        return [self.R2]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "ranges = {'A': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'B': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'C': np.array([[0, 1], [1, 2], [2, 3], [3, 4]]),\n",
    "          'D': np.array([[0, 1], [1, 2], [2, 3], [3, 4]])}\n",
    "\n",
    "weights = {'A': np.array([1, 2, 3, 4])}\n",
    "\n",
    "target = torch.tensor([[[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                        [0.5, 1.5, 2.5, 3.5, 4.5, 5.5]]], device=device, dtype=torch.float32)\n",
    "\n",
    "input = target + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "def test_LossMetrics():\n",
    "    loss = wMAELoss(ranges, weights).to(device)\n",
    "    metrics = LossMetrics(loss, 'SolFSMY').get_metrics()\n",
    "\n",
    "    loss_value = loss(input, target)\n",
    "    metrics_values = [metric(input, target) for metric in metrics]\n",
    "\n",
    "    assert torch.isclose(loss_value, sum(metrics_values)), f\"Expected {loss_value}, but got {sum(metrics_values)} ({metrics_values})\"\n",
    "    print(\"LossMetrics test passed!\")\n",
    "\n",
    "def test_LossMetrics_for_classification():\n",
    "    loss = ClassificationLoss(ranges, MSELoss()).to(device)\n",
    "    metrics = SOLFMYClassificationMetrics(loss)\n",
    "\n",
    "    # Compute the total misclassifications manually for all specific positions\n",
    "    total_counts = 0\n",
    "    total_counts += metrics.Missclassifications_Low(input, target)\n",
    "    total_counts += metrics.Missclassifications_Moderate(input, target)\n",
    "    total_counts += metrics.Missclassifications_Elevated(input, target)\n",
    "    total_counts += metrics.Missclassifications_High(input, target)\n",
    "\n",
    "    # Use the generate_metrics method to retrieve and calculate all defined metrics\n",
    "    metrics_functions = LossMetrics(loss, 'SolFSMY').get_metrics()\n",
    "    metrics_values = [metric(input, target) for metric in metrics_functions]\n",
    "\n",
    "    # Assert that the total manually calculated matches the sum of individual metrics\n",
    "    assert np.isclose(total_counts, sum(metrics_values)), f\"Expected {total_counts}, but got {sum(metrics_values)} ({metrics_values})\"\n",
    "    print(\"LossMetrics for classification loss test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.tensor([\n",
    "    [[10, 12, 12, 13, 12, 12, 12, 14, 12, 100], [20, 22, 23, 20, 22, 20, 21, 22, 23, 200]],\n",
    "    [[-11, -12, -13, -13, -14, -14, -12, -13, -14, -105], [-22, -23, -25, -23, -22, -24, -23, -22, -25, -210]]\n",
    "], dtype=torch.float)\n",
    "\n",
    "y_pred = torch.tensor([\n",
    "    [[11, 12, 12, 13, 12, 12, 13, 14, 13, 90], [21, 22, 23, 21, 22, 21, 22, 22, 23, 195]],\n",
    "    [[-12, -13, -14, -14, -15, -14, -13, -14, -14, -10], [-23, -24, -26, -24, -23, -25, -24, -23, -26, -205]]\n",
    "], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_OutlierDetectionMetrics():\n",
    "    metrics = F1ScoreMetrics().get_metrics()\n",
    "    metrics_precision = metrics[0](y_true, y_pred) \n",
    "    metrics_recall = metrics[1](y_true, y_pred)\n",
    "    metrics_f1_score = metrics[2](y_true, y_pred)\n",
    "    metrics_outliers_difference = metrics[3](y_true, y_pred)\n",
    "\n",
    "    f1 = 2 * (metrics_precision * metrics_recall) / (metrics_precision + metrics_recall)\n",
    "\n",
    "    assert metrics_precision == 1.0, f\"Expected 1.0, but got {metrics_precision}\"\n",
    "    assert metrics_recall == 0.75, f\"Expected 0.75, but got {metrics_recall}\"\n",
    "    assert metrics_f1_score == f1, f\"Expected {f1}, but got {metrics_f1_score}\"\n",
    "    assert metrics_outliers_difference == 1, f\"Expected 2, but got {metrics_outliers_difference}\"\n",
    "\n",
    "    print(\"OutlierDetectionMetrics test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LossMetrics test passed!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LossMetrics for classification loss test passed!\n",
      "OutlierDetectionMetrics test passed!\n"
     ]
    }
   ],
   "source": [
    "test_LossMetrics()\n",
    "test_LossMetrics_for_classification()\n",
    "test_OutlierDetectionMetrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
